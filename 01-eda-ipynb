{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9353254,"sourceType":"datasetVersion","datasetId":988298}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3W Dataset (WELL-only) — Row-per-file Event Classification\n\n**Goal:** Convert each time-series file into a single feature row (**leakage-safe by well grouping**), then train and evaluate multi-class classifiers for **`event_type_code` (0..9)**.\n\n## Summary (Engineer Story)\n\n* Built a file index from the 3W dataset and filtered to **WELL files only** (**1119 files**, **40 wells**).\n* Cleaned each file (per parquet): parsed/validated timestamps, sorted by time, renamed sensors to standardized names, and coerced numeric dtypes.\n* Engineered **row-per-file features** (one row per time-series file):\n\n  * **Continuous sensors:** raw **median/IQR/last**, robust **z-stats** (mean/std/min/max + **last valid z**), **first-vs-last time-window deltas** (and abs delta), and **missing fraction**.\n  * **State/valve sensors:** **last state**, **transition count/rate**, and **time-in-state proportions** for states 0..3 plus **p_other**.\n* Implemented leakage-safe preprocessing **inside the pipeline**:\n\n  * **Train-only column filtering** (drops all-missing / ~always-missing / constant features per fold).\n  * **Hybrid imputation** (median for continuous; -1 sentinel for state-level features; probability blocks fixed so they form valid distributions).\n  * Added **missing indicators** (`__isna`) for all features.\n* Evaluated with **group splits by `well_id`** (harder but realistic):\n\n  * **CV on train wells only** (StratifiedGroupKFold, n_splits=4 when feasible)\n  * **Final holdout** (GroupShuffleSplit: **32 train wells / 8 holdout wells**)\n  * **Stability check:** repeated group holdout (**30 splits**)\n* Compared baseline vs boosting:\n\n  * **Baseline:** Logistic Regression (balanced class weights)\n  * **Main:** HistGradientBoosting with **class-balanced sample weights**","metadata":{}},{"cell_type":"markdown","source":"## 0. Setup & Configuration\n\n- Paths, random seeds, output folders\n- Sensor name mapping and label mapping","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 3W Dataset (WELL-only) — Row-per-file Classification (v3_1)\n# ============================================================\n\nimport os, json, re\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\nfrom sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import (\n    f1_score,\n    classification_report,\n    confusion_matrix,\n)\nfrom sklearn.inspection import permutation_importance\n\n\n# -------------------------\n# Config\n# -------------------------\nBASE = \"/kaggle/input/3w-dataset/2.0.0\"\nRANDOM_STATE = 42\nN_WELL_FILES = None  # None = all WELL files\n\nFEATURES_VERSION = \"v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix\"\nOUT_DIR = f\"/kaggle/working/3w_prepared_{FEATURES_VERSION}\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nCACHE_PATH = f\"{OUT_DIR}/df_ml_well_{FEATURES_VERSION}.parquet\"\nprint(\"OUT_DIR:\", OUT_DIR)\nprint(\"CACHE_PATH:\", CACHE_PATH)\n\n# -------------------------\n# Dataset constants / mappings\n# -------------------------\nVAR_RENAME = {\n    \"ABER-CKGL\": \"gl_choke_opening_pct\",\n    \"ABER-CKP\":  \"prod_choke_opening_pct\",\n    \"ESTADO-DHSV\":   \"dhsv_state\",\n    \"ESTADO-M1\":     \"prod_master_valve_state\",\n    \"ESTADO-M2\":     \"ann_master_valve_state\",\n    \"ESTADO-PXO\":    \"pig_crossover_valve_state\",\n    \"ESTADO-SDV-GL\": \"gl_shutdown_valve_state\",\n    \"ESTADO-SDV-P\":  \"prod_shutdown_valve_state\",\n    \"ESTADO-W1\":     \"prod_wing_valve_state\",\n    \"ESTADO-W2\":     \"ann_wing_valve_state\",\n    \"ESTADO-XO\":     \"crossover_valve_state\",\n    \"P-ANULAR\":     \"annulus_pressure_pa\",\n    \"P-JUS-BS\":     \"svc_pump_downstream_pressure_pa\",\n    \"P-JUS-CKGL\":   \"gl_choke_downstream_pressure_pa\",\n    \"P-JUS-CKP\":    \"prod_choke_downstream_pressure_pa\",\n    \"P-MON-CKGL\":   \"gl_choke_upstream_pressure_pa\",\n    \"P-MON-CKP\":    \"prod_choke_upstream_pressure_pa\",\n    \"P-MON-SDV-P\":  \"prod_sdv_upstream_pressure_pa\",\n    \"P-PDG\":        \"pdg_downhole_pressure_pa\",\n    \"PT-P\":         \"xmas_tree_prod_line_pressure_pa\",\n    \"P-TPT\":        \"tpt_pressure_pa\",\n    \"QBS\": \"svc_pump_flow_m3s\",\n    \"QGL\": \"gas_lift_flow_m3s\",\n    \"T-JUS-CKP\": \"prod_choke_downstream_temp_c\",\n    \"T-MON-CKP\": \"prod_choke_upstream_temp_c\",\n    \"T-PDG\":     \"pdg_downhole_temp_c\",\n    \"T-TPT\":     \"tpt_temp_c\",\n    \"class\": \"class_code\",\n    \"state\": \"state_code\",\n}\n\nEVENT_TYPE_CODE_TO_NAME = {\n    0:\"Normal Operation\", 1:\"Abrupt Increase of BSW\", 2:\"Spurious Closure of DHSV\",\n    3:\"Severe Slugging\", 4:\"Flow Instability\", 5:\"Rapid Productivity Loss\",\n    6:\"Quick Restriction in PCK\", 7:\"Scaling in PCK\",\n    8:\"Hydrate in Production Line\", 9:\"Hydrate in Service Line\",\n}\n\nLABEL_COLS = {\"class_code\", \"state_code\", \"class_label\", \"state_label\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:07:16.977412Z","iopub.execute_input":"2026-01-09T19:07:16.977795Z","iopub.status.idle":"2026-01-09T19:07:18.940503Z","shell.execute_reply.started":"2026-01-09T19:07:16.977764Z","shell.execute_reply":"2026-01-09T19:07:18.939277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Build File Index (Metadata Table)\n\n- Scan all parquet files\n- Extract: event_type_code, source, well_id, run timestamp\n- Filter to WELL-only","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 1) Build file index\n# ============================================================\ndef build_file_index(base: str) -> pd.DataFrame:\n    paths = []\n    for root, _, files in os.walk(base):\n        for f in files:\n            if f.endswith(\".parquet\"):\n                paths.append(os.path.join(root, f))\n\n    df = pd.DataFrame({\"path\": paths})\n    codes = df[\"path\"].str.extract(r\"/2\\.0\\.0/(\\d+)/\", expand=False)\n    df[\"event_type_code\"] = pd.to_numeric(codes, errors=\"coerce\").astype(\"Int64\")\n    df = df.dropna(subset=[\"event_type_code\"])\n    df[\"event_type_code\"] = df[\"event_type_code\"].astype(int)\n\n    df[\"file\"] = df[\"path\"].str.split(\"/\").str[-1]\n    df[\"source\"] = df[\"file\"].str.extract(r\"^(WELL|SIMULATED|DRAWN)\")\n    df[\"well_id\"] = df[\"file\"].str.extract(r\"(WELL-\\d+)\")\n    df[\"run_ts\"] = df[\"file\"].str.extract(r\"_(\\d{14})\")\n    df[\"run_ts\"] = pd.to_datetime(df[\"run_ts\"], format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n\n    return df.sort_values([\"event_type_code\",\"source\",\"well_id\",\"run_ts\"]).reset_index(drop=True)\n\ndf_files = build_file_index(BASE)\ndf_w_files = df_files[df_files[\"source\"] == \"WELL\"].copy()\nassert df_w_files[\"well_id\"].notna().all()\n\nprint(\"Total files:\", len(df_files), \"| WELL files:\", len(df_w_files), \"| Wells:\", df_w_files[\"well_id\"].nunique())\nprint(\"Class counts:\\n\", df_w_files[\"event_type_code\"].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:07:18.942105Z","iopub.execute_input":"2026-01-09T19:07:18.942502Z","iopub.status.idle":"2026-01-09T19:07:23.606735Z","shell.execute_reply.started":"2026-01-09T19:07:18.942472Z","shell.execute_reply":"2026-01-09T19:07:23.605798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Cleaning (Per-file)\n\n- Parse timestamps and sort\n- Rename sensors\n- Coerce numeric dtypes\n","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 2) Cleaning\n# ============================================================\ndef clean_3w_instance(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"timestamp\" in df.columns:\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n        df = df.set_index(\"timestamp\")\n    else:\n        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n\n    df = df[~df.index.isna()].sort_index()\n    df.index.name = \"timestamp\"\n    df = df.rename(columns=VAR_RENAME)\n\n    for c in df.columns:\n        if c in (\"class_code\", \"state_code\"):\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int16\")\n        else:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:07:23.607749Z","iopub.execute_input":"2026-01-09T19:07:23.608061Z","iopub.status.idle":"2026-01-09T19:07:23.615387Z","shell.execute_reply.started":"2026-01-09T19:07:23.608033Z","shell.execute_reply":"2026-01-09T19:07:23.614684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Feature Engineering (Per-file → Single Row)\n\n- Continuous sensors: robust z-stats + deltas between first/last time windows + missingness\n- State sensors: last state, transition counts/rates, state proportions","metadata":{}},{"cell_type":"code","source":"\n\n# ============================================================\n# 3) Feature extraction (time-window first/last + last-valid z_last)\n# ============================================================\ndef summarize_timeseries_v3_1(df_clean: pd.DataFrame, frac: float = 0.1, state_max: int = 3) -> dict:\n    sensors = df_clean.drop(columns=list(LABEL_COLS), errors=\"ignore\")\n    num = sensors.select_dtypes(include=[np.number])\n\n    out = {\n        \"n_obs\": int(len(df_clean)),\n        \"duration_s\": float((df_clean.index.max() - df_clean.index.min()).total_seconds())\n                      if len(df_clean) else np.nan,\n    }\n    if num.shape[1] == 0 or len(num) == 0:\n        return out\n\n    state_cols = [c for c in num.columns if c.endswith(\"_state\")]\n    cont_cols  = [c for c in num.columns if c not in state_cols]\n\n    def _first_last_masks(index, frac: float, n: int):\n        k = max(1, int(n * frac))\n        if n < 2:\n            m = np.ones(n, dtype=bool)\n            return m, m\n\n        try:\n            idx = pd.DatetimeIndex(index)\n        except Exception:\n            idx = None\n\n        if idx is not None and idx.notna().all():\n            tmin, tmax = idx.min(), idx.max()\n            dur_s = (tmax - tmin).total_seconds()\n            if np.isfinite(dur_s) and dur_s > 0:\n                w = dur_s * frac\n                t_first_end = tmin + pd.Timedelta(seconds=w)\n                t_last_start = tmax - pd.Timedelta(seconds=w)\n\n                first_mask = np.asarray(idx <= t_first_end, dtype=bool)\n                last_mask  = np.asarray(idx >= t_last_start, dtype=bool)\n\n                if first_mask.sum() == 0:\n                    first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n                if last_mask.sum() == 0:\n                    last_mask = np.zeros(n, dtype=bool); last_mask[-k:] = True\n                return first_mask, last_mask\n\n        first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n        last_mask  = np.zeros(n, dtype=bool); last_mask[-k:] = True\n        return first_mask, last_mask\n\n    # Continuous sensors\n    if len(cont_cols):\n        cont = num[cont_cols]\n        med_raw = cont.median()\n        iqr_raw = (cont.quantile(0.75) - cont.quantile(0.25))\n\n        iqr = iqr_raw.replace(0, np.nan)\n        z = (cont - med_raw) / iqr\n\n        first_mask, last_mask = _first_last_masks(df_clean.index, frac=frac, n=len(z))\n        first = z.iloc[first_mask].mean()\n        last  = z.iloc[last_mask].mean()\n\n        agg = z.agg([\"mean\", \"std\", \"min\", \"max\"]).T\n        miss = cont.isna().mean()\n\n        for col in cont_cols:\n            out[f\"{col}__raw_median\"] = med_raw[col]\n            out[f\"{col}__raw_iqr\"]    = iqr_raw[col]\n\n            s_raw = cont[col].dropna()\n            out[f\"{col}__raw_last\"] = s_raw.iloc[-1] if len(s_raw) else np.nan\n\n            out[f\"{col}__z_mean\"] = agg.loc[col, \"mean\"]\n            out[f\"{col}__z_std\"]  = agg.loc[col, \"std\"]\n            out[f\"{col}__z_min\"]  = agg.loc[col, \"min\"]\n            out[f\"{col}__z_max\"]  = agg.loc[col, \"max\"]\n\n            # last VALID z (not last row)\n            s_z = z[col].dropna()\n            out[f\"{col}__z_last\"] = s_z.iloc[-1] if len(s_z) else np.nan\n\n            out[f\"{col}__delta_last_first\"] = (last[col] - first[col])\n            out[f\"{col}__abs_delta\"]        = abs(last[col] - first[col])\n            out[f\"{col}__missing_frac\"]     = miss[col]\n\n    # State-like sensors\n    if len(state_cols):\n        st = num[state_cols]\n        for col in state_cols:\n            s = st[col]\n            s_non = s.dropna()\n\n            out[f\"{col}__missing_frac\"] = float(s.isna().mean())\n            out[f\"{col}__last\"] = float(s_non.iloc[-1]) if len(s_non) else np.nan\n\n            if len(s_non) >= 2:\n                n_trans = int((s_non != s_non.shift()).sum() - 1)\n            else:\n                n_trans = 0\n\n            out[f\"{col}__n_transitions\"] = n_trans\n            out[f\"{col}__transitions_rate\"] = n_trans / max(1, len(s_non))\n\n            known = 0.0\n            for v in range(state_max + 1):\n                p = float((s_non == v).mean()) if len(s_non) else np.nan\n                out[f\"{col}__p_state_{v}\"] = p\n                if not np.isnan(p):\n                    known += p\n            out[f\"{col}__p_state_other\"] = (1.0 - known) if len(s_non) else np.nan\n\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:07:23.617766Z","iopub.execute_input":"2026-01-09T19:07:23.618061Z","iopub.status.idle":"2026-01-09T19:07:23.642143Z","shell.execute_reply.started":"2026-01-09T19:07:23.618033Z","shell.execute_reply":"2026-01-09T19:07:23.640998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Build Row-per-file Dataset (+ Caching)\n\n- Loop over WELL files\n- Read parquet → clean → summarize → append\n- Save/load cached parquet to speed up reruns","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 4) Build row-per-file dataset (with caching)\n# ============================================================\ndef build_row_per_file_dataset(df_files: pd.DataFrame, n_files: int | None = None, random_state: int = 42) -> pd.DataFrame:\n    if n_files is None:\n        sample = df_files.reset_index(drop=True)\n    else:\n        sample = df_files.sample(n_files, random_state=random_state).reset_index(drop=True)\n\n    rows = []\n    for _, r in tqdm(sample.iterrows(), total=len(sample), desc=\"Building WELL dataset\"):\n        df_raw = pd.read_parquet(r[\"path\"])\n        df_clean = clean_3w_instance(df_raw)\n\n        feats = summarize_timeseries_v3_1(df_clean)\n        feats[\"event_type_code\"] = int(r[\"event_type_code\"])\n        feats[\"event_type_name\"] = EVENT_TYPE_CODE_TO_NAME.get(int(r[\"event_type_code\"]), \"Unknown\")\n        feats[\"well_id\"] = r[\"well_id\"]\n        feats[\"run_ts\"] = r[\"run_ts\"]\n        feats[\"file\"] = r[\"file\"]\n        rows.append(feats)\n\n    return pd.DataFrame(rows)\n\nif os.path.exists(CACHE_PATH):\n    df_ml_well = pd.read_parquet(CACHE_PATH)\n    print(\"Loaded cached features:\", df_ml_well.shape)\nelse:\n    df_ml_well = build_row_per_file_dataset(df_w_files, n_files=N_WELL_FILES, random_state=RANDOM_STATE)\n    df_ml_well.to_parquet(CACHE_PATH, index=False)\n    print(\"Built + saved features:\", df_ml_well.shape)\n\nwith open(f\"{OUT_DIR}/dataset_config.json\", \"w\") as f:\n    json.dump({\n        \"base\": BASE,\n        \"random_state\": RANDOM_STATE,\n        \"n_well_files_used\": int(len(df_ml_well)),\n        \"features_version\": FEATURES_VERSION,\n        \"notes\": \"continuous: raw median/iqr/last + robust-z stats + deltas; \"\n                 \"state: last + transitions + proportions (+other); WELL-only; \"\n                 \"fixes: time-window first/last + z_last last-valid; \"\n                 \"imputer: hybrid median/-1 + prob semantic fix + indicators; \"\n                 \"cache versioned to avoid staleness\"\n    }, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:07:23.643540Z","iopub.execute_input":"2026-01-09T19:07:23.644324Z","iopub.status.idle":"2026-01-09T19:09:49.282056Z","shell.execute_reply.started":"2026-01-09T19:07:23.644278Z","shell.execute_reply":"2026-01-09T19:09:49.281134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Prepare ML Matrices\n\n- X: engineered features\n- y: event_type_code\n- groups: well_id (for leakage-safe splitting)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 5) Build X/y/groups (NO global filtering leakage)\n# ============================================================\ndef make_Xy_groups(df: pd.DataFrame):\n    y = df[\"event_type_code\"].astype(int).copy()\n    groups = df[\"well_id\"].astype(str).copy()\n    drop_cols = [\"event_type_code\",\"event_type_name\",\"file\",\"run_ts\",\"well_id\"]\n    X = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n    X = X.replace([np.inf, -np.inf], np.nan)\n    return X, y, groups\n\nX, y, groups = make_Xy_groups(df_ml_well)\nprint(\"X:\", X.shape, \"| y:\", y.shape, \"| wells:\", groups.nunique())\nprint(\"Label counts:\\n\", y.value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:09:49.283169Z","iopub.execute_input":"2026-01-09T19:09:49.283476Z","iopub.status.idle":"2026-01-09T19:09:49.298541Z","shell.execute_reply.started":"2026-01-09T19:09:49.283440Z","shell.execute_reply":"2026-01-09T19:09:49.297330Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Leakage-safe Preprocessing inside Pipeline\n\n- Train-only column filtering\n- Hybrid imputation with probability semantics fix\n- Missing indicators for all features","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 6) Train-only column filter (inside pipeline)\n# ============================================================\nclass TrainOnlyColumnFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, missing_threshold: float = 0.98):\n        self.missing_threshold = missing_threshold\n\n    def fit(self, X, y=None):\n        X = X.copy().replace([np.inf, -np.inf], np.nan)\n        all_missing = X.columns[X.isna().all()]\n        miss = X.isna().mean()\n        high_missing = miss[miss > self.missing_threshold].index\n        const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n        drop = set(all_missing) | set(high_missing) | set(const_cols)\n        self.keep_columns_ = [c for c in X.columns if c not in drop]\n        return self\n\n    def transform(self, X):\n        X = X.copy().replace([np.inf, -np.inf], np.nan)\n        for c in getattr(self, \"keep_columns_\", []):\n            if c not in X.columns:\n                X[c] = np.nan\n        return X[self.keep_columns_]\n\n# ============================================================\n# 7) Hybrid imputer (probability semantics fixed)\n#    - median for continuous\n#    - -1 for state-ish \"level/last/transition\" features\n#    - probability block per state sensor:\n#         * if ALL probs missing -> p_other=1, p_state_*=0\n#         * else fill NaN with 0 and renormalize to sum to 1\n#    - + missing indicators for all columns\n# ============================================================\nclass HybridImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, state_token: str = \"_state\", fill_value_state: float = -1.0):\n        self.state_token = state_token\n        self.fill_value_state = fill_value_state\n\n    @staticmethod\n    def _parse_prob_block(col: str):\n        \"\"\"\n        Expect columns like: <sensor>_state__p_state_<k> or <sensor>_state__p_state_other\n        Returns (base_sensor, is_other, k or None) or None if not match.\n        \"\"\"\n        # examples:\n        # dhsv_state__p_state_0\n        # dhsv_state__p_state_other\n        m_k = re.match(r\"^(.*_state)__p_state_(\\d+)$\", col)\n        if m_k:\n            return m_k.group(1), False, int(m_k.group(2))\n        m_o = re.match(r\"^(.*_state)__p_state_other$\", col)\n        if m_o:\n            return m_o.group(1), True, None\n        return None\n\n    def fit(self, X, y=None):\n        X = X.copy()\n        self.cols_ = list(X.columns)\n\n        # any feature derived from a state sensor has \"_state\" in name\n        state_like = [c for c in self.cols_ if self.state_token in c]\n\n        # probability columns + group them by state sensor base\n        prob_cols = []\n        prob_groups = {}  # base -> {\"other\": col or None, \"states\": {k: col}}\n        for c in self.cols_:\n            parsed = self._parse_prob_block(c)\n            if parsed is None:\n                continue\n            base, is_other, k = parsed\n            prob_cols.append(c)\n            prob_groups.setdefault(base, {\"other\": None, \"states\": {}})\n            if is_other:\n                prob_groups[base][\"other\"] = c\n            else:\n                prob_groups[base][\"states\"][k] = c\n\n        self.prob_cols_ = prob_cols\n        self.prob_groups_ = prob_groups\n\n        # remaining state-like (non-prob) -> sentinel -1\n        self.state_cols_ = [c for c in state_like if c not in self.prob_cols_]\n\n        # continuous-ish are not state-derived -> median\n        self.cont_cols_ = [c for c in self.cols_ if c not in state_like]\n        self.cont_medians_ = (\n            X[self.cont_cols_].median(numeric_only=True)\n            if len(self.cont_cols_) else pd.Series(dtype=float)\n        )\n\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        # Ensure all fit columns exist\n        for c in self.cols_:\n            if c not in X.columns:\n                X[c] = np.nan\n        X = X[self.cols_]\n\n        # Missing indicators\n        miss = X.isna().astype(np.int8)\n        miss.columns = [f\"{c}__isna\" for c in miss.columns]\n\n        # Continuous: median then fallback to 0.0 if median was NaN\n        if len(self.cont_cols_):\n            X[self.cont_cols_] = X[self.cont_cols_].fillna(self.cont_medians_)\n            X[self.cont_cols_] = X[self.cont_cols_].fillna(0.0)\n\n        # Probabilities: semantic fix per sensor group\n        # - if all probs are NaN -> set p_other=1, p_state_k=0\n        # - else fill NaN with 0 and renormalize\n        if len(self.prob_cols_):\n            # work group-by-group\n            for base, g in self.prob_groups_.items():\n                other_col = g.get(\"other\", None)\n                state_map = g.get(\"states\", {})\n                cols = []\n                if other_col is not None:\n                    cols.append(other_col)\n                cols.extend([state_map[k] for k in sorted(state_map.keys())])\n\n                if not cols:\n                    continue\n\n                block = X[cols]\n                all_nan = block.isna().all(axis=1)\n\n                # initialize by filling NaNs with 0\n                block_filled = block.fillna(0.0)\n\n                # rows where all probs were missing: force p_other=1 and others 0\n                if other_col is not None:\n                    block_filled.loc[all_nan, other_col] = 1.0\n                # ensure all p_state_k are 0 for all_nan rows\n                for c in cols:\n                    if c != other_col:\n                        block_filled.loc[all_nan, c] = 0.0\n\n                # renormalize rows where NOT all_nan (sum>0) to sum to 1\n                not_all = ~all_nan\n                if not_all.any():\n                    s = block_filled.loc[not_all, cols].sum(axis=1)\n                    # if sum is 0 (possible if everything was NaN but other_col missing in schema)\n                    # fall back to \"other=1\" when available, else leave zeros.\n                    zero_sum = s == 0\n                    if zero_sum.any() and other_col is not None:\n                        block_filled.loc[not_all & zero_sum, other_col] = 1.0\n                        for c in cols:\n                            if c != other_col:\n                                block_filled.loc[not_all & zero_sum, c] = 0.0\n                        s = block_filled.loc[not_all, cols].sum(axis=1)\n\n                    # divide by sum safely\n                    block_filled.loc[not_all, cols] = block_filled.loc[not_all, cols].div(s, axis=0)\n\n                X[cols] = block_filled\n\n        # State-level / transition features: fill missing with -1 sentinel\n        if len(self.state_cols_):\n            X[self.state_cols_] = X[self.state_cols_].fillna(self.fill_value_state)\n\n        return pd.concat([X, miss], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:09:49.300223Z","iopub.execute_input":"2026-01-09T19:09:49.301012Z","iopub.status.idle":"2026-01-09T19:09:49.329345Z","shell.execute_reply.started":"2026-01-09T19:09:49.300963Z","shell.execute_reply":"2026-01-09T19:09:49.328324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Models\n\n- Baseline: Logistic Regression\n- Main: Weighted HistGradientBoosting (class-balanced sample weights)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 8) Weighted HGB estimator (cloneable) + predict_proba\n# ============================================================\nclass WeightedHGBClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(\n        self,\n        max_depth=6,\n        learning_rate=0.06,\n        max_iter=700,\n        max_leaf_nodes=31,\n        min_samples_leaf=20,\n        l2_regularization=0.1,\n        random_state=42,\n    ):\n        self.max_depth = max_depth\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_samples_leaf = min_samples_leaf\n        self.l2_regularization = l2_regularization\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        self.clf_ = HistGradientBoostingClassifier(\n            max_depth=self.max_depth,\n            learning_rate=self.learning_rate,\n            max_iter=self.max_iter,\n            max_leaf_nodes=self.max_leaf_nodes,\n            min_samples_leaf=self.min_samples_leaf,\n            l2_regularization=self.l2_regularization,\n            random_state=self.random_state,\n        )\n\n        y_arr = np.asarray(y)\n        classes = np.unique(y_arr)\n        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_arr)\n        cw_map = dict(zip(classes, cw))\n        sample_weight = np.array([cw_map[v] for v in y_arr], dtype=float)\n\n        self.clf_.fit(X, y_arr, sample_weight=sample_weight)\n        self.classes_ = classes\n        return self\n\n    def predict(self, X):\n        return self.clf_.predict(X)\n\n    def predict_proba(self, X):\n        return self.clf_.predict_proba(X)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:09:49.330625Z","iopub.execute_input":"2026-01-09T19:09:49.331078Z","iopub.status.idle":"2026-01-09T19:09:49.350451Z","shell.execute_reply.started":"2026-01-09T19:09:49.331047Z","shell.execute_reply":"2026-01-09T19:09:49.349486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Evaluation\n\n- Final holdout split by well_id (GroupShuffleSplit)\n- CV on train wells only (StratifiedGroupKFold)\n- Stability: repeated group holdout\n","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 9) Evaluation helpers\n# ============================================================\ndef choose_stratified_group_n_splits(X, y, groups, max_splits=4, random_state=42) -> int:\n    \"\"\"Pick the largest n_splits (<= max_splits) that doesn't raise ValueError.\"\"\"\n    for k in range(max_splits, 1, -1):\n        try:\n            cv = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=random_state)\n            _ = next(cv.split(X, y, groups=groups))\n            return k\n        except ValueError:\n            continue\n    return 2\n\ndef evaluate_stratified_group_cv(model, X, y, groups, n_splits=4, random_state=42, major_min_support=10):\n    labels = np.sort(pd.unique(y))\n    cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    macro_f1s, major_f1s, bin_f1s = [], [], []\n    per_class_f1 = {int(lbl): [] for lbl in labels}\n\n    for _, (tr, te) in enumerate(cv.split(X, y, groups=groups), start=1):\n        Xtr, Xte = X.iloc[tr], X.iloc[te]\n        ytr, yte = y.iloc[tr], y.iloc[te]\n\n        m = clone(model)\n        m.fit(Xtr, ytr)\n        pred = m.predict(Xte)\n\n        macro_f1s.append(f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0))\n\n        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n        if major_classes:\n            major_labels = np.array(sorted(major_classes))\n            major_mask = yte.isin(major_labels)\n            if major_mask.any():\n                major_f1s.append(\n                    f1_score(\n                        yte[major_mask], pred[major_mask],\n                        average=\"macro\", labels=major_labels, zero_division=0\n                    )\n                )\n            else:\n                major_f1s.append(np.nan)\n        else:\n            major_f1s.append(np.nan)\n\n        bin_f1s.append(\n            f1_score((yte != 0).astype(int), (pred != 0).astype(int), zero_division=0)\n        )\n\n        rep = classification_report(yte, pred, labels=labels, output_dict=True, zero_division=0)\n        for lbl in labels:\n            per_class_f1[int(lbl)].append(rep[str(int(lbl))][\"f1-score\"])\n\n    return {\n        \"macro_f1_mean\": float(np.nanmean(macro_f1s)),\n        \"macro_f1_std\":  float(np.nanstd(macro_f1s)),\n        \"major_macro_f1_mean\": float(np.nanmean(major_f1s)),\n        \"fault_vs_normal_f1_mean\": float(np.nanmean(bin_f1s)),\n        \"per_class_f1_mean\": {str(k): float(np.nanmean(v)) for k, v in per_class_f1.items()},\n    }\n\ndef evaluate_on_holdout(model, Xtr, ytr, Xho, yho, major_min_support=10):\n    labels = np.sort(pd.unique(pd.concat([ytr, yho], axis=0)))\n\n    m = clone(model)\n    m.fit(Xtr, ytr)\n    pred = m.predict(Xho)\n\n    macro = f1_score(yho, pred, average=\"macro\", labels=labels, zero_division=0)\n\n    major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n    major_labels = np.array(sorted(major_classes)) if major_classes else np.array([], dtype=int)\n    if len(major_labels) and yho.isin(major_labels).any():\n        major = f1_score(\n            yho[yho.isin(major_labels)], pred[yho.isin(major_labels)],\n            average=\"macro\", labels=major_labels, zero_division=0\n        )\n    else:\n        major = np.nan\n\n    bin_f1 = f1_score((yho != 0).astype(int), (pred != 0).astype(int), zero_division=0)\n    report_text = classification_report(yho, pred, labels=labels, zero_division=0)\n    report_dict = classification_report(yho, pred, labels=labels, zero_division=0, output_dict=True)\n\n    return {\n        \"macro_f1\": float(macro),\n        \"major_macro_f1\": float(major) if np.isfinite(major) else np.nan,\n        \"fault_vs_normal_f1\": float(bin_f1),\n        \"classification_report\": report_text,\n        \"classification_report_dict\": report_dict,  # for tables\n        \"labels\": labels,\n        \"y_pred\": pred,\n    }\n\ndef repeated_holdout(model, X, y, groups, repeats=30, test_size=0.2, random_state=42, major_min_support=10):\n    labels = np.sort(pd.unique(y))\n    gss = GroupShuffleSplit(n_splits=repeats, test_size=test_size, random_state=random_state)\n\n    macs, bins, majors = [], [], []\n    for tr, te in gss.split(X, y, groups=groups):\n        Xtr, Xte = X.iloc[tr], X.iloc[te]\n        ytr, yte = y.iloc[tr], y.iloc[te]\n\n        m = clone(model)\n        m.fit(Xtr, ytr)\n        pred = m.predict(Xte)\n\n        macs.append(f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0))\n        bins.append(f1_score((yte != 0).astype(int), (pred != 0).astype(int), zero_division=0))\n\n        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n        major_labels = np.array(sorted(major_classes)) if major_classes else np.array([], dtype=int)\n        if len(major_labels) and yte.isin(major_labels).any():\n            majors.append(\n                f1_score(\n                    yte[yte.isin(major_labels)], pred[yte.isin(major_labels)],\n                    average=\"macro\", labels=major_labels, zero_division=0\n                )\n            )\n        else:\n            majors.append(np.nan)\n\n    return {\n        \"macro_f1_mean\": float(np.nanmean(macs)),\n        \"macro_f1_std\": float(np.nanstd(macs)),\n        \"major_macro_f1_mean\": float(np.nanmean(majors)),\n        \"fault_vs_normal_f1_mean\": float(np.nanmean(bins)),\n        \"fault_vs_normal_f1_std\": float(np.nanstd(bins)),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:09:49.352043Z","iopub.execute_input":"2026-01-09T19:09:49.352427Z","iopub.status.idle":"2026-01-09T19:09:49.381102Z","shell.execute_reply.started":"2026-01-09T19:09:49.352387Z","shell.execute_reply":"2026-01-09T19:09:49.379798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Diagnostics & Explainability\n\n- Confusion matrix (CSV)\n- Top confusions table (CSV)\n- Per-class metrics (CSV)\n- Permutation importance on holdout (CSV)\n- Per-well performance (CSV)\n- Save a single results_summary.json with full traceability","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# 10) Models (consistent preprocessing)\n# ============================================================\npreprocess_common = [\n    (\"col_filter\", TrainOnlyColumnFilter(missing_threshold=0.98)),\n    (\"imputer\", HybridImputer(state_token=\"_state\", fill_value_state=-1.0)),\n]\n\nlogreg = Pipeline(preprocess_common + [\n    (\"scaler\", StandardScaler(with_mean=False)),\n    (\"clf\", LogisticRegression(max_iter=8000, class_weight=\"balanced\"))\n])\n\nhgb = Pipeline(preprocess_common + [\n    (\"clf\", WeightedHGBClassifier(\n        max_depth=6,\n        learning_rate=0.06,\n        max_iter=700,\n        max_leaf_nodes=31,\n        min_samples_leaf=20,\n        l2_regularization=0.1,\n        random_state=RANDOM_STATE\n    ))\n])\n\n# ============================================================\n# 11) Final holdout by WELL + CV on train only + repeated holdout\n#     + NEW: Save holdout diagnostics (HGB confusion matrix + per-class table)\n#     + NEW: Log holdout well IDs in JSON\n# ============================================================\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\ntr_idx, ho_idx = next(gss.split(X, y, groups=groups))\n\nX_tr, y_tr, g_tr = X.iloc[tr_idx], y.iloc[tr_idx], groups.iloc[tr_idx]\nX_ho, y_ho, g_ho = X.iloc[ho_idx], y.iloc[ho_idx], groups.iloc[ho_idx]\n\nholdout_wells = sorted(g_ho.unique().tolist())\ntrain_wells = sorted(g_tr.unique().tolist())\n\nprint(\"\\nTrain wells:\", len(train_wells), \"| Holdout wells:\", len(holdout_wells))\nprint(\"Holdout wells:\", holdout_wells)\nprint(\"Holdout label counts:\\n\", y_ho.value_counts().sort_index())\n\nn_splits = choose_stratified_group_n_splits(X_tr, y_tr, g_tr, max_splits=4, random_state=RANDOM_STATE)\nprint(f\"\\nUsing StratifiedGroupKFold with n_splits={n_splits}\")\n\nprint(\"\\n=== CV on TRAIN ONLY ===\")\nlogreg_cv = evaluate_stratified_group_cv(logreg, X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\nhgb_cv    = evaluate_stratified_group_cv(hgb,   X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\nprint(\"LogReg CV:\", logreg_cv)\nprint(\"HGB   CV:\", hgb_cv)\n\nprint(\"\\n=== FINAL HOLDOUT (train wells -> holdout wells) ===\")\nlogreg_hold = evaluate_on_holdout(logreg, X_tr, y_tr, X_ho, y_ho)\nhgb_hold    = evaluate_on_holdout(hgb,   X_tr, y_tr, X_ho, y_ho)\n\nprint(\"LogReg Holdout:\", {k: v for k, v in logreg_hold.items() if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}})\nprint(\"HGB   Holdout:\", {k: v for k, v in hgb_hold.items() if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}})\n\nprint(\"\\nLogReg classification report (holdout):\\n\", logreg_hold[\"classification_report\"])\nprint(\"\\nHGB classification report (holdout):\\n\", hgb_hold[\"classification_report\"])\n\n# ---- NEW: Holdout diagnostics for HGB (confusion matrix + per-class table)\nlabels = hgb_hold[\"labels\"]\npred_hgb = hgb_hold[\"y_pred\"]\n\ncm = confusion_matrix(y_ho, pred_hgb, labels=labels)\ncm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\ncm_path = f\"{OUT_DIR}/hgb_holdout_confusion_matrix.csv\"\ncm_df.to_csv(cm_path, index=True)\nprint(f\"\\nSaved HGB holdout confusion matrix: {cm_path}\")\n\n\n# ---- (A) Error analysis: Top confusions (off-diagonal)\ncm_counts = cm.copy()\nnp.fill_diagonal(cm_counts, 0)\n\nrow_support = cm.sum(axis=1)  # support per true class\npairs = []\nfor i, t in enumerate(labels):\n    for j, p in enumerate(labels):\n        if i == j:\n            continue\n        c = int(cm_counts[i, j])\n        if c > 0:\n            pairs.append({\n                \"true_class\": int(t),\n                \"true_name\": EVENT_TYPE_CODE_TO_NAME.get(int(t), \"Unknown\"),\n                \"pred_class\": int(p),\n                \"pred_name\": EVENT_TYPE_CODE_TO_NAME.get(int(p), \"Unknown\"),\n                \"count\": c,\n                \"pct_of_true_class\": c / max(1, int(row_support[i]))\n            })\n\ntop_conf_df = (pd.DataFrame(pairs)\n               .sort_values([\"count\", \"pct_of_true_class\"], ascending=False)\n               .head(10))\n\ntop_conf_path = f\"{OUT_DIR}/hgb_holdout_top_confusions.csv\"\ntop_conf_df.to_csv(top_conf_path, index=False)\nprint(f\"Saved top confusions table: {top_conf_path}\")\n\nprint(\"\\nTop-3 confusions (HGB holdout):\")\nprint(top_conf_df.head(3).to_string(index=False))\n\n# ---- (B) Permutation importance on HOLDOUT (macro-F1)\nhgb_final = clone(hgb).fit(X_tr, y_tr)\n\nperm = permutation_importance(\n    hgb_final,\n    X_ho, y_ho,\n    n_repeats=8,\n    random_state=RANDOM_STATE,\n    scoring=\"f1_macro\",\n    n_jobs=1\n)\n\nperm_df = (pd.DataFrame({\n    \"feature\": X_ho.columns,\n    \"importance_mean\": perm.importances_mean,\n    \"importance_std\": perm.importances_std\n}).sort_values(\"importance_mean\", ascending=False))\n\nperm_path = f\"{OUT_DIR}/hgb_holdout_permutation_importance_f1macro.csv\"\nperm_df.to_csv(perm_path, index=False)\nprint(f\"\\nSaved permutation importance: {perm_path}\")\n\nprint(\"\\nTop-15 important features (perm importance, macro-F1):\")\nprint(perm_df.head(15).to_string(index=False))\n\n# ---- (C) Per-well performance on HOLDOUT\npred_hgb_final = hgb_final.predict(X_ho)\n\ndf_well_perf = pd.DataFrame({\n    \"well_id\": g_ho.values,\n    \"y_true\": y_ho.values,\n    \"y_pred\": pred_hgb_final\n})\n\nwell_rows = []\nfor well, gdf in df_well_perf.groupby(\"well_id\"):\n    yt = gdf[\"y_true\"].astype(int)\n    yp = gdf[\"y_pred\"].astype(int)\n    well_rows.append({\n        \"well_id\": well,\n        \"n_samples\": int(len(gdf)),\n        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", labels=labels, zero_division=0)),\n        \"fault_vs_normal_f1\": float(f1_score((yt != 0).astype(int), (yp != 0).astype(int), zero_division=0)),\n    })\n\nwell_metrics_df = (pd.DataFrame(well_rows)\n                   .sort_values([\"macro_f1\", \"n_samples\"], ascending=[True, False]))\n\nwell_metrics_path = f\"{OUT_DIR}/hgb_holdout_per_well_metrics.csv\"\nwell_metrics_df.to_csv(well_metrics_path, index=False)\nprint(f\"\\nSaved per-well metrics: {well_metrics_path}\")\n\nprint(\"\\nWorst 5 wells by macro-F1 (HGB holdout):\")\nprint(well_metrics_df.head(5).to_string(index=False))\n\nprint(\"\\nBest 5 wells by macro-F1 (HGB holdout):\")\nprint(well_metrics_df.tail(5).to_string(index=False))\n\n\nrep_dict = hgb_hold[\"classification_report_dict\"]\nrows = []\nfor l in labels:\n    key = str(int(l))\n    if key in rep_dict:\n        rows.append({\n            \"class\": int(l),\n            \"name\": EVENT_TYPE_CODE_TO_NAME.get(int(l), \"Unknown\"),\n            \"precision\": rep_dict[key][\"precision\"],\n            \"recall\": rep_dict[key][\"recall\"],\n            \"f1\": rep_dict[key][\"f1-score\"],\n            \"support\": rep_dict[key][\"support\"],\n        })\nper_class_df = pd.DataFrame(rows).sort_values(\"class\")\nper_class_path = f\"{OUT_DIR}/hgb_holdout_per_class_metrics.csv\"\nper_class_df.to_csv(per_class_path, index=False)\nprint(f\"Saved HGB holdout per-class metrics table: {per_class_path}\")\n\n# Optional print: quick recall view\nprint(\"\\nHGB holdout per-class recall (quick view):\")\nprint(per_class_df[[\"class\",\"name\",\"recall\",\"support\"]].to_string(index=False))\n\nprint(\"\\n=== Repeated Group Holdout (30 splits) ===\")\nlogreg_rep = repeated_holdout(logreg, X, y, groups, repeats=30, test_size=0.2, random_state=RANDOM_STATE)\nhgb_rep    = repeated_holdout(hgb,   X, y, groups, repeats=30, test_size=0.2, random_state=RANDOM_STATE)\nprint(\"LogReg:\", logreg_rep)\nprint(\"HGB   :\", hgb_rep)\n\n# ============================================================\n# Save results (JSON) + NEW: holdout wells + artifact paths\n# ============================================================\nresults = {\n    \"features_version\": FEATURES_VERSION,\n    \"random_state\": RANDOM_STATE,\n\n    \"split_traceability\": {\n        \"train_wells\": train_wells,\n        \"holdout_wells\": holdout_wells,\n        \"n_train_wells\": len(train_wells),\n        \"n_holdout_wells\": len(holdout_wells),\n    },\n\n    \"logreg_cv\": logreg_cv,\n    \"hgb_cv\": hgb_cv,\n\n    \"logreg_holdout\": {\n        k: v for k, v in logreg_hold.items()\n        if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}\n    },\n    \"hgb_holdout\": {\n        k: v for k, v in hgb_hold.items()\n        if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}\n    },\n\n    \"holdout_diagnostics\": {\n    \"hgb_confusion_matrix_csv\": cm_path,\n    \"hgb_per_class_metrics_csv\": per_class_path,\n    \"hgb_top_confusions_csv\": top_conf_path,\n    \"hgb_permutation_importance_csv\": perm_path,\n    \"hgb_per_well_metrics_csv\": well_metrics_path,\n},\n\n    \"logreg_repeated_holdout\": logreg_rep,\n    \"hgb_repeated_holdout\": hgb_rep,\n\n    \"notes\": {\n        \"leakage_control\": [\n            \"train-only column filtering inside pipeline\",\n            \"group splitting by well_id\",\n            \"CV on train wells only (plus a separate holdout)\",\n            \"major classes computed from y_train per fold\",\n            \"fixed-label macro-F1 with zero_division=0\",\n            \"repeated group holdout metrics (mean/std)\",\n            \"hybrid imputation: median for continuous; prob semantic fix + renorm; state-level->-1; + missing indicators\",\n            \"cache path versioned to avoid stale features\",\n        ]\n    }\n}\n\nwith open(f\"{OUT_DIR}/results_summary.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\nSaved: {OUT_DIR}/results_summary.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:09:49.384029Z","iopub.execute_input":"2026-01-09T19:09:49.384378Z","iopub.status.idle":"2026-01-09T19:36:59.055246Z","shell.execute_reply.started":"2026-01-09T19:09:49.384350Z","shell.execute_reply":"2026-01-09T19:36:59.054285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results (Leakage-safe by well, v3_1)\n\n* **Logistic Regression (repeated GroupShuffleSplit by well, 30 repeats):**\n\n  * **macro-F1 ≈ 0.20 ± 0.08**\n  * **fault-vs-normal F1 ≈ 0.61 ± 0.24**\n    **Interpretation:** weak and unstable under well-level splits; often fails to separate *Normal (0)* from dominant fault patterns reliably.\n\n* **HistGradientBoosting (class-balanced sample weights, repeated GroupShuffleSplit by well, 30 repeats):**\n\n  * **macro-F1 ≈ 0.39 ± 0.11**\n  * **fault-vs-normal F1 ≈ 0.89 ± 0.14**\n    **Interpretation:** clearly stronger and more consistent; performs very well as a *fault detector* (0 vs non-0), while fine-grained fault typing remains limited by rare-class scarcity.\n\n* **Single final holdout (20% wells; 32 train wells / 8 holdout wells):**\n\n  * **LogReg:** macro-F1 **0.246**, fault-vs-normal F1 **0.356**\n  * **HGB:** macro-F1 **0.464**, fault-vs-normal F1 **0.961**, accuracy **0.90**\n    **Interpretation:** HGB generalizes well to unseen wells on this split; high accuracy is partly driven by holdout being dominated by classes **0** and **4**, so macro-F1 and per-class recall are the more honest indicators.\n\n* **Note on macro-F1 instability / limits:**\n  Macro-F1 is inherently volatile here because several classes are extremely rare (e.g., **class 1 has 4 files total; class 6 has 6**), and some holdout splits contain **zero** examples for certain classes. Most remaining errors are confusions among rarer fault types (e.g., **7→9**, **5→2** on the holdout), which is expected with minimal support.\n","metadata":{}}]}