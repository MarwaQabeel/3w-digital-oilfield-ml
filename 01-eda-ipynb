{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9353254,"sourceType":"datasetVersion","datasetId":988298}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# 3W Dataset (WELL-only) — Row-per-file Classification (v3_1)\n# Fully runnable, leakage-safe, interview-ready\n# ============================================================\n\nimport os, json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\nfrom sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score, classification_report\n\n# -------------------------\n# Config\n# -------------------------\nBASE = \"/kaggle/input/3w-dataset/2.0.0\"\nRANDOM_STATE = 42\nN_WELL_FILES = None  # None = all WELL files\n\nOUT_DIR = \"/kaggle/working/3w_prepared_v3_1_fixed\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nCACHE_PATH = f\"{OUT_DIR}/df_ml_well_v3_1_fixed.parquet\"\nprint(\"OUT_DIR:\", OUT_DIR)\nprint(\"CACHE_PATH:\", CACHE_PATH)\n\n# -------------------------\n# Dataset constants / mappings\n# -------------------------\nVAR_RENAME = {\n    \"ABER-CKGL\": \"gl_choke_opening_pct\",\n    \"ABER-CKP\":  \"prod_choke_opening_pct\",\n    \"ESTADO-DHSV\":   \"dhsv_state\",\n    \"ESTADO-M1\":     \"prod_master_valve_state\",\n    \"ESTADO-M2\":     \"ann_master_valve_state\",\n    \"ESTADO-PXO\":    \"pig_crossover_valve_state\",\n    \"ESTADO-SDV-GL\": \"gl_shutdown_valve_state\",\n    \"ESTADO-SDV-P\":  \"prod_shutdown_valve_state\",\n    \"ESTADO-W1\":     \"prod_wing_valve_state\",\n    \"ESTADO-W2\":     \"ann_wing_valve_state\",\n    \"ESTADO-XO\":     \"crossover_valve_state\",\n    \"P-ANULAR\":     \"annulus_pressure_pa\",\n    \"P-JUS-BS\":     \"svc_pump_downstream_pressure_pa\",\n    \"P-JUS-CKGL\":   \"gl_choke_downstream_pressure_pa\",\n    \"P-JUS-CKP\":    \"prod_choke_downstream_pressure_pa\",\n    \"P-MON-CKGL\":   \"gl_choke_upstream_pressure_pa\",\n    \"P-MON-CKP\":    \"prod_choke_upstream_pressure_pa\",\n    \"P-MON-SDV-P\":  \"prod_sdv_upstream_pressure_pa\",\n    \"P-PDG\":        \"pdg_downhole_pressure_pa\",\n    \"PT-P\":         \"xmas_tree_prod_line_pressure_pa\",\n    \"P-TPT\":        \"tpt_pressure_pa\",\n    \"QBS\": \"svc_pump_flow_m3s\",\n    \"QGL\": \"gas_lift_flow_m3s\",\n    \"T-JUS-CKP\": \"prod_choke_downstream_temp_c\",\n    \"T-MON-CKP\": \"prod_choke_upstream_temp_c\",\n    \"T-PDG\":     \"pdg_downhole_temp_c\",\n    \"T-TPT\":     \"tpt_temp_c\",\n    \"class\": \"class_code\",\n    \"state\": \"state_code\",\n}\n\nEVENT_TYPE_CODE_TO_NAME = {\n    0:\"Normal Operation\", 1:\"Abrupt Increase of BSW\", 2:\"Spurious Closure of DHSV\",\n    3:\"Severe Slugging\", 4:\"Flow Instability\", 5:\"Rapid Productivity Loss\",\n    6:\"Quick Restriction in PCK\", 7:\"Scaling in PCK\",\n    8:\"Hydrate in Production Line\", 9:\"Hydrate in Service Line\",\n}\n\nLABEL_COLS = {\"class_code\", \"state_code\", \"class_label\", \"state_label\"}\n\n# ============================================================\n# 1) Build file index\n# ============================================================\ndef build_file_index(base: str) -> pd.DataFrame:\n    paths = []\n    for root, _, files in os.walk(base):\n        for f in files:\n            if f.endswith(\".parquet\"):\n                paths.append(os.path.join(root, f))\n\n    df = pd.DataFrame({\"path\": paths})\n    # event type code is in folder name /2.0.0/<code>/\n    codes = df[\"path\"].str.extract(r\"/2\\.0\\.0/(\\d+)/\", expand=False)\n    df[\"event_type_code\"] = pd.to_numeric(codes, errors=\"coerce\").astype(\"Int64\")\n    df = df.dropna(subset=[\"event_type_code\"])\n    df[\"event_type_code\"] = df[\"event_type_code\"].astype(int)\n\n    df[\"file\"] = df[\"path\"].str.split(\"/\").str[-1]\n    df[\"source\"] = df[\"file\"].str.extract(r\"^(WELL|SIMULATED|DRAWN)\")\n    df[\"well_id\"] = df[\"file\"].str.extract(r\"(WELL-\\d+)\")\n    df[\"run_ts\"] = df[\"file\"].str.extract(r\"_(\\d{14})\")\n    df[\"run_ts\"] = pd.to_datetime(df[\"run_ts\"], format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n\n    return df.sort_values([\"event_type_code\",\"source\",\"well_id\",\"run_ts\"]).reset_index(drop=True)\n\ndf_files = build_file_index(BASE)\ndf_w_files = df_files[df_files[\"source\"] == \"WELL\"].copy()\nassert df_w_files[\"well_id\"].notna().all()\n\nprint(\"Total files:\", len(df_files), \"| WELL files:\", len(df_w_files), \"| Wells:\", df_w_files[\"well_id\"].nunique())\nprint(\"Class counts:\\n\", df_w_files[\"event_type_code\"].value_counts().sort_index())\n\n# ============================================================\n# 2) Cleaning\n# ============================================================\ndef clean_3w_instance(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"timestamp\" in df.columns:\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n        df = df.set_index(\"timestamp\")\n    else:\n        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n\n    df = df[~df.index.isna()].sort_index()\n    df.index.name = \"timestamp\"\n\n    df = df.rename(columns=VAR_RENAME)\n\n    for c in df.columns:\n        if c in (\"class_code\", \"state_code\"):\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int16\")\n        else:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n    return df\n\n# ============================================================\n# 3) Feature extraction (fixed z_last + time-window first/last)\n# ============================================================\ndef summarize_timeseries_v3_1(df_clean: pd.DataFrame, frac: float = 0.1, state_max: int = 3) -> dict:\n    sensors = df_clean.drop(columns=list(LABEL_COLS), errors=\"ignore\")\n    num = sensors.select_dtypes(include=[np.number])\n\n    out = {\n        \"n_obs\": int(len(df_clean)),\n        \"duration_s\": float((df_clean.index.max() - df_clean.index.min()).total_seconds())\n                      if len(df_clean) else np.nan,\n    }\n    if num.shape[1] == 0 or len(num) == 0:\n        return out\n\n    state_cols = [c for c in num.columns if c.endswith(\"_state\")]\n    cont_cols  = [c for c in num.columns if c not in state_cols]\n\n    # Helper: first/last window by TIME (fallback to row-count)\n    def _first_last_masks(index, frac: float, n: int):\n        k = max(1, int(n * frac))\n        if n < 2:\n            m = np.ones(n, dtype=bool)\n            return m, m\n\n        # Try to treat index as datetime\n        try:\n            idx = pd.DatetimeIndex(index)\n        except Exception:\n            idx = None\n\n        if idx is not None and idx.notna().all():\n            tmin, tmax = idx.min(), idx.max()\n            dur_s = (tmax - tmin).total_seconds()\n            if np.isfinite(dur_s) and dur_s > 0:\n                w = dur_s * frac\n                t_first_end = tmin + pd.Timedelta(seconds=w)\n                t_last_start = tmax - pd.Timedelta(seconds=w)\n\n                # IMPORTANT: comparison may already be numpy arrays → wrap with np.asarray\n                first_mask = np.asarray(idx <= t_first_end, dtype=bool)\n                last_mask  = np.asarray(idx >= t_last_start, dtype=bool)\n\n                # fallback if empty windows\n                if first_mask.sum() == 0:\n                    first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n                if last_mask.sum() == 0:\n                    last_mask = np.zeros(n, dtype=bool); last_mask[-k:] = True\n\n                return first_mask, last_mask\n\n        # row-based fallback\n        first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n        last_mask  = np.zeros(n, dtype=bool); last_mask[-k:] = True\n        return first_mask, last_mask\n\n    # Continuous sensors\n    if len(cont_cols):\n        cont = num[cont_cols]\n        med_raw = cont.median()\n        iqr_raw = (cont.quantile(0.75) - cont.quantile(0.25))\n\n        iqr = iqr_raw.replace(0, np.nan)\n        z = (cont - med_raw) / iqr\n\n        first_mask, last_mask = _first_last_masks(df_clean.index, frac=frac, n=len(z))\n\n        # Safer than z.index[first_mask]\n        first = z.iloc[first_mask].mean()\n        last  = z.iloc[last_mask].mean()\n\n        agg = z.agg([\"mean\", \"std\", \"min\", \"max\"]).T\n        miss = cont.isna().mean()\n\n        for col in cont_cols:\n            out[f\"{col}__raw_median\"] = med_raw[col]\n            out[f\"{col}__raw_iqr\"]    = iqr_raw[col]\n\n            s_raw = cont[col].dropna()\n            out[f\"{col}__raw_last\"] = s_raw.iloc[-1] if len(s_raw) else np.nan\n\n            out[f\"{col}__z_mean\"] = agg.loc[col, \"mean\"]\n            out[f\"{col}__z_std\"]  = agg.loc[col, \"std\"]\n            out[f\"{col}__z_min\"]  = agg.loc[col, \"min\"]\n            out[f\"{col}__z_max\"]  = agg.loc[col, \"max\"]\n\n            # ✅ fix: last VALID z (not last row)\n            s_z = z[col].dropna()\n            out[f\"{col}__z_last\"] = s_z.iloc[-1] if len(s_z) else np.nan\n\n            out[f\"{col}__delta_last_first\"] = (last[col] - first[col])\n            out[f\"{col}__abs_delta\"]        = abs(last[col] - first[col])\n            out[f\"{col}__missing_frac\"]     = miss[col]\n\n    # State-like sensors\n    if len(state_cols):\n        st = num[state_cols]\n        for col in state_cols:\n            s = st[col]\n            s_non = s.dropna()\n\n            out[f\"{col}__missing_frac\"] = float(s.isna().mean())\n            out[f\"{col}__last\"] = float(s_non.iloc[-1]) if len(s_non) else np.nan\n\n            if len(s_non) >= 2:\n                n_trans = int((s_non != s_non.shift()).sum() - 1)\n            else:\n                n_trans = 0\n\n            out[f\"{col}__n_transitions\"] = n_trans\n            out[f\"{col}__transitions_rate\"] = n_trans / max(1, len(s_non))\n\n            known = 0.0\n            for v in range(state_max + 1):\n                p = float((s_non == v).mean()) if len(s_non) else np.nan\n                out[f\"{col}__p_state_{v}\"] = p\n                if not np.isnan(p):\n                    known += p\n            out[f\"{col}__p_state_other\"] = (1.0 - known) if len(s_non) else np.nan\n\n    return out\n\n# ============================================================\n# 4) Build row-per-file dataset (with caching)\n# ============================================================\ndef build_row_per_file_dataset(df_files: pd.DataFrame, n_files: int | None = None, random_state: int = 42) -> pd.DataFrame:\n    if n_files is None:\n        sample = df_files.reset_index(drop=True)\n    else:\n        sample = df_files.sample(n_files, random_state=random_state).reset_index(drop=True)\n\n    rows = []\n    for _, r in tqdm(sample.iterrows(), total=len(sample), desc=\"Building WELL dataset\"):\n        df_raw = pd.read_parquet(r[\"path\"])\n        df_clean = clean_3w_instance(df_raw)\n\n        feats = summarize_timeseries_v3_1(df_clean)\n        feats[\"event_type_code\"] = int(r[\"event_type_code\"])\n        feats[\"event_type_name\"] = EVENT_TYPE_CODE_TO_NAME.get(int(r[\"event_type_code\"]), \"Unknown\")\n        feats[\"well_id\"] = r[\"well_id\"]\n        feats[\"run_ts\"] = r[\"run_ts\"]\n        feats[\"file\"] = r[\"file\"]\n        rows.append(feats)\n\n    return pd.DataFrame(rows)\n\nif os.path.exists(CACHE_PATH):\n    df_ml_well = pd.read_parquet(CACHE_PATH)\n    print(\"Loaded cached features:\", df_ml_well.shape)\nelse:\n    df_ml_well = build_row_per_file_dataset(df_w_files, n_files=N_WELL_FILES, random_state=RANDOM_STATE)\n    df_ml_well.to_parquet(CACHE_PATH, index=False)\n    print(\"Built + saved features:\", df_ml_well.shape)\n\nwith open(f\"{OUT_DIR}/dataset_config.json\", \"w\") as f:\n    json.dump({\n        \"base\": BASE,\n        \"random_state\": RANDOM_STATE,\n        \"n_well_files_used\": int(len(df_ml_well)),\n        \"features_version\": \"row_per_file_v3_1_fixed\",\n        \"notes\": \"continuous: raw median/iqr/last + robust-z stats + deltas; state: last + transitions + proportions (+other); WELL-only; fixes: time-window first/last + z_last last-valid\"\n    }, f, indent=2)\n\n# ============================================================\n# 5) Build X/y/groups (NO global filtering leakage)\n# ============================================================\ndef make_Xy_groups(df: pd.DataFrame):\n    y = df[\"event_type_code\"].astype(int).copy()\n    groups = df[\"well_id\"].astype(str).copy()\n    drop_cols = [\"event_type_code\",\"event_type_name\",\"file\",\"run_ts\",\"well_id\"]\n    X = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n    X = X.replace([np.inf, -np.inf], np.nan)\n    return X, y, groups\n\nX, y, groups = make_Xy_groups(df_ml_well)\nprint(\"X:\", X.shape, \"| y:\", y.shape, \"| wells:\", groups.nunique())\nprint(\"Label counts:\\n\", y.value_counts().sort_index())\n\n# ============================================================\n# 6) Train-only column filter (inside pipeline) — fixes leakage\n# ============================================================\nclass TrainOnlyColumnFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, missing_threshold: float = 0.98):\n        self.missing_threshold = missing_threshold\n\n    def fit(self, X, y=None):\n        X = X.copy().replace([np.inf, -np.inf], np.nan)\n\n        all_missing = X.columns[X.isna().all()]\n        miss = X.isna().mean()\n        high_missing = miss[miss > self.missing_threshold].index\n        const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n\n        drop = set(all_missing) | set(high_missing) | set(const_cols)\n        self.keep_columns_ = [c for c in X.columns if c not in drop]\n        return self\n\n    def transform(self, X):\n        X = X.copy().replace([np.inf, -np.inf], np.nan)\n        for c in getattr(self, \"keep_columns_\", []):\n            if c not in X.columns:\n                X[c] = np.nan\n        return X[self.keep_columns_]\n\n# ============================================================\n# 7) Proper cloneable weighted HGB estimator\n# ============================================================\nclass WeightedHGBClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(\n        self,\n        max_depth=6,\n        learning_rate=0.06,\n        max_iter=700,\n        max_leaf_nodes=31,\n        min_samples_leaf=20,\n        l2_regularization=0.1,\n        random_state=42,\n    ):\n        self.max_depth = max_depth\n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.max_leaf_nodes = max_leaf_nodes\n        self.min_samples_leaf = min_samples_leaf\n        self.l2_regularization = l2_regularization\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        self.clf_ = HistGradientBoostingClassifier(\n            max_depth=self.max_depth,\n            learning_rate=self.learning_rate,\n            max_iter=self.max_iter,\n            max_leaf_nodes=self.max_leaf_nodes,\n            min_samples_leaf=self.min_samples_leaf,\n            l2_regularization=self.l2_regularization,\n            random_state=self.random_state,\n        )\n\n        y_arr = np.asarray(y)\n        classes = np.unique(y_arr)\n        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_arr)\n        cw_map = dict(zip(classes, cw))\n        sample_weight = np.array([cw_map[v] for v in y_arr], dtype=float)\n\n        self.clf_.fit(X, y_arr, sample_weight=sample_weight)\n        self.classes_ = classes\n        return self\n\n    def predict(self, X):\n        return self.clf_.predict(X)\n\n# ============================================================\n# 8) Evaluation (fixed labels; major classes from train per fold)\n# ============================================================\ndef evaluate_stratified_group_cv(model, X, y, groups, n_splits=4, random_state=42, major_min_support=10):\n    labels = np.sort(pd.unique(y))\n    cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    macro_f1s, major_f1s, bin_f1s = [], [], []\n    per_class_f1 = {int(lbl): [] for lbl in labels}\n\n    for fold, (tr, te) in enumerate(cv.split(X, y, groups=groups), start=1):\n        Xtr, Xte = X.iloc[tr], X.iloc[te]\n        ytr, yte = y.iloc[tr], y.iloc[te]\n\n        m = clone(model)\n        m.fit(Xtr, ytr)\n        pred = m.predict(Xte)\n\n        macro = f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0)\n        macro_f1s.append(macro)\n\n        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n        if len(major_classes) > 0:\n            major_labels = np.array(sorted(major_classes))\n            major_mask = yte.isin(major_labels)\n            if major_mask.any():\n                major = f1_score(\n                    yte[major_mask], pred[major_mask],\n                    average=\"macro\", labels=major_labels, zero_division=0\n                )\n            else:\n                major = np.nan\n        else:\n            major = np.nan\n        major_f1s.append(major)\n\n        yte_bin = (yte != 0).astype(int)\n        pred_bin = (pred != 0).astype(int)\n        bin_f1s.append(f1_score(yte_bin, pred_bin, zero_division=0))\n\n        rep = classification_report(yte, pred, labels=labels, output_dict=True, zero_division=0)\n        for lbl in labels:\n            per_class_f1[int(lbl)].append(rep[str(int(lbl))][\"f1-score\"])\n\n    return {\n        \"macro_f1_mean\": float(np.nanmean(macro_f1s)),\n        \"macro_f1_std\":  float(np.nanstd(macro_f1s)),\n        \"major_macro_f1_mean\": float(np.nanmean(major_f1s)),\n        \"fault_vs_normal_f1_mean\": float(np.nanmean(bin_f1s)),\n        \"per_class_f1_mean\": {k: float(np.nanmean(v)) for k, v in per_class_f1.items()},\n    }\n\ndef evaluate_on_holdout(model, Xtr, ytr, Xho, yho, major_min_support=10):\n    labels = np.sort(pd.unique(pd.concat([ytr, yho], axis=0)))\n\n    m = clone(model)\n    m.fit(Xtr, ytr)\n    pred = m.predict(Xho)\n\n    macro = f1_score(yho, pred, average=\"macro\", labels=labels, zero_division=0)\n\n    major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n    major_labels = np.array(sorted(major_classes)) if len(major_classes) else np.array([], dtype=int)\n    if len(major_labels) and yho.isin(major_labels).any():\n        major = f1_score(\n            yho[yho.isin(major_labels)], pred[yho.isin(major_labels)],\n            average=\"macro\", labels=major_labels, zero_division=0\n        )\n    else:\n        major = np.nan\n\n    yho_bin = (yho != 0).astype(int)\n    pred_bin = (pred != 0).astype(int)\n    bin_f1 = f1_score(yho_bin, pred_bin, zero_division=0)\n\n    report = classification_report(yho, pred, labels=labels, zero_division=0)\n\n    return {\n        \"macro_f1\": float(macro),\n        \"major_macro_f1\": float(major) if np.isfinite(major) else np.nan,\n        \"fault_vs_normal_f1\": float(bin_f1),\n        \"classification_report\": report,\n    }\n\n# ============================================================\n# 9) Models (consistent preprocessing)\n# ============================================================\npreprocess_common = [\n    (\"col_filter\", TrainOnlyColumnFilter(missing_threshold=0.98)),\n    (\"imputer\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n]\n\nlogreg = Pipeline(preprocess_common + [\n    (\"scaler\", StandardScaler(with_mean=False)),\n    (\"clf\", LogisticRegression(max_iter=8000, class_weight=\"balanced\"))\n])\n\nhgb = Pipeline(preprocess_common + [\n    (\"clf\", WeightedHGBClassifier(\n        max_depth=6,\n        learning_rate=0.06,\n        max_iter=700,\n        max_leaf_nodes=31,\n        min_samples_leaf=20,\n        l2_regularization=0.1,\n        random_state=RANDOM_STATE\n    ))\n])\n\n# ============================================================\n# 10) Final holdout by WELL + CV on train only (reduces selection leakage)\n# ============================================================\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\ntr_idx, ho_idx = next(gss.split(X, y, groups=groups))\n\nX_tr, y_tr, g_tr = X.iloc[tr_idx], y.iloc[tr_idx], groups.iloc[tr_idx]\nX_ho, y_ho, g_ho = X.iloc[ho_idx], y.iloc[ho_idx], groups.iloc[ho_idx]\n\nprint(\"\\nTrain wells:\", g_tr.nunique(), \"| Holdout wells:\", g_ho.nunique())\nprint(\"Holdout label counts:\\n\", y_ho.value_counts().sort_index())\n\n# n_splits feasibility with extreme imbalance\nmin_support = int(y_tr.value_counts().min())\nn_splits = min(4, min_support)   # try up to 4 folds\nn_splits = max(2, n_splits)      # at least 2 folds\n\nprint(f\"\\nUsing StratifiedGroupKFold with n_splits={n_splits} (min class support in train = {min_support})\")\n\nprint(\"\\n=== CV on TRAIN ONLY ===\")\nlogreg_cv = evaluate_stratified_group_cv(logreg, X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\nhgb_cv    = evaluate_stratified_group_cv(hgb,   X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\n\nprint(\"LogReg CV:\", logreg_cv)\nprint(\"HGB   CV:\", hgb_cv)\n\nprint(\"\\n=== FINAL HOLDOUT (train wells -> holdout wells) ===\")\nlogreg_hold = evaluate_on_holdout(logreg, X_tr, y_tr, X_ho, y_ho)\nhgb_hold    = evaluate_on_holdout(hgb,   X_tr, y_tr, X_ho, y_ho)\n\nprint(\"LogReg Holdout:\", {k: v for k, v in logreg_hold.items() if k != \"classification_report\"})\nprint(\"HGB   Holdout:\", {k: v for k, v in hgb_hold.items() if k != \"classification_report\"})\n\nprint(\"\\nLogReg classification report (holdout):\\n\", logreg_hold[\"classification_report\"])\nprint(\"\\nHGB classification report (holdout):\\n\", hgb_hold[\"classification_report\"])\n\n# Optional: save results\nwith open(f\"{OUT_DIR}/results_summary.json\", \"w\") as f:\n    json.dump({\n        \"logreg_cv\": logreg_cv,\n        \"hgb_cv\": hgb_cv,\n        \"logreg_holdout\": {k: v for k, v in logreg_hold.items() if k != \"classification_report\"},\n        \"hgb_holdout\": {k: v for k, v in hgb_hold.items() if k != \"classification_report\"},\n        \"notes\": {\n            \"leakage_control\": [\n                \"train-only column filtering inside pipeline\",\n                \"final group holdout by well\",\n                \"CV performed on train wells only\",\n                \"major classes computed from y_train per fold\",\n                \"fixed-label macro f1 with zero_division=0\",\n            ]\n        }\n    }, f, indent=2)\n\nprint(f\"\\nSaved: {OUT_DIR}/results_summary.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:21:00.640245Z","iopub.execute_input":"2026-01-04T11:21:00.640608Z"}},"outputs":[{"name":"stdout","text":"OUT_DIR: /kaggle/working/3w_prepared_v3_1_fixed\nCACHE_PATH: /kaggle/working/3w_prepared_v3_1_fixed/df_ml_well_v3_1_fixed.parquet\nTotal files: 2228 | WELL files: 1119 | Wells: 40\nClass counts:\n event_type_code\n0    594\n1      4\n2     22\n3     32\n4    343\n5     11\n6      6\n7     36\n8     14\n9     57\nName: count, dtype: int64\nLoaded cached features: (1119, 286)\nX: (1119, 281) | y: (1119,) | wells: 40\nLabel counts:\n event_type_code\n0    594\n1      4\n2     22\n3     32\n4    343\n5     11\n6      6\n7     36\n8     14\n9     57\nName: count, dtype: int64\n\nTrain wells: 32 | Holdout wells: 8\nHoldout label counts:\n event_type_code\n0    82\n2     1\n4    38\n5     4\n7     8\n8     6\n9     4\nName: count, dtype: int64\n\nUsing StratifiedGroupKFold with n_splits=4 (min class support in train = 4)\n\n=== CV on TRAIN ONLY ===\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os, json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom sklearn.model_selection import StratifiedGroupKFold, GroupShuffleSplit\nfrom sklearn.metrics import f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\n\nBASE = \"/kaggle/input/3w-dataset/2.0.0\"\nRANDOM_STATE = 42\nN_WELL_FILES = None  # None = all WELL files\n\nOUT_DIR = \"/kaggle/working/3w_prepared_v3_1\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nCACHE_PATH = f\"{OUT_DIR}/df_ml_well_v3_1.parquet\"\nprint(\"OUT_DIR:\", OUT_DIR)\n","metadata":{"_uuid":"d0ade4b7-7960-4103-9c19-458aa1cbc34f","_cell_guid":"a7c6a5d1-8959-4210-8ad1-1a97420747f2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3W Dataset (WELL-only) — Row-per-file Classification (v3_1)\n\n## Summary (Engineer Story)\n- Built a file index from the 3W dataset and filtered to **WELL files only** (1119 files, 40 wells).\n- Cleaned each file: parsed timestamp index, renamed sensors, converted to numeric types.\n- Engineered **row-per-file features**:\n  - Continuous sensors: raw median/IQR/last + robust z-stats (mean/std/min/max/last) + delta(first→last) + missing ratio.\n  - State/valve signals: last state + transition count/rate + time-in-state proportions (+ “other” state).\n- Evaluated with **Group splits by `well_id`** to avoid leakage across wells (harder but realistic).\n- Compared baseline (Logistic Regression) vs boosting (HistGradientBoosting with class-weighted sample_weight).\n- Result: **HGB outperformed baseline**, reaching about **macro-F1 ~0.42 (repeated group shuffle)** and **fault-vs-normal F1 ~0.76**, with variance due to very rare classes.\n","metadata":{}},{"cell_type":"code","source":"def build_file_index(base: str) -> pd.DataFrame:\n    paths = []\n    for root, _, files in os.walk(base):\n        for f in files:\n            if f.endswith(\".parquet\"):\n                paths.append(os.path.join(root, f))\n\n    df = pd.DataFrame({\"path\": paths})\n    codes = df[\"path\"].str.extract(r\"/2\\.0\\.0/(\\d+)/\", expand=False)\n    df[\"event_type_code\"] = pd.to_numeric(codes, errors=\"coerce\").astype(\"Int64\")\n    df = df.dropna(subset=[\"event_type_code\"])\n    df[\"event_type_code\"] = df[\"event_type_code\"].astype(int)\n    df[\"file\"] = df[\"path\"].str.split(\"/\").str[-1]\n    df[\"source\"] = df[\"file\"].str.extract(r\"^(WELL|SIMULATED|DRAWN)\")\n    df[\"well_id\"] = df[\"file\"].str.extract(r\"(WELL-\\d+)\")\n    df[\"run_ts\"] = df[\"file\"].str.extract(r\"_(\\d{14})\")\n    df[\"run_ts\"] = pd.to_datetime(df[\"run_ts\"], format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n    return df.sort_values([\"event_type_code\",\"source\",\"well_id\",\"run_ts\"]).reset_index(drop=True)\n\ndf_files = build_file_index(BASE)\ndf_w_files = df_files[df_files[\"source\"]==\"WELL\"].copy()\nassert df_w_files[\"well_id\"].notna().all()\n\n\nprint(\"Total files:\", len(df_files), \"| WELL files:\", len(df_w_files), \"| Wells:\", df_w_files[\"well_id\"].nunique())\ndisplay(df_w_files[\"event_type_code\"].value_counts().sort_index())\n","metadata":{"_uuid":"b5ce1d34-f068-49cd-a840-6f1a42682c0c","_cell_guid":"43dd3beb-1e81-4aa6-ae49-562ace3591fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VAR_RENAME = {\n    \"ABER-CKGL\": \"gl_choke_opening_pct\",\n    \"ABER-CKP\":  \"prod_choke_opening_pct\",\n    \"ESTADO-DHSV\":   \"dhsv_state\",\n    \"ESTADO-M1\":     \"prod_master_valve_state\",\n    \"ESTADO-M2\":     \"ann_master_valve_state\",\n    \"ESTADO-PXO\":    \"pig_crossover_valve_state\",\n    \"ESTADO-SDV-GL\": \"gl_shutdown_valve_state\",\n    \"ESTADO-SDV-P\":  \"prod_shutdown_valve_state\",\n    \"ESTADO-W1\":     \"prod_wing_valve_state\",\n    \"ESTADO-W2\":     \"ann_wing_valve_state\",\n    \"ESTADO-XO\":     \"crossover_valve_state\",\n    \"P-ANULAR\":     \"annulus_pressure_pa\",\n    \"P-JUS-BS\":     \"svc_pump_downstream_pressure_pa\",\n    \"P-JUS-CKGL\":   \"gl_choke_downstream_pressure_pa\",\n    \"P-JUS-CKP\":    \"prod_choke_downstream_pressure_pa\",\n    \"P-MON-CKGL\":   \"gl_choke_upstream_pressure_pa\",\n    \"P-MON-CKP\":    \"prod_choke_upstream_pressure_pa\",\n    \"P-MON-SDV-P\":  \"prod_sdv_upstream_pressure_pa\",\n    \"P-PDG\":        \"pdg_downhole_pressure_pa\",\n    \"PT-P\":         \"xmas_tree_prod_line_pressure_pa\",\n    \"P-TPT\":        \"tpt_pressure_pa\",\n    \"QBS\": \"svc_pump_flow_m3s\",\n    \"QGL\": \"gas_lift_flow_m3s\",\n    \"T-JUS-CKP\": \"prod_choke_downstream_temp_c\",\n    \"T-MON-CKP\": \"prod_choke_upstream_temp_c\",\n    \"T-PDG\":     \"pdg_downhole_temp_c\",\n    \"T-TPT\":     \"tpt_temp_c\",\n    \"class\": \"class_code\",\n    \"state\": \"state_code\",\n}\n\nEVENT_TYPE_CODE_TO_NAME = {\n    0:\"Normal Operation\", 1:\"Abrupt Increase of BSW\", 2:\"Spurious Closure of DHSV\",\n    3:\"Severe Slugging\", 4:\"Flow Instability\", 5:\"Rapid Productivity Loss\",\n    6:\"Quick Restriction in PCK\", 7:\"Scaling in PCK\",\n    8:\"Hydrate in Production Line\", 9:\"Hydrate in Service Line\",\n}\n\nLABEL_COLS = {\"class_code\", \"state_code\", \"class_label\", \"state_label\"}\n\ndef clean_3w_instance(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"timestamp\" in df.columns:\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n        df = df.set_index(\"timestamp\")\n    else:\n        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n\n    df = df[~df.index.isna()].sort_index()\n    df.index.name = \"timestamp\"\n    df = df.rename(columns=VAR_RENAME)\n\n    for c in df.columns:\n        if c in (\"class_code\", \"state_code\"):\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int16\")\n        else:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n    return df\n\ndef summarize_timeseries_v3_1(df_clean: pd.DataFrame, frac: float = 0.1, state_max: int = 3) -> dict:\n    sensors = df_clean.drop(columns=list(LABEL_COLS), errors=\"ignore\")\n    num = sensors.select_dtypes(include=[np.number])\n\n    out = {\n        \"n_obs\": int(len(df_clean)),\n        \"duration_s\": float((df_clean.index.max() - df_clean.index.min()).total_seconds())\n                      if len(df_clean) else np.nan,\n    }\n    if num.shape[1] == 0 or len(num) == 0:\n        return out\n\n    state_cols = [c for c in num.columns if c.endswith(\"_state\")]\n    cont_cols  = [c for c in num.columns if c not in state_cols]\n\n    if len(cont_cols):\n        cont = num[cont_cols]\n        med_raw = cont.median()\n        iqr_raw = (cont.quantile(0.75) - cont.quantile(0.25))\n\n        iqr = iqr_raw.replace(0, np.nan)\n        z = (cont - med_raw) / iqr\n\n        k = max(1, int(len(z) * frac))\n        first = z.iloc[:k].mean()\n        last  = z.iloc[-k:].mean()\n\n        agg = z.agg([\"mean\",\"std\",\"min\",\"max\"]).T\n        miss = cont.isna().mean()\n\n        for col in cont_cols:\n            out[f\"{col}__raw_median\"] = med_raw[col]\n            out[f\"{col}__raw_iqr\"]    = iqr_raw[col]\n            s = cont[col].dropna()\n            out[f\"{col}__raw_last\"] = s.iloc[-1] if len(s) else np.nan\n\n            out[f\"{col}__z_mean\"] = agg.loc[col, \"mean\"]\n            out[f\"{col}__z_std\"]  = agg.loc[col, \"std\"]\n            out[f\"{col}__z_min\"]  = agg.loc[col, \"min\"]\n            out[f\"{col}__z_max\"]  = agg.loc[col, \"max\"]\n            out[f\"{col}__z_last\"] = z[col].iloc[-1]\n\n            out[f\"{col}__delta_last_first\"] = (last[col] - first[col])\n            out[f\"{col}__abs_delta\"]        = abs(last[col] - first[col])\n            out[f\"{col}__missing_frac\"]     = miss[col]\n\n    if len(state_cols):\n        st = num[state_cols]\n        for col in state_cols:\n            s = st[col]\n            s_non = s.dropna()\n\n            out[f\"{col}__missing_frac\"] = float(s.isna().mean())\n            out[f\"{col}__last\"] = float(s_non.iloc[-1]) if len(s_non) else np.nan\n\n            if len(s_non) >= 2:\n                n_trans = int((s_non != s_non.shift()).sum() - 1)\n            else:\n                n_trans = 0\n\n            out[f\"{col}__n_transitions\"] = n_trans\n            out[f\"{col}__transitions_rate\"] = n_trans / max(1, len(s_non))\n\n            known = 0.0\n            for v in range(state_max + 1):\n                p = float((s_non == v).mean()) if len(s_non) else np.nan\n                out[f\"{col}__p_state_{v}\"] = p\n                if not np.isnan(p):\n                    known += p\n            out[f\"{col}__p_state_other\"] = (1.0 - known) if len(s_non) else np.nan\n\n    return out\n","metadata":{"_uuid":"ccb30fdb-e518-4f00-a9a7-92d2839e785d","_cell_guid":"7d3f90e3-30d8-4693-be35-bc8c37ad4f94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_row_per_file_dataset(df_files: pd.DataFrame, n_files: int | None = None, random_state: int = 42) -> pd.DataFrame:\n    if n_files is None:\n        sample = df_files.reset_index(drop=True)\n    else:\n        sample = df_files.sample(n_files, random_state=random_state).reset_index(drop=True)\n\n    rows = []\n    for _, r in tqdm(sample.iterrows(), total=len(sample), desc=\"Building WELL dataset\"):\n        df_raw = pd.read_parquet(r[\"path\"])\n        df_clean = clean_3w_instance(df_raw)\n\n        feats = summarize_timeseries_v3_1(df_clean)\n        feats[\"event_type_code\"] = int(r[\"event_type_code\"])\n        feats[\"event_type_name\"] = EVENT_TYPE_CODE_TO_NAME.get(int(r[\"event_type_code\"]), \"Unknown\")\n        feats[\"well_id\"] = r[\"well_id\"]\n        feats[\"run_ts\"] = r[\"run_ts\"]\n        feats[\"file\"] = r[\"file\"]\n        rows.append(feats)\n\n    return pd.DataFrame(rows)\n\nif os.path.exists(CACHE_PATH):\n    df_ml_well = pd.read_parquet(CACHE_PATH)\n    print(\"Loaded cached features:\", df_ml_well.shape)\nelse:\n    df_ml_well = build_row_per_file_dataset(df_w_files, n_files=N_WELL_FILES, random_state=RANDOM_STATE)\n    df_ml_well.to_parquet(CACHE_PATH, index=False)\n    print(\"Built + saved features:\", df_ml_well.shape)\n\nwith open(f\"{OUT_DIR}/dataset_config.json\", \"w\") as f:\n    json.dump({\n        \"base\": BASE,\n        \"random_state\": RANDOM_STATE,\n        \"n_well_files_used\": int(len(df_ml_well)),\n        \"features_version\": \"row_per_file_v3_1\",\n        \"notes\": \"continuous: raw median/iqr/last + robust-z stats + deltas; state: last + transitions + proportions (+other); WELL-only\"\n    }, f, indent=2)\n","metadata":{"_uuid":"00df3321-d196-4aa5-b89b-ec6a55e7336b","_cell_guid":"5d497b20-eeb4-40ac-b462-9dca2ca076e3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_Xy_groups(df: pd.DataFrame):\n    y = df[\"event_type_code\"].copy()\n    groups = df[\"well_id\"].copy()\n\n    drop_cols = [\"event_type_code\",\"event_type_name\",\"file\",\"run_ts\",\"well_id\"]\n    X = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n\n    X = X.replace([np.inf, -np.inf], np.nan)\n\n    X = X.drop(columns=X.columns[X.isna().all()])          # all-missing\n    miss = X.isna().mean()\n    X = X.drop(columns=miss[miss > 0.98].index)            # very high missing\n    const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n    X = X.drop(columns=const_cols)                          # constants\n\n    return X, y, groups\n\nX, y, groups = make_Xy_groups(df_ml_well)\nprint(\"X:\", X.shape, \"| y:\", y.shape, \"| wells:\", groups.nunique())\ndisplay(y.value_counts().sort_index())\n","metadata":{"_uuid":"2bf22d68-f92b-4b74-b046-05686ad32cc8","_cell_guid":"a5e900d0-cb5b-4f37-8984-ac7e340f4dcf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_repeated_group_shuffle(model, X, y, groups, repeats=30, test_size=0.2):\n    gss = GroupShuffleSplit(n_splits=repeats, test_size=test_size, random_state=42)\n    major_classes = set(y.value_counts()[lambda s: s >= 10].index)\n\n    f1s, f1_major, f1_bin = [], [], []\n    for tr, te in gss.split(X, y, groups=groups):\n        Xtr, Xte = X.iloc[tr], X.iloc[te]\n        ytr, yte = y.iloc[tr], y.iloc[te]\n\n        model.fit(Xtr, ytr)\n        pred = model.predict(Xte)\n\n        f1s.append(f1_score(yte, pred, average=\"macro\"))\n\n        mask = yte.isin(major_classes)\n        f1_major.append(f1_score(yte[mask], pred[mask], average=\"macro\") if mask.sum() else np.nan)\n\n        yte_bin = (yte != 0).astype(int)\n        pred_bin = (pred != 0).astype(int)\n        f1_bin.append(f1_score(yte_bin, pred_bin))\n\n    return {\n        \"macro_f1_mean\": float(np.nanmean(f1s)),\n        \"macro_f1_std\":  float(np.nanstd(f1s)),\n        \"major_macro_f1_mean\": float(np.nanmean(f1_major)),\n        \"fault_vs_normal_f1_mean\": float(np.nanmean(f1_bin)),\n    }\n\nlogreg = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=True)),\n    (\"scaler\", StandardScaler(with_mean=False)),\n    (\"clf\", LogisticRegression(max_iter=8000, class_weight=\"balanced\"))\n])\n\nclass HGBWrapper:\n    def __init__(self,\n                 max_depth=6,\n                 learning_rate=0.06,\n                 max_iter=700,\n                 max_leaf_nodes=31,\n                 min_samples_leaf=20,\n                 l2_regularization=0.1):\n        self.clf = HistGradientBoostingClassifier(\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            max_iter=max_iter,\n            max_leaf_nodes=max_leaf_nodes,\n            min_samples_leaf=min_samples_leaf,\n            l2_regularization=l2_regularization\n        )\n\n    def fit(self, X, y):\n        classes = np.unique(y)\n        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n        w = pd.Series(y).map(dict(zip(classes, cw))).to_numpy()\n        self.clf.fit(X, y, sample_weight=w)\n        return self\n\n    def predict(self, X):\n        return self.clf.predict(X.fillna(0))\n\nhgb = HGBWrapper()\n\nprint(\"LogReg:\", eval_repeated_group_shuffle(logreg, X, y, groups, repeats=30))\nprint(\"HGB   :\", eval_repeated_group_shuffle(hgb, X, y, groups, repeats=30))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results (Current)\n- Logistic Regression (repeated group shuffle, 30): macro-F1 ≈ 0.22, fault-vs-normal F1 ≈ 0.61\n- HistGradientBoosting (repeated group shuffle, 30): macro-F1 ≈ 0.42, fault-vs-normal F1 ≈ 0.76\n- Note: Macro-F1 is unstable because some classes have very few samples (e.g., class 1 has 4 files).\n","metadata":{}}]}