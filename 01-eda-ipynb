{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f2db21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T12:58:48.576880Z",
     "iopub.status.busy": "2026-01-04T12:58:48.576535Z",
     "iopub.status.idle": "2026-01-04T13:19:34.804662Z",
     "shell.execute_reply": "2026-01-04T13:19:34.803490Z"
    },
    "papermill": {
     "duration": 1246.240995,
     "end_time": "2026-01-04T13:19:34.809568",
     "exception": false,
     "start_time": "2026-01-04T12:58:48.568573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer\n",
      "CACHE_PATH: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer/df_ml_well_v3_1_fixed_timewin_zlast_hybridImputer.parquet\n",
      "Total files: 2228 | WELL files: 1119 | Wells: 40\n",
      "Class counts:\n",
      " event_type_code\n",
      "0    594\n",
      "1      4\n",
      "2     22\n",
      "3     32\n",
      "4    343\n",
      "5     11\n",
      "6      6\n",
      "7     36\n",
      "8     14\n",
      "9     57\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d477d70aec274c76805009b9e7878336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building WELL dataset:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built + saved features: (1119, 286)\n",
      "X: (1119, 281) | y: (1119,) | wells: 40\n",
      "Label counts:\n",
      " event_type_code\n",
      "0    594\n",
      "1      4\n",
      "2     22\n",
      "3     32\n",
      "4    343\n",
      "5     11\n",
      "6      6\n",
      "7     36\n",
      "8     14\n",
      "9     57\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train wells: 32 | Holdout wells: 8\n",
      "Holdout label counts:\n",
      " event_type_code\n",
      "0    82\n",
      "2     1\n",
      "4    38\n",
      "5     4\n",
      "7     8\n",
      "8     6\n",
      "9     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Using StratifiedGroupKFold with n_splits=4\n",
      "\n",
      "=== CV on TRAIN ONLY ===\n",
      "LogReg CV: {'macro_f1_mean': 0.2480699192254695, 'macro_f1_std': 0.11211145249103659, 'major_macro_f1_mean': 0.34582027055795184, 'fault_vs_normal_f1_mean': 0.6572586102781942, 'per_class_f1_mean': {'0': 0.02054794520547945, '1': 0.0, '2': 0.5986581920903955, '3': 0.0, '4': 0.35916489738145785, '5': 0.0, '6': 0.0, '7': 0.4297297297297298, '8': 0.3333333333333333, '9': 0.7392650945142991}}\n",
      "HGB   CV: {'macro_f1_mean': 0.414913949489049, 'macro_f1_std': 0.04739455413528661, 'major_macro_f1_mean': 0.6226991965158268, 'fault_vs_normal_f1_mean': 0.9249655129770701, 'per_class_f1_mean': {'0': 0.6873414934167565, '1': 0.08333333333333333, '2': 0.9074074074074073, '3': 0.0, '4': 0.8526504132709201, '5': 0.0, '6': 0.0, '7': 0.33461538461538465, '8': 0.5416666666666666, '9': 0.7421247961800218}}\n",
      "\n",
      "=== FINAL HOLDOUT (train wells -> holdout wells) ===\n",
      "LogReg Holdout: {'macro_f1': 0.24770784770784773, 'major_macro_f1': 0.37865364609550656, 'fault_vs_normal_f1': 0.375}\n",
      "HGB   Holdout: {'macro_f1': 0.46379380829345, 'major_macro_f1': 0.5952119027113056, 'fault_vs_normal_f1': 0.9606299212598425}\n",
      "\n",
      "LogReg classification report (holdout):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        82\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.11      0.26      0.15        38\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       1.00      0.38      0.55         8\n",
      "           8       0.67      0.33      0.44         6\n",
      "           9       0.25      0.50      0.33         4\n",
      "\n",
      "    accuracy                           0.13       143\n",
      "   macro avg       0.30      0.25      0.25       143\n",
      "weighted avg       0.13      0.13      0.11       143\n",
      "\n",
      "\n",
      "HGB classification report (holdout):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        82\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.20      1.00      0.33         1\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.93      1.00      0.96        38\n",
      "           5       1.00      0.25      0.40         4\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       1.00      0.38      0.55         8\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.30      0.75      0.43         4\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.54      0.53      0.46       143\n",
      "weighted avg       0.96      0.90      0.91       143\n",
      "\n",
      "\n",
      "=== Repeated Group Holdout (30 splits) ===\n",
      "LogReg: {'macro_f1_mean': 0.1998072491629742, 'macro_f1_std': 0.07904093244227857, 'major_macro_f1_mean': 0.25770697910259127, 'fault_vs_normal_f1_mean': 0.6055958340852101, 'fault_vs_normal_f1_std': 0.24127830241058731}\n",
      "HGB   : {'macro_f1_mean': 0.38878672931629693, 'macro_f1_std': 0.1110308237109982, 'major_macro_f1_mean': 0.48737787121416637, 'fault_vs_normal_f1_mean': 0.8875212104777906, 'fault_vs_normal_f1_std': 0.13754766056670029}\n",
      "\n",
      "Saved: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer/results_summary.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3W Dataset (WELL-only) — Row-per-file Classification (v3_1)\n",
    "# Fully runnable, leakage-safe, interview-ready (+ stability eval)\n",
    "#\n",
    "# ✅ Key fixes included\n",
    "# - Group splits by well_id\n",
    "# - Train-only feature filtering inside pipeline (no pre-CV contamination)\n",
    "# - Fixed-label macro-F1 (comparable across folds)\n",
    "# - \"Major classes\" derived from y_train per fold\n",
    "# - Cloneable weighted HGB estimator (+ random_state)\n",
    "# - Time-window first/last; z_last uses last valid\n",
    "# - Repeated group holdout (mean/std)\n",
    "# - Cache versioning to avoid stale-feature mistakes\n",
    "# - Hybrid imputation: median for continuous, -1 for state-ish (+ missing indicators)\n",
    "# ============================================================\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "BASE = \"/kaggle/input/3w-dataset/2.0.0\"\n",
    "RANDOM_STATE = 42\n",
    "N_WELL_FILES = None  # None = all WELL files\n",
    "\n",
    "FEATURES_VERSION = \"v3_1_fixed_timewin_zlast_hybridImputer\"\n",
    "OUT_DIR = f\"/kaggle/working/3w_prepared_{FEATURES_VERSION}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CACHE_PATH = f\"{OUT_DIR}/df_ml_well_{FEATURES_VERSION}.parquet\"\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"CACHE_PATH:\", CACHE_PATH)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset constants / mappings\n",
    "# -------------------------\n",
    "VAR_RENAME = {\n",
    "    \"ABER-CKGL\": \"gl_choke_opening_pct\",\n",
    "    \"ABER-CKP\":  \"prod_choke_opening_pct\",\n",
    "    \"ESTADO-DHSV\":   \"dhsv_state\",\n",
    "    \"ESTADO-M1\":     \"prod_master_valve_state\",\n",
    "    \"ESTADO-M2\":     \"ann_master_valve_state\",\n",
    "    \"ESTADO-PXO\":    \"pig_crossover_valve_state\",\n",
    "    \"ESTADO-SDV-GL\": \"gl_shutdown_valve_state\",\n",
    "    \"ESTADO-SDV-P\":  \"prod_shutdown_valve_state\",\n",
    "    \"ESTADO-W1\":     \"prod_wing_valve_state\",\n",
    "    \"ESTADO-W2\":     \"ann_wing_valve_state\",\n",
    "    \"ESTADO-XO\":     \"crossover_valve_state\",\n",
    "    \"P-ANULAR\":     \"annulus_pressure_pa\",\n",
    "    \"P-JUS-BS\":     \"svc_pump_downstream_pressure_pa\",\n",
    "    \"P-JUS-CKGL\":   \"gl_choke_downstream_pressure_pa\",\n",
    "    \"P-JUS-CKP\":    \"prod_choke_downstream_pressure_pa\",\n",
    "    \"P-MON-CKGL\":   \"gl_choke_upstream_pressure_pa\",\n",
    "    \"P-MON-CKP\":    \"prod_choke_upstream_pressure_pa\",\n",
    "    \"P-MON-SDV-P\":  \"prod_sdv_upstream_pressure_pa\",\n",
    "    \"P-PDG\":        \"pdg_downhole_pressure_pa\",\n",
    "    \"PT-P\":         \"xmas_tree_prod_line_pressure_pa\",\n",
    "    \"P-TPT\":        \"tpt_pressure_pa\",\n",
    "    \"QBS\": \"svc_pump_flow_m3s\",\n",
    "    \"QGL\": \"gas_lift_flow_m3s\",\n",
    "    \"T-JUS-CKP\": \"prod_choke_downstream_temp_c\",\n",
    "    \"T-MON-CKP\": \"prod_choke_upstream_temp_c\",\n",
    "    \"T-PDG\":     \"pdg_downhole_temp_c\",\n",
    "    \"T-TPT\":     \"tpt_temp_c\",\n",
    "    \"class\": \"class_code\",\n",
    "    \"state\": \"state_code\",\n",
    "}\n",
    "\n",
    "EVENT_TYPE_CODE_TO_NAME = {\n",
    "    0:\"Normal Operation\", 1:\"Abrupt Increase of BSW\", 2:\"Spurious Closure of DHSV\",\n",
    "    3:\"Severe Slugging\", 4:\"Flow Instability\", 5:\"Rapid Productivity Loss\",\n",
    "    6:\"Quick Restriction in PCK\", 7:\"Scaling in PCK\",\n",
    "    8:\"Hydrate in Production Line\", 9:\"Hydrate in Service Line\",\n",
    "}\n",
    "\n",
    "LABEL_COLS = {\"class_code\", \"state_code\", \"class_label\", \"state_label\"}\n",
    "\n",
    "# ============================================================\n",
    "# 1) Build file index\n",
    "# ============================================================\n",
    "def build_file_index(base: str) -> pd.DataFrame:\n",
    "    paths = []\n",
    "    for root, _, files in os.walk(base):\n",
    "        for f in files:\n",
    "            if f.endswith(\".parquet\"):\n",
    "                paths.append(os.path.join(root, f))\n",
    "\n",
    "    df = pd.DataFrame({\"path\": paths})\n",
    "    codes = df[\"path\"].str.extract(r\"/2\\.0\\.0/(\\d+)/\", expand=False)\n",
    "    df[\"event_type_code\"] = pd.to_numeric(codes, errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df.dropna(subset=[\"event_type_code\"])\n",
    "    df[\"event_type_code\"] = df[\"event_type_code\"].astype(int)\n",
    "\n",
    "    df[\"file\"] = df[\"path\"].str.split(\"/\").str[-1]\n",
    "    df[\"source\"] = df[\"file\"].str.extract(r\"^(WELL|SIMULATED|DRAWN)\")\n",
    "    df[\"well_id\"] = df[\"file\"].str.extract(r\"(WELL-\\d+)\")\n",
    "    df[\"run_ts\"] = df[\"file\"].str.extract(r\"_(\\d{14})\")\n",
    "    df[\"run_ts\"] = pd.to_datetime(df[\"run_ts\"], format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n",
    "\n",
    "    return df.sort_values([\"event_type_code\",\"source\",\"well_id\",\"run_ts\"]).reset_index(drop=True)\n",
    "\n",
    "df_files = build_file_index(BASE)\n",
    "df_w_files = df_files[df_files[\"source\"] == \"WELL\"].copy()\n",
    "assert df_w_files[\"well_id\"].notna().all()\n",
    "\n",
    "print(\"Total files:\", len(df_files), \"| WELL files:\", len(df_w_files), \"| Wells:\", df_w_files[\"well_id\"].nunique())\n",
    "print(\"Class counts:\\n\", df_w_files[\"event_type_code\"].value_counts().sort_index())\n",
    "\n",
    "# ============================================================\n",
    "# 2) Cleaning\n",
    "# ============================================================\n",
    "def clean_3w_instance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "        df = df.set_index(\"timestamp\")\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "\n",
    "    df = df[~df.index.isna()].sort_index()\n",
    "    df.index.name = \"timestamp\"\n",
    "    df = df.rename(columns=VAR_RENAME)\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c in (\"class_code\", \"state_code\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int16\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 3) Feature extraction (time-window first/last + last-valid z_last)\n",
    "# ============================================================\n",
    "def summarize_timeseries_v3_1(df_clean: pd.DataFrame, frac: float = 0.1, state_max: int = 3) -> dict:\n",
    "    sensors = df_clean.drop(columns=list(LABEL_COLS), errors=\"ignore\")\n",
    "    num = sensors.select_dtypes(include=[np.number])\n",
    "\n",
    "    out = {\n",
    "        \"n_obs\": int(len(df_clean)),\n",
    "        \"duration_s\": float((df_clean.index.max() - df_clean.index.min()).total_seconds())\n",
    "                      if len(df_clean) else np.nan,\n",
    "    }\n",
    "    if num.shape[1] == 0 or len(num) == 0:\n",
    "        return out\n",
    "\n",
    "    state_cols = [c for c in num.columns if c.endswith(\"_state\")]\n",
    "    cont_cols  = [c for c in num.columns if c not in state_cols]\n",
    "\n",
    "    def _first_last_masks(index, frac: float, n: int):\n",
    "        k = max(1, int(n * frac))\n",
    "        if n < 2:\n",
    "            m = np.ones(n, dtype=bool)\n",
    "            return m, m\n",
    "\n",
    "        try:\n",
    "            idx = pd.DatetimeIndex(index)\n",
    "        except Exception:\n",
    "            idx = None\n",
    "\n",
    "        if idx is not None and idx.notna().all():\n",
    "            tmin, tmax = idx.min(), idx.max()\n",
    "            dur_s = (tmax - tmin).total_seconds()\n",
    "            if np.isfinite(dur_s) and dur_s > 0:\n",
    "                w = dur_s * frac\n",
    "                t_first_end = tmin + pd.Timedelta(seconds=w)\n",
    "                t_last_start = tmax - pd.Timedelta(seconds=w)\n",
    "\n",
    "                first_mask = np.asarray(idx <= t_first_end, dtype=bool)\n",
    "                last_mask  = np.asarray(idx >= t_last_start, dtype=bool)\n",
    "\n",
    "                # fallback if empty due to duplicate/irregular timestamps\n",
    "                if first_mask.sum() == 0:\n",
    "                    first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n",
    "                if last_mask.sum() == 0:\n",
    "                    last_mask = np.zeros(n, dtype=bool); last_mask[-k:] = True\n",
    "                return first_mask, last_mask\n",
    "\n",
    "        # row fallback\n",
    "        first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n",
    "        last_mask  = np.zeros(n, dtype=bool); last_mask[-k:] = True\n",
    "        return first_mask, last_mask\n",
    "\n",
    "    # Continuous sensors\n",
    "    if len(cont_cols):\n",
    "        cont = num[cont_cols]\n",
    "        med_raw = cont.median()\n",
    "        iqr_raw = (cont.quantile(0.75) - cont.quantile(0.25))\n",
    "\n",
    "        iqr = iqr_raw.replace(0, np.nan)\n",
    "        z = (cont - med_raw) / iqr\n",
    "\n",
    "        first_mask, last_mask = _first_last_masks(df_clean.index, frac=frac, n=len(z))\n",
    "        first = z.iloc[first_mask].mean()\n",
    "        last  = z.iloc[last_mask].mean()\n",
    "\n",
    "        agg = z.agg([\"mean\", \"std\", \"min\", \"max\"]).T\n",
    "        miss = cont.isna().mean()\n",
    "\n",
    "        for col in cont_cols:\n",
    "            out[f\"{col}__raw_median\"] = med_raw[col]\n",
    "            out[f\"{col}__raw_iqr\"]    = iqr_raw[col]\n",
    "\n",
    "            s_raw = cont[col].dropna()\n",
    "            out[f\"{col}__raw_last\"] = s_raw.iloc[-1] if len(s_raw) else np.nan\n",
    "\n",
    "            out[f\"{col}__z_mean\"] = agg.loc[col, \"mean\"]\n",
    "            out[f\"{col}__z_std\"]  = agg.loc[col, \"std\"]\n",
    "            out[f\"{col}__z_min\"]  = agg.loc[col, \"min\"]\n",
    "            out[f\"{col}__z_max\"]  = agg.loc[col, \"max\"]\n",
    "\n",
    "            # last VALID z (not last row)\n",
    "            s_z = z[col].dropna()\n",
    "            out[f\"{col}__z_last\"] = s_z.iloc[-1] if len(s_z) else np.nan\n",
    "\n",
    "            out[f\"{col}__delta_last_first\"] = (last[col] - first[col])\n",
    "            out[f\"{col}__abs_delta\"]        = abs(last[col] - first[col])\n",
    "            out[f\"{col}__missing_frac\"]     = miss[col]\n",
    "\n",
    "    # State-like sensors\n",
    "    if len(state_cols):\n",
    "        st = num[state_cols]\n",
    "        for col in state_cols:\n",
    "            s = st[col]\n",
    "            s_non = s.dropna()\n",
    "\n",
    "            out[f\"{col}__missing_frac\"] = float(s.isna().mean())\n",
    "            out[f\"{col}__last\"] = float(s_non.iloc[-1]) if len(s_non) else np.nan\n",
    "\n",
    "            if len(s_non) >= 2:\n",
    "                n_trans = int((s_non != s_non.shift()).sum() - 1)\n",
    "            else:\n",
    "                n_trans = 0\n",
    "\n",
    "            out[f\"{col}__n_transitions\"] = n_trans\n",
    "            out[f\"{col}__transitions_rate\"] = n_trans / max(1, len(s_non))\n",
    "\n",
    "            known = 0.0\n",
    "            for v in range(state_max + 1):\n",
    "                p = float((s_non == v).mean()) if len(s_non) else np.nan\n",
    "                out[f\"{col}__p_state_{v}\"] = p\n",
    "                if not np.isnan(p):\n",
    "                    known += p\n",
    "            out[f\"{col}__p_state_other\"] = (1.0 - known) if len(s_non) else np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 4) Build row-per-file dataset (with caching)\n",
    "# ============================================================\n",
    "def build_row_per_file_dataset(df_files: pd.DataFrame, n_files: int | None = None, random_state: int = 42) -> pd.DataFrame:\n",
    "    if n_files is None:\n",
    "        sample = df_files.reset_index(drop=True)\n",
    "    else:\n",
    "        sample = df_files.sample(n_files, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in tqdm(sample.iterrows(), total=len(sample), desc=\"Building WELL dataset\"):\n",
    "        df_raw = pd.read_parquet(r[\"path\"])\n",
    "        df_clean = clean_3w_instance(df_raw)\n",
    "\n",
    "        feats = summarize_timeseries_v3_1(df_clean)\n",
    "        feats[\"event_type_code\"] = int(r[\"event_type_code\"])\n",
    "        feats[\"event_type_name\"] = EVENT_TYPE_CODE_TO_NAME.get(int(r[\"event_type_code\"]), \"Unknown\")\n",
    "        feats[\"well_id\"] = r[\"well_id\"]\n",
    "        feats[\"run_ts\"] = r[\"run_ts\"]\n",
    "        feats[\"file\"] = r[\"file\"]\n",
    "        rows.append(feats)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    df_ml_well = pd.read_parquet(CACHE_PATH)\n",
    "    print(\"Loaded cached features:\", df_ml_well.shape)\n",
    "else:\n",
    "    df_ml_well = build_row_per_file_dataset(df_w_files, n_files=N_WELL_FILES, random_state=RANDOM_STATE)\n",
    "    df_ml_well.to_parquet(CACHE_PATH, index=False)\n",
    "    print(\"Built + saved features:\", df_ml_well.shape)\n",
    "\n",
    "with open(f\"{OUT_DIR}/dataset_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"base\": BASE,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_well_files_used\": int(len(df_ml_well)),\n",
    "        \"features_version\": FEATURES_VERSION,\n",
    "        \"notes\": \"continuous: raw median/iqr/last + robust-z stats + deltas; \"\n",
    "                 \"state: last + transitions + proportions (+other); WELL-only; \"\n",
    "                 \"fixes: time-window first/last + z_last last-valid; \"\n",
    "                 \"imputer: hybrid median/-1 + missing indicators; \"\n",
    "                 \"cache versioned to avoid staleness\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "# ============================================================\n",
    "# 5) Build X/y/groups (NO global filtering leakage)\n",
    "# ============================================================\n",
    "def make_Xy_groups(df: pd.DataFrame):\n",
    "    y = df[\"event_type_code\"].astype(int).copy()\n",
    "    groups = df[\"well_id\"].astype(str).copy()\n",
    "    drop_cols = [\"event_type_code\",\"event_type_name\",\"file\",\"run_ts\",\"well_id\"]\n",
    "    X = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    return X, y, groups\n",
    "\n",
    "X, y, groups = make_Xy_groups(df_ml_well)\n",
    "print(\"X:\", X.shape, \"| y:\", y.shape, \"| wells:\", groups.nunique())\n",
    "print(\"Label counts:\\n\", y.value_counts().sort_index())\n",
    "\n",
    "# ============================================================\n",
    "# 6) Train-only column filter (inside pipeline)\n",
    "# ============================================================\n",
    "class TrainOnlyColumnFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_threshold: float = 0.98):\n",
    "        self.missing_threshold = missing_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy().replace([np.inf, -np.inf], np.nan)\n",
    "        all_missing = X.columns[X.isna().all()]\n",
    "        miss = X.isna().mean()\n",
    "        high_missing = miss[miss > self.missing_threshold].index\n",
    "        const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "        drop = set(all_missing) | set(high_missing) | set(const_cols)\n",
    "        self.keep_columns_ = [c for c in X.columns if c not in drop]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy().replace([np.inf, -np.inf], np.nan)\n",
    "        for c in getattr(self, \"keep_columns_\", []):\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "        return X[self.keep_columns_]\n",
    "\n",
    "# ============================================================\n",
    "# 7) Hybrid imputer: median for continuous, -1 for state-ish (+ indicators)\n",
    "# ============================================================\n",
    "class HybridImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    - Median imputation for most features (continuous sensors)\n",
    "    - Constant -1 imputation for state-ish features (names containing state_token)\n",
    "    - Adds missing indicators for all kept columns\n",
    "    \"\"\"\n",
    "    def __init__(self, state_token: str = \"_state\", fill_value_state: float = -1.0):\n",
    "        self.state_token = state_token\n",
    "        self.fill_value_state = fill_value_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.cols_ = list(X.columns)\n",
    "        self.state_cols_ = [c for c in self.cols_ if self.state_token in c]\n",
    "        self.cont_cols_ = [c for c in self.cols_ if c not in self.state_cols_]\n",
    "        # numeric-only median is fine; if some cols are all-NaN, median becomes NaN and will be handled in transform\n",
    "        self.cont_medians_ = X[self.cont_cols_].median(numeric_only=True) if len(self.cont_cols_) else pd.Series(dtype=float)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Ensure all fit columns exist\n",
    "        for c in self.cols_:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "        X = X[self.cols_]\n",
    "\n",
    "        # Missing indicators\n",
    "        miss = X.isna().astype(np.int8)\n",
    "        miss.columns = [f\"{c}__isna\" for c in miss.columns]\n",
    "\n",
    "        # Impute continuous with medians\n",
    "        if len(self.cont_cols_):\n",
    "            # fillna with Series aligns by column name\n",
    "            X[self.cont_cols_] = X[self.cont_cols_].fillna(self.cont_medians_)\n",
    "            # any remaining NaNs (e.g., median was NaN) -> 0\n",
    "            X[self.cont_cols_] = X[self.cont_cols_].fillna(0.0)\n",
    "\n",
    "        # Impute state-ish with -1\n",
    "        if len(self.state_cols_):\n",
    "            X[self.state_cols_] = X[self.state_cols_].fillna(self.fill_value_state)\n",
    "\n",
    "        return pd.concat([X, miss], axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 8) Weighted HGB estimator (cloneable)\n",
    "# ============================================================\n",
    "class WeightedHGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=700,\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf_ = HistGradientBoostingClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            max_iter=self.max_iter,\n",
    "            max_leaf_nodes=self.max_leaf_nodes,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            l2_regularization=self.l2_regularization,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "        y_arr = np.asarray(y)\n",
    "        classes = np.unique(y_arr)\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_arr)\n",
    "        cw_map = dict(zip(classes, cw))\n",
    "        sample_weight = np.array([cw_map[v] for v in y_arr], dtype=float)\n",
    "\n",
    "        self.clf_.fit(X, y_arr, sample_weight=sample_weight)\n",
    "        self.classes_ = classes\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf_.predict(X)\n",
    "\n",
    "# ============================================================\n",
    "# 9) Evaluation helpers\n",
    "# ============================================================\n",
    "def choose_stratified_group_n_splits(X, y, groups, max_splits=4, random_state=42) -> int:\n",
    "    \"\"\"Pick the largest n_splits (<= max_splits) that doesn't raise ValueError.\"\"\"\n",
    "    for k in range(max_splits, 1, -1):\n",
    "        try:\n",
    "            cv = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "            _ = next(cv.split(X, y, groups=groups))\n",
    "            return k\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return 2\n",
    "\n",
    "def evaluate_stratified_group_cv(model, X, y, groups, n_splits=4, random_state=42, major_min_support=10):\n",
    "    labels = np.sort(pd.unique(y))\n",
    "    cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    macro_f1s, major_f1s, bin_f1s = [], [], []\n",
    "    per_class_f1 = {int(lbl): [] for lbl in labels}\n",
    "\n",
    "    for _, (tr, te) in enumerate(cv.split(X, y, groups=groups), start=1):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        m = clone(model)\n",
    "        m.fit(Xtr, ytr)\n",
    "        pred = m.predict(Xte)\n",
    "\n",
    "        macro_f1s.append(f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0))\n",
    "\n",
    "        # major classes based on TRAIN only\n",
    "        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n",
    "        if major_classes:\n",
    "            major_labels = np.array(sorted(major_classes))\n",
    "            major_mask = yte.isin(major_labels)\n",
    "            if major_mask.any():\n",
    "                major_f1s.append(\n",
    "                    f1_score(yte[major_mask], pred[major_mask],\n",
    "                             average=\"macro\", labels=major_labels, zero_division=0)\n",
    "                )\n",
    "            else:\n",
    "                major_f1s.append(np.nan)\n",
    "        else:\n",
    "            major_f1s.append(np.nan)\n",
    "\n",
    "        # fault vs normal\n",
    "        bin_f1s.append(\n",
    "            f1_score((yte != 0).astype(int), (pred != 0).astype(int), zero_division=0)\n",
    "        )\n",
    "\n",
    "        rep = classification_report(yte, pred, labels=labels, output_dict=True, zero_division=0)\n",
    "        for lbl in labels:\n",
    "            per_class_f1[int(lbl)].append(rep[str(int(lbl))][\"f1-score\"])\n",
    "\n",
    "    return {\n",
    "        \"macro_f1_mean\": float(np.nanmean(macro_f1s)),\n",
    "        \"macro_f1_std\":  float(np.nanstd(macro_f1s)),\n",
    "        \"major_macro_f1_mean\": float(np.nanmean(major_f1s)),\n",
    "        \"fault_vs_normal_f1_mean\": float(np.nanmean(bin_f1s)),\n",
    "        \"per_class_f1_mean\": {str(k): float(np.nanmean(v)) for k, v in per_class_f1.items()},\n",
    "    }\n",
    "\n",
    "def evaluate_on_holdout(model, Xtr, ytr, Xho, yho, major_min_support=10):\n",
    "    labels = np.sort(pd.unique(pd.concat([ytr, yho], axis=0)))\n",
    "\n",
    "    m = clone(model)\n",
    "    m.fit(Xtr, ytr)\n",
    "    pred = m.predict(Xho)\n",
    "\n",
    "    macro = f1_score(yho, pred, average=\"macro\", labels=labels, zero_division=0)\n",
    "\n",
    "    major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n",
    "    major_labels = np.array(sorted(major_classes)) if major_classes else np.array([], dtype=int)\n",
    "    if len(major_labels) and yho.isin(major_labels).any():\n",
    "        major = f1_score(\n",
    "            yho[yho.isin(major_labels)], pred[yho.isin(major_labels)],\n",
    "            average=\"macro\", labels=major_labels, zero_division=0\n",
    "        )\n",
    "    else:\n",
    "        major = np.nan\n",
    "\n",
    "    bin_f1 = f1_score((yho != 0).astype(int), (pred != 0).astype(int), zero_division=0)\n",
    "    report = classification_report(yho, pred, labels=labels, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"macro_f1\": float(macro),\n",
    "        \"major_macro_f1\": float(major) if np.isfinite(major) else np.nan,\n",
    "        \"fault_vs_normal_f1\": float(bin_f1),\n",
    "        \"classification_report\": report,\n",
    "    }\n",
    "\n",
    "def repeated_holdout(model, X, y, groups, repeats=30, test_size=0.2, random_state=42, major_min_support=10):\n",
    "    labels = np.sort(pd.unique(y))\n",
    "    gss = GroupShuffleSplit(n_splits=repeats, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    macs, bins, majors = [], [], []\n",
    "    for tr, te in gss.split(X, y, groups=groups):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        m = clone(model)\n",
    "        m.fit(Xtr, ytr)\n",
    "        pred = m.predict(Xte)\n",
    "\n",
    "        macs.append(f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0))\n",
    "        bins.append(f1_score((yte != 0).astype(int), (pred != 0).astype(int), zero_division=0))\n",
    "\n",
    "        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n",
    "        major_labels = np.array(sorted(major_classes)) if major_classes else np.array([], dtype=int)\n",
    "        if len(major_labels) and yte.isin(major_labels).any():\n",
    "            majors.append(\n",
    "                f1_score(yte[yte.isin(major_labels)], pred[yte.isin(major_labels)],\n",
    "                         average=\"macro\", labels=major_labels, zero_division=0)\n",
    "            )\n",
    "        else:\n",
    "            majors.append(np.nan)\n",
    "\n",
    "    return {\n",
    "        \"macro_f1_mean\": float(np.nanmean(macs)),\n",
    "        \"macro_f1_std\": float(np.nanstd(macs)),\n",
    "        \"major_macro_f1_mean\": float(np.nanmean(majors)),\n",
    "        \"fault_vs_normal_f1_mean\": float(np.nanmean(bins)),\n",
    "        \"fault_vs_normal_f1_std\": float(np.nanstd(bins)),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 10) Models (consistent preprocessing)\n",
    "# ============================================================\n",
    "preprocess_common = [\n",
    "    (\"col_filter\", TrainOnlyColumnFilter(missing_threshold=0.98)),\n",
    "    (\"imputer\", HybridImputer(state_token=\"_state\", fill_value_state=-1.0)),\n",
    "]\n",
    "\n",
    "logreg = Pipeline(preprocess_common + [\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(max_iter=8000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "hgb = Pipeline(preprocess_common + [\n",
    "    (\"clf\", WeightedHGBClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=700,\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# 11) Final holdout by WELL + CV on train only + repeated holdout\n",
    "# ============================================================\n",
    "# One final holdout split (good for a clean \"final test\" demo)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "tr_idx, ho_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_tr, y_tr, g_tr = X.iloc[tr_idx], y.iloc[tr_idx], groups.iloc[tr_idx]\n",
    "X_ho, y_ho, g_ho = X.iloc[ho_idx], y.iloc[ho_idx], groups.iloc[ho_idx]\n",
    "\n",
    "print(\"\\nTrain wells:\", g_tr.nunique(), \"| Holdout wells:\", g_ho.nunique())\n",
    "print(\"Holdout label counts:\\n\", y_ho.value_counts().sort_index())\n",
    "\n",
    "# Robust n_splits selection (avoids StratifiedGroupKFold ValueError)\n",
    "n_splits = choose_stratified_group_n_splits(X_tr, y_tr, g_tr, max_splits=4, random_state=RANDOM_STATE)\n",
    "print(f\"\\nUsing StratifiedGroupKFold with n_splits={n_splits}\")\n",
    "\n",
    "print(\"\\n=== CV on TRAIN ONLY ===\")\n",
    "logreg_cv = evaluate_stratified_group_cv(logreg, X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\n",
    "hgb_cv    = evaluate_stratified_group_cv(hgb,   X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\n",
    "print(\"LogReg CV:\", logreg_cv)\n",
    "print(\"HGB   CV:\", hgb_cv)\n",
    "\n",
    "print(\"\\n=== FINAL HOLDOUT (train wells -> holdout wells) ===\")\n",
    "logreg_hold = evaluate_on_holdout(logreg, X_tr, y_tr, X_ho, y_ho)\n",
    "hgb_hold    = evaluate_on_holdout(hgb,   X_tr, y_tr, X_ho, y_ho)\n",
    "print(\"LogReg Holdout:\", {k: v for k, v in logreg_hold.items() if k != \"classification_report\"})\n",
    "print(\"HGB   Holdout:\", {k: v for k, v in hgb_hold.items() if k != \"classification_report\"})\n",
    "print(\"\\nLogReg classification report (holdout):\\n\", logreg_hold[\"classification_report\"])\n",
    "print(\"\\nHGB classification report (holdout):\\n\", hgb_hold[\"classification_report\"])\n",
    "\n",
    "print(\"\\n=== Repeated Group Holdout (30 splits) ===\")\n",
    "logreg_rep = repeated_holdout(logreg, X, y, groups, repeats=30, test_size=0.2, random_state=RANDOM_STATE)\n",
    "hgb_rep    = repeated_holdout(hgb,   X, y, groups, repeats=30, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"LogReg:\", logreg_rep)\n",
    "print(\"HGB   :\", hgb_rep)\n",
    "\n",
    "# Save results\n",
    "with open(f\"{OUT_DIR}/results_summary.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"features_version\": FEATURES_VERSION,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"logreg_cv\": logreg_cv,\n",
    "        \"hgb_cv\": hgb_cv,\n",
    "        \"logreg_holdout\": {k: v for k, v in logreg_hold.items() if k != \"classification_report\"},\n",
    "        \"hgb_holdout\": {k: v for k, v in hgb_hold.items() if k != \"classification_report\"},\n",
    "        \"logreg_repeated_holdout\": logreg_rep,\n",
    "        \"hgb_repeated_holdout\": hgb_rep,\n",
    "        \"notes\": {\n",
    "            \"leakage_control\": [\n",
    "                \"train-only column filtering inside pipeline\",\n",
    "                \"group splitting by well_id\",\n",
    "                \"CV on train wells only (plus a separate holdout)\",\n",
    "                \"major classes computed from y_train per fold\",\n",
    "                \"fixed-label macro-F1 with zero_division=0\",\n",
    "                \"repeated group holdout metrics (mean/std)\",\n",
    "                \"hybrid imputation (median for continuous, -1 for state-ish) + missing indicators\",\n",
    "                \"cache path versioned to avoid stale features\"\n",
    "            ]\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved: {OUT_DIR}/results_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1c32a9",
   "metadata": {
    "_cell_guid": "a7c6a5d1-8959-4210-8ad1-1a97420747f2",
    "_uuid": "d0ade4b7-7960-4103-9c19-458aa1cbc34f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-04T13:19:34.819518Z",
     "iopub.status.busy": "2026-01-04T13:19:34.819039Z",
     "iopub.status.idle": "2026-01-04T13:19:34.871794Z",
     "shell.execute_reply": "2026-01-04T13:19:34.870597Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.060549,
     "end_time": "2026-01-04T13:19:34.874149",
     "exception": false,
     "start_time": "2026-01-04T13:19:34.813600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: /kaggle/working/3w_prepared_v3_1\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "BASE = \"/kaggle/input/3w-dataset/2.0.0\"\n",
    "RANDOM_STATE = 42\n",
    "N_WELL_FILES = None  # None = all WELL files\n",
    "\n",
    "OUT_DIR = \"/kaggle/working/3w_prepared_v3_1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CACHE_PATH = f\"{OUT_DIR}/df_ml_well_v3_1.parquet\"\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c8c34",
   "metadata": {
    "papermill": {
     "duration": 0.00393,
     "end_time": "2026-01-04T13:19:34.882163",
     "exception": false,
     "start_time": "2026-01-04T13:19:34.878233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3W Dataset (WELL-only) — Row-per-file Classification (v3_1)\n",
    "\n",
    "## Summary (Engineer Story)\n",
    "- Built a file index from the 3W dataset and filtered to **WELL files only** (1119 files, 40 wells).\n",
    "- Cleaned each file: parsed timestamp index, renamed sensors, converted to numeric types.\n",
    "- Engineered **row-per-file features**:\n",
    "  - Continuous sensors: raw median/IQR/last + robust z-stats (mean/std/min/max/last) + delta(first→last) + missing ratio.\n",
    "  - State/valve signals: last state + transition count/rate + time-in-state proportions (+ “other” state).\n",
    "- Evaluated with **Group splits by `well_id`** to avoid leakage across wells (harder but realistic).\n",
    "- Compared baseline (Logistic Regression) vs boosting (HistGradientBoosting with class-weighted sample_weight).\n",
    "- Result: **HGB outperformed baseline**, reaching about **macro-F1 ~0.42 (repeated group shuffle)** and **fault-vs-normal F1 ~0.76**, with variance due to very rare classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c9d5e0",
   "metadata": {
    "_cell_guid": "43dd3beb-1e81-4aa6-ae49-562ace3591fc",
    "_uuid": "b5ce1d34-f068-49cd-a840-6f1a42682c0c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-04T13:19:34.891608Z",
     "iopub.status.busy": "2026-01-04T13:19:34.891255Z",
     "iopub.status.idle": "2026-01-04T13:19:36.511508Z",
     "shell.execute_reply": "2026-01-04T13:19:36.510554Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.627617,
     "end_time": "2026-01-04T13:19:36.513636",
     "exception": false,
     "start_time": "2026-01-04T13:19:34.886019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 2228 | WELL files: 1119 | Wells: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type_code\n",
       "0    594\n",
       "1      4\n",
       "2     22\n",
       "3     32\n",
       "4    343\n",
       "5     11\n",
       "6      6\n",
       "7     36\n",
       "8     14\n",
       "9     57\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_file_index(base: str) -> pd.DataFrame:\n",
    "    paths = []\n",
    "    for root, _, files in os.walk(base):\n",
    "        for f in files:\n",
    "            if f.endswith(\".parquet\"):\n",
    "                paths.append(os.path.join(root, f))\n",
    "\n",
    "    df = pd.DataFrame({\"path\": paths})\n",
    "    codes = df[\"path\"].str.extract(r\"/2\\.0\\.0/(\\d+)/\", expand=False)\n",
    "    df[\"event_type_code\"] = pd.to_numeric(codes, errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df.dropna(subset=[\"event_type_code\"])\n",
    "    df[\"event_type_code\"] = df[\"event_type_code\"].astype(int)\n",
    "    df[\"file\"] = df[\"path\"].str.split(\"/\").str[-1]\n",
    "    df[\"source\"] = df[\"file\"].str.extract(r\"^(WELL|SIMULATED|DRAWN)\")\n",
    "    df[\"well_id\"] = df[\"file\"].str.extract(r\"(WELL-\\d+)\")\n",
    "    df[\"run_ts\"] = df[\"file\"].str.extract(r\"_(\\d{14})\")\n",
    "    df[\"run_ts\"] = pd.to_datetime(df[\"run_ts\"], format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n",
    "    return df.sort_values([\"event_type_code\",\"source\",\"well_id\",\"run_ts\"]).reset_index(drop=True)\n",
    "\n",
    "df_files = build_file_index(BASE)\n",
    "df_w_files = df_files[df_files[\"source\"]==\"WELL\"].copy()\n",
    "assert df_w_files[\"well_id\"].notna().all()\n",
    "\n",
    "\n",
    "print(\"Total files:\", len(df_files), \"| WELL files:\", len(df_w_files), \"| Wells:\", df_w_files[\"well_id\"].nunique())\n",
    "display(df_w_files[\"event_type_code\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67bc136",
   "metadata": {
    "_cell_guid": "7d3f90e3-30d8-4693-be35-bc8c37ad4f94",
    "_uuid": "ccb30fdb-e518-4f00-a9a7-92d2839e785d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-04T13:19:36.524473Z",
     "iopub.status.busy": "2026-01-04T13:19:36.523881Z",
     "iopub.status.idle": "2026-01-04T13:19:36.546125Z",
     "shell.execute_reply": "2026-01-04T13:19:36.545039Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030197,
     "end_time": "2026-01-04T13:19:36.548222",
     "exception": false,
     "start_time": "2026-01-04T13:19:36.518025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VAR_RENAME = {\n",
    "    \"ABER-CKGL\": \"gl_choke_opening_pct\",\n",
    "    \"ABER-CKP\":  \"prod_choke_opening_pct\",\n",
    "    \"ESTADO-DHSV\":   \"dhsv_state\",\n",
    "    \"ESTADO-M1\":     \"prod_master_valve_state\",\n",
    "    \"ESTADO-M2\":     \"ann_master_valve_state\",\n",
    "    \"ESTADO-PXO\":    \"pig_crossover_valve_state\",\n",
    "    \"ESTADO-SDV-GL\": \"gl_shutdown_valve_state\",\n",
    "    \"ESTADO-SDV-P\":  \"prod_shutdown_valve_state\",\n",
    "    \"ESTADO-W1\":     \"prod_wing_valve_state\",\n",
    "    \"ESTADO-W2\":     \"ann_wing_valve_state\",\n",
    "    \"ESTADO-XO\":     \"crossover_valve_state\",\n",
    "    \"P-ANULAR\":     \"annulus_pressure_pa\",\n",
    "    \"P-JUS-BS\":     \"svc_pump_downstream_pressure_pa\",\n",
    "    \"P-JUS-CKGL\":   \"gl_choke_downstream_pressure_pa\",\n",
    "    \"P-JUS-CKP\":    \"prod_choke_downstream_pressure_pa\",\n",
    "    \"P-MON-CKGL\":   \"gl_choke_upstream_pressure_pa\",\n",
    "    \"P-MON-CKP\":    \"prod_choke_upstream_pressure_pa\",\n",
    "    \"P-MON-SDV-P\":  \"prod_sdv_upstream_pressure_pa\",\n",
    "    \"P-PDG\":        \"pdg_downhole_pressure_pa\",\n",
    "    \"PT-P\":         \"xmas_tree_prod_line_pressure_pa\",\n",
    "    \"P-TPT\":        \"tpt_pressure_pa\",\n",
    "    \"QBS\": \"svc_pump_flow_m3s\",\n",
    "    \"QGL\": \"gas_lift_flow_m3s\",\n",
    "    \"T-JUS-CKP\": \"prod_choke_downstream_temp_c\",\n",
    "    \"T-MON-CKP\": \"prod_choke_upstream_temp_c\",\n",
    "    \"T-PDG\":     \"pdg_downhole_temp_c\",\n",
    "    \"T-TPT\":     \"tpt_temp_c\",\n",
    "    \"class\": \"class_code\",\n",
    "    \"state\": \"state_code\",\n",
    "}\n",
    "\n",
    "EVENT_TYPE_CODE_TO_NAME = {\n",
    "    0:\"Normal Operation\", 1:\"Abrupt Increase of BSW\", 2:\"Spurious Closure of DHSV\",\n",
    "    3:\"Severe Slugging\", 4:\"Flow Instability\", 5:\"Rapid Productivity Loss\",\n",
    "    6:\"Quick Restriction in PCK\", 7:\"Scaling in PCK\",\n",
    "    8:\"Hydrate in Production Line\", 9:\"Hydrate in Service Line\",\n",
    "}\n",
    "\n",
    "LABEL_COLS = {\"class_code\", \"state_code\", \"class_label\", \"state_label\"}\n",
    "\n",
    "def clean_3w_instance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "        df = df.set_index(\"timestamp\")\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "\n",
    "    df = df[~df.index.isna()].sort_index()\n",
    "    df.index.name = \"timestamp\"\n",
    "    df = df.rename(columns=VAR_RENAME)\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c in (\"class_code\", \"state_code\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int16\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    return df\n",
    "\n",
    "def summarize_timeseries_v3_1(df_clean: pd.DataFrame, frac: float = 0.1, state_max: int = 3) -> dict:\n",
    "    sensors = df_clean.drop(columns=list(LABEL_COLS), errors=\"ignore\")\n",
    "    num = sensors.select_dtypes(include=[np.number])\n",
    "\n",
    "    out = {\n",
    "        \"n_obs\": int(len(df_clean)),\n",
    "        \"duration_s\": float((df_clean.index.max() - df_clean.index.min()).total_seconds())\n",
    "                      if len(df_clean) else np.nan,\n",
    "    }\n",
    "    if num.shape[1] == 0 or len(num) == 0:\n",
    "        return out\n",
    "\n",
    "    state_cols = [c for c in num.columns if c.endswith(\"_state\")]\n",
    "    cont_cols  = [c for c in num.columns if c not in state_cols]\n",
    "\n",
    "    if len(cont_cols):\n",
    "        cont = num[cont_cols]\n",
    "        med_raw = cont.median()\n",
    "        iqr_raw = (cont.quantile(0.75) - cont.quantile(0.25))\n",
    "\n",
    "        iqr = iqr_raw.replace(0, np.nan)\n",
    "        z = (cont - med_raw) / iqr\n",
    "\n",
    "        k = max(1, int(len(z) * frac))\n",
    "        first = z.iloc[:k].mean()\n",
    "        last  = z.iloc[-k:].mean()\n",
    "\n",
    "        agg = z.agg([\"mean\",\"std\",\"min\",\"max\"]).T\n",
    "        miss = cont.isna().mean()\n",
    "\n",
    "        for col in cont_cols:\n",
    "            out[f\"{col}__raw_median\"] = med_raw[col]\n",
    "            out[f\"{col}__raw_iqr\"]    = iqr_raw[col]\n",
    "            s = cont[col].dropna()\n",
    "            out[f\"{col}__raw_last\"] = s.iloc[-1] if len(s) else np.nan\n",
    "\n",
    "            out[f\"{col}__z_mean\"] = agg.loc[col, \"mean\"]\n",
    "            out[f\"{col}__z_std\"]  = agg.loc[col, \"std\"]\n",
    "            out[f\"{col}__z_min\"]  = agg.loc[col, \"min\"]\n",
    "            out[f\"{col}__z_max\"]  = agg.loc[col, \"max\"]\n",
    "            out[f\"{col}__z_last\"] = z[col].iloc[-1]\n",
    "\n",
    "            out[f\"{col}__delta_last_first\"] = (last[col] - first[col])\n",
    "            out[f\"{col}__abs_delta\"]        = abs(last[col] - first[col])\n",
    "            out[f\"{col}__missing_frac\"]     = miss[col]\n",
    "\n",
    "    if len(state_cols):\n",
    "        st = num[state_cols]\n",
    "        for col in state_cols:\n",
    "            s = st[col]\n",
    "            s_non = s.dropna()\n",
    "\n",
    "            out[f\"{col}__missing_frac\"] = float(s.isna().mean())\n",
    "            out[f\"{col}__last\"] = float(s_non.iloc[-1]) if len(s_non) else np.nan\n",
    "\n",
    "            if len(s_non) >= 2:\n",
    "                n_trans = int((s_non != s_non.shift()).sum() - 1)\n",
    "            else:\n",
    "                n_trans = 0\n",
    "\n",
    "            out[f\"{col}__n_transitions\"] = n_trans\n",
    "            out[f\"{col}__transitions_rate\"] = n_trans / max(1, len(s_non))\n",
    "\n",
    "            known = 0.0\n",
    "            for v in range(state_max + 1):\n",
    "                p = float((s_non == v).mean()) if len(s_non) else np.nan\n",
    "                out[f\"{col}__p_state_{v}\"] = p\n",
    "                if not np.isnan(p):\n",
    "                    known += p\n",
    "            out[f\"{col}__p_state_other\"] = (1.0 - known) if len(s_non) else np.nan\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a81bd32",
   "metadata": {
    "_cell_guid": "5d497b20-eeb4-40ac-b462-9dca2ca076e3",
    "_uuid": "00df3321-d196-4aa5-b89b-ec6a55e7336b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-04T13:19:36.559608Z",
     "iopub.status.busy": "2026-01-04T13:19:36.558872Z",
     "iopub.status.idle": "2026-01-04T13:21:34.568761Z",
     "shell.execute_reply": "2026-01-04T13:21:34.567544Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 118.017875,
     "end_time": "2026-01-04T13:21:34.571021",
     "exception": false,
     "start_time": "2026-01-04T13:19:36.553146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3469e4313d294ed4b4f4ce4dde8783ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building WELL dataset:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built + saved features: (1119, 286)\n"
     ]
    }
   ],
   "source": [
    "def build_row_per_file_dataset(df_files: pd.DataFrame, n_files: int | None = None, random_state: int = 42) -> pd.DataFrame:\n",
    "    if n_files is None:\n",
    "        sample = df_files.reset_index(drop=True)\n",
    "    else:\n",
    "        sample = df_files.sample(n_files, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in tqdm(sample.iterrows(), total=len(sample), desc=\"Building WELL dataset\"):\n",
    "        df_raw = pd.read_parquet(r[\"path\"])\n",
    "        df_clean = clean_3w_instance(df_raw)\n",
    "\n",
    "        feats = summarize_timeseries_v3_1(df_clean)\n",
    "        feats[\"event_type_code\"] = int(r[\"event_type_code\"])\n",
    "        feats[\"event_type_name\"] = EVENT_TYPE_CODE_TO_NAME.get(int(r[\"event_type_code\"]), \"Unknown\")\n",
    "        feats[\"well_id\"] = r[\"well_id\"]\n",
    "        feats[\"run_ts\"] = r[\"run_ts\"]\n",
    "        feats[\"file\"] = r[\"file\"]\n",
    "        rows.append(feats)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    df_ml_well = pd.read_parquet(CACHE_PATH)\n",
    "    print(\"Loaded cached features:\", df_ml_well.shape)\n",
    "else:\n",
    "    df_ml_well = build_row_per_file_dataset(df_w_files, n_files=N_WELL_FILES, random_state=RANDOM_STATE)\n",
    "    df_ml_well.to_parquet(CACHE_PATH, index=False)\n",
    "    print(\"Built + saved features:\", df_ml_well.shape)\n",
    "\n",
    "with open(f\"{OUT_DIR}/dataset_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"base\": BASE,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_well_files_used\": int(len(df_ml_well)),\n",
    "        \"features_version\": \"row_per_file_v3_1\",\n",
    "        \"notes\": \"continuous: raw median/iqr/last + robust-z stats + deltas; state: last + transitions + proportions (+other); WELL-only\"\n",
    "    }, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992371bb",
   "metadata": {
    "_cell_guid": "a5e900d0-cb5b-4f37-8984-ac7e340f4dcf",
    "_uuid": "2bf22d68-f92b-4b74-b046-05686ad32cc8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-04T13:21:34.582047Z",
     "iopub.status.busy": "2026-01-04T13:21:34.581617Z",
     "iopub.status.idle": "2026-01-04T13:21:34.623370Z",
     "shell.execute_reply": "2026-01-04T13:21:34.622206Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.049892,
     "end_time": "2026-01-04T13:21:34.625556",
     "exception": false,
     "start_time": "2026-01-04T13:21:34.575664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1119, 182) | y: (1119,) | wells: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_type_code\n",
       "0    594\n",
       "1      4\n",
       "2     22\n",
       "3     32\n",
       "4    343\n",
       "5     11\n",
       "6      6\n",
       "7     36\n",
       "8     14\n",
       "9     57\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_Xy_groups(df: pd.DataFrame):\n",
    "    y = df[\"event_type_code\"].copy()\n",
    "    groups = df[\"well_id\"].copy()\n",
    "\n",
    "    drop_cols = [\"event_type_code\",\"event_type_name\",\"file\",\"run_ts\",\"well_id\"]\n",
    "    X = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    X = X.drop(columns=X.columns[X.isna().all()])          # all-missing\n",
    "    miss = X.isna().mean()\n",
    "    X = X.drop(columns=miss[miss > 0.98].index)            # very high missing\n",
    "    const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "    X = X.drop(columns=const_cols)                          # constants\n",
    "\n",
    "    return X, y, groups\n",
    "\n",
    "X, y, groups = make_Xy_groups(df_ml_well)\n",
    "print(\"X:\", X.shape, \"| y:\", y.shape, \"| wells:\", groups.nunique())\n",
    "display(y.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc1e8e",
   "metadata": {
    "papermill": {
     "duration": 0.004523,
     "end_time": "2026-01-04T13:21:34.634877",
     "exception": false,
     "start_time": "2026-01-04T13:21:34.630354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1eac51",
   "metadata": {
    "papermill": {
     "duration": 0.004518,
     "end_time": "2026-01-04T13:21:34.643941",
     "exception": false,
     "start_time": "2026-01-04T13:21:34.639423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783ff3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:21:34.655787Z",
     "iopub.status.busy": "2026-01-04T13:21:34.654614Z",
     "iopub.status.idle": "2026-01-04T13:34:59.414737Z",
     "shell.execute_reply": "2026-01-04T13:34:59.413654Z"
    },
    "papermill": {
     "duration": 804.772365,
     "end_time": "2026-01-04T13:34:59.420935",
     "exception": false,
     "start_time": "2026-01-04T13:21:34.648570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg: {'macro_f1_mean': 0.22398705848338343, 'macro_f1_std': 0.09435939619898277, 'major_macro_f1_mean': 0.24794970780289896, 'fault_vs_normal_f1_mean': 0.6073600789459409}\n",
      "HGB   : {'macro_f1_mean': 0.4331954202538017, 'macro_f1_std': 0.15897862328939008, 'major_macro_f1_mean': 0.4594612367402339, 'fault_vs_normal_f1_mean': 0.8007543910780439}\n"
     ]
    }
   ],
   "source": [
    "def eval_repeated_group_shuffle(model, X, y, groups, repeats=30, test_size=0.2):\n",
    "    gss = GroupShuffleSplit(n_splits=repeats, test_size=test_size, random_state=42)\n",
    "    major_classes = set(y.value_counts()[lambda s: s >= 10].index)\n",
    "\n",
    "    f1s, f1_major, f1_bin = [], [], []\n",
    "    for tr, te in gss.split(X, y, groups=groups):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        model.fit(Xtr, ytr)\n",
    "        pred = model.predict(Xte)\n",
    "\n",
    "        f1s.append(f1_score(yte, pred, average=\"macro\"))\n",
    "\n",
    "        mask = yte.isin(major_classes)\n",
    "        f1_major.append(f1_score(yte[mask], pred[mask], average=\"macro\") if mask.sum() else np.nan)\n",
    "\n",
    "        yte_bin = (yte != 0).astype(int)\n",
    "        pred_bin = (pred != 0).astype(int)\n",
    "        f1_bin.append(f1_score(yte_bin, pred_bin))\n",
    "\n",
    "    return {\n",
    "        \"macro_f1_mean\": float(np.nanmean(f1s)),\n",
    "        \"macro_f1_std\":  float(np.nanstd(f1s)),\n",
    "        \"major_macro_f1_mean\": float(np.nanmean(f1_major)),\n",
    "        \"fault_vs_normal_f1_mean\": float(np.nanmean(f1_bin)),\n",
    "    }\n",
    "\n",
    "logreg = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=True)),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(max_iter=8000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "class HGBWrapper:\n",
    "    def __init__(self,\n",
    "                 max_depth=6,\n",
    "                 learning_rate=0.06,\n",
    "                 max_iter=700,\n",
    "                 max_leaf_nodes=31,\n",
    "                 min_samples_leaf=20,\n",
    "                 l2_regularization=0.1):\n",
    "        self.clf = HistGradientBoostingClassifier(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iter=max_iter,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            l2_regularization=l2_regularization\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "        w = pd.Series(y).map(dict(zip(classes, cw))).to_numpy()\n",
    "        self.clf.fit(X, y, sample_weight=w)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X.fillna(0))\n",
    "\n",
    "hgb = HGBWrapper()\n",
    "\n",
    "print(\"LogReg:\", eval_repeated_group_shuffle(logreg, X, y, groups, repeats=30))\n",
    "print(\"HGB   :\", eval_repeated_group_shuffle(hgb, X, y, groups, repeats=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f01776",
   "metadata": {
    "papermill": {
     "duration": 0.004675,
     "end_time": "2026-01-04T13:34:59.430327",
     "exception": false,
     "start_time": "2026-01-04T13:34:59.425652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results (Current)\n",
    "- Logistic Regression (repeated group shuffle, 30): macro-F1 ≈ 0.22, fault-vs-normal F1 ≈ 0.61\n",
    "- HistGradientBoosting (repeated group shuffle, 30): macro-F1 ≈ 0.42, fault-vs-normal F1 ≈ 0.76\n",
    "- Note: Macro-F1 is unstable because some classes have very few samples (e.g., class 1 has 4 files).\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 988298,
     "sourceId": 9353254,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2174.232624,
   "end_time": "2026-01-04T13:35:00.056389",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-04T12:58:45.823765",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01fd461e4df340c3b4db50fe0f0e5275": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b30342f0a2a144b7b9fcc4124ccdc566",
       "max": 1119.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_be697e0a05ec436489f315c08af9398e",
       "tabbable": null,
       "tooltip": null,
       "value": 1119.0
      }
     },
     "188b5b6420a84a46958d81c3018535d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19c3e5fac8a6401fa30b0968a6e09543": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e665ac373dc42289d72e1835de6a256": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1ecbf35b57cf49e5839bfa6ec9b3bd8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fb069eaa2ea14af8b096552bf36b58b7",
       "placeholder": "​",
       "style": "IPY_MODEL_aeb41d2a562047ceb82ddd8b5e488099",
       "tabbable": null,
       "tooltip": null,
       "value": " 1119/1119 [01:57&lt;00:00,  6.34it/s]"
      }
     },
     "279c485d9f1d4bb7b88c172cf5ca7054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4f9bc1c8c1c44159b56886e4fc255820",
       "placeholder": "​",
       "style": "IPY_MODEL_2e9a91ce41644b59ab464b6d1f755021",
       "tabbable": null,
       "tooltip": null,
       "value": " 1119/1119 [02:18&lt;00:00,  6.19it/s]"
      }
     },
     "2dc2aa5f9f6547b89fad1b6913fce32f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2e9a91ce41644b59ab464b6d1f755021": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3469e4313d294ed4b4f4ce4dde8783ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b540b148a01f45cfb39bf3bb88be0fd9",
        "IPY_MODEL_b9f32f73174e46d6b109291c4392a49f",
        "IPY_MODEL_1ecbf35b57cf49e5839bfa6ec9b3bd8d"
       ],
       "layout": "IPY_MODEL_188b5b6420a84a46958d81c3018535d7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4805d051ba0b4a0bafadcfcf24cfa3f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49be11de9e6a459f851fb2394404ed4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f9bc1c8c1c44159b56886e4fc255820": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bb93292d2ab464aab0cbbc33ae521f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83596a1369e9416089f673d2a47284bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "acc58cd1930440b195406c3b13b105f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_49be11de9e6a459f851fb2394404ed4c",
       "placeholder": "​",
       "style": "IPY_MODEL_2dc2aa5f9f6547b89fad1b6913fce32f",
       "tabbable": null,
       "tooltip": null,
       "value": "Building WELL dataset: 100%"
      }
     },
     "aeb41d2a562047ceb82ddd8b5e488099": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b30342f0a2a144b7b9fcc4124ccdc566": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b540b148a01f45cfb39bf3bb88be0fd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7bb93292d2ab464aab0cbbc33ae521f8",
       "placeholder": "​",
       "style": "IPY_MODEL_83596a1369e9416089f673d2a47284bf",
       "tabbable": null,
       "tooltip": null,
       "value": "Building WELL dataset: 100%"
      }
     },
     "b9f32f73174e46d6b109291c4392a49f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4805d051ba0b4a0bafadcfcf24cfa3f5",
       "max": 1119.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1e665ac373dc42289d72e1835de6a256",
       "tabbable": null,
       "tooltip": null,
       "value": 1119.0
      }
     },
     "be697e0a05ec436489f315c08af9398e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d477d70aec274c76805009b9e7878336": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_acc58cd1930440b195406c3b13b105f7",
        "IPY_MODEL_01fd461e4df340c3b4db50fe0f0e5275",
        "IPY_MODEL_279c485d9f1d4bb7b88c172cf5ca7054"
       ],
       "layout": "IPY_MODEL_19c3e5fac8a6401fa30b0968a6e09543",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fb069eaa2ea14af8b096552bf36b58b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
