{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f25d9a2",
   "metadata": {
    "papermill": {
     "duration": 0.005943,
     "end_time": "2026-01-08T17:37:33.597597",
     "exception": false,
     "start_time": "2026-01-08T17:37:33.591654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3W Dataset (WELL-only) — Row-per-file Event Classification\n",
    "\n",
    "Goal: Convert each time-series file into a single feature row (leakage-safe by well grouping), then train and evaluate multi-class classifiers for event_type_code (0..9).\n",
    "\n",
    "\n",
    "## Summary (Engineer Story)\n",
    "- Built a file index from the 3W dataset and filtered to **WELL files only** (1119 files, 40 wells).\n",
    "- Cleaned each file: parsed timestamp index, renamed sensors, converted to numeric types.\n",
    "- Engineered **row-per-file features**:\n",
    "  - Continuous sensors: raw median/IQR/last + robust z-stats (mean/std/min/max/last) + delta(first→last) + missing ratio.\n",
    "  - State/valve signals: last state + transition count/rate + time-in-state proportions (+ “other” state).\n",
    "- Evaluated with **Group splits by `well_id`** to avoid leakage across wells (harder but realistic).\n",
    "- Compared baseline (Logistic Regression) vs boosting (HistGradientBoosting with class-weighted sample_weight).\n",
    "- Result: **HGB outperformed baseline**, reaching about **macro-F1 ~0.42 (repeated group shuffle)** and **fault-vs-normal F1 ~0.76**, with variance due to very rare classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7afc9",
   "metadata": {
    "papermill": {
     "duration": 0.005638,
     "end_time": "2026-01-08T17:37:33.607798",
     "exception": false,
     "start_time": "2026-01-08T17:37:33.602160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Paths, random seeds, output folders\n",
    "- Sensor name mapping and label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd26fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:37:33.618928Z",
     "iopub.status.busy": "2026-01-08T17:37:33.618382Z",
     "iopub.status.idle": "2026-01-08T17:37:37.099045Z",
     "shell.execute_reply": "2026-01-08T17:37:37.097892Z"
    },
    "papermill": {
     "duration": 3.488828,
     "end_time": "2026-01-08T17:37:37.101010",
     "exception": false,
     "start_time": "2026-01-08T17:37:33.612182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix\n",
      "CACHE_PATH: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/df_ml_well_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3W Dataset (WELL-only) — Row-per-file Classification (v3_1)\n",
    "# Fully runnable, leakage-safe, interview-ready (+ stability eval)\n",
    "#\n",
    "# ✅ Key upgrades included (your requested 3)\n",
    "# 1) Holdout diagnostics for HGB:\n",
    "#    - confusion matrix (counts) saved as CSV\n",
    "#    - per-class precision/recall/F1/support table saved as CSV\n",
    "# 2) Holdout wells are logged into results_summary.json (traceability)\n",
    "# 3) Probability-imputation semantics fixed:\n",
    "#    - if a state sensor has ALL probability features missing -> set p_other=1 and p_state_* = 0\n",
    "#    - otherwise: fill missing probs with 0.0 and renormalize so probs sum to 1 (per sensor)\n",
    "#    - plus missing indicators for all features (kept)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "BASE = \"/kaggle/input/3w-dataset/2.0.0\"\n",
    "RANDOM_STATE = 42\n",
    "N_WELL_FILES = None  # None = all WELL files\n",
    "\n",
    "FEATURES_VERSION = \"v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix\"\n",
    "OUT_DIR = f\"/kaggle/working/3w_prepared_{FEATURES_VERSION}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CACHE_PATH = f\"{OUT_DIR}/df_ml_well_{FEATURES_VERSION}.parquet\"\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"CACHE_PATH:\", CACHE_PATH)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset constants / mappings\n",
    "# -------------------------\n",
    "VAR_RENAME = {\n",
    "    \"ABER-CKGL\": \"gl_choke_opening_pct\",\n",
    "    \"ABER-CKP\":  \"prod_choke_opening_pct\",\n",
    "    \"ESTADO-DHSV\":   \"dhsv_state\",\n",
    "    \"ESTADO-M1\":     \"prod_master_valve_state\",\n",
    "    \"ESTADO-M2\":     \"ann_master_valve_state\",\n",
    "    \"ESTADO-PXO\":    \"pig_crossover_valve_state\",\n",
    "    \"ESTADO-SDV-GL\": \"gl_shutdown_valve_state\",\n",
    "    \"ESTADO-SDV-P\":  \"prod_shutdown_valve_state\",\n",
    "    \"ESTADO-W1\":     \"prod_wing_valve_state\",\n",
    "    \"ESTADO-W2\":     \"ann_wing_valve_state\",\n",
    "    \"ESTADO-XO\":     \"crossover_valve_state\",\n",
    "    \"P-ANULAR\":     \"annulus_pressure_pa\",\n",
    "    \"P-JUS-BS\":     \"svc_pump_downstream_pressure_pa\",\n",
    "    \"P-JUS-CKGL\":   \"gl_choke_downstream_pressure_pa\",\n",
    "    \"P-JUS-CKP\":    \"prod_choke_downstream_pressure_pa\",\n",
    "    \"P-MON-CKGL\":   \"gl_choke_upstream_pressure_pa\",\n",
    "    \"P-MON-CKP\":    \"prod_choke_upstream_pressure_pa\",\n",
    "    \"P-MON-SDV-P\":  \"prod_sdv_upstream_pressure_pa\",\n",
    "    \"P-PDG\":        \"pdg_downhole_pressure_pa\",\n",
    "    \"PT-P\":         \"xmas_tree_prod_line_pressure_pa\",\n",
    "    \"P-TPT\":        \"tpt_pressure_pa\",\n",
    "    \"QBS\": \"svc_pump_flow_m3s\",\n",
    "    \"QGL\": \"gas_lift_flow_m3s\",\n",
    "    \"T-JUS-CKP\": \"prod_choke_downstream_temp_c\",\n",
    "    \"T-MON-CKP\": \"prod_choke_upstream_temp_c\",\n",
    "    \"T-PDG\":     \"pdg_downhole_temp_c\",\n",
    "    \"T-TPT\":     \"tpt_temp_c\",\n",
    "    \"class\": \"class_code\",\n",
    "    \"state\": \"state_code\",\n",
    "}\n",
    "\n",
    "EVENT_TYPE_CODE_TO_NAME = {\n",
    "    0:\"Normal Operation\", 1:\"Abrupt Increase of BSW\", 2:\"Spurious Closure of DHSV\",\n",
    "    3:\"Severe Slugging\", 4:\"Flow Instability\", 5:\"Rapid Productivity Loss\",\n",
    "    6:\"Quick Restriction in PCK\", 7:\"Scaling in PCK\",\n",
    "    8:\"Hydrate in Production Line\", 9:\"Hydrate in Service Line\",\n",
    "}\n",
    "\n",
    "LABEL_COLS = {\"class_code\", \"state_code\", \"class_label\", \"state_label\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479163fa",
   "metadata": {
    "papermill": {
     "duration": 0.004391,
     "end_time": "2026-01-08T17:37:37.110374",
     "exception": false,
     "start_time": "2026-01-08T17:37:37.105983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Build File Index (Metadata Table)\n",
    "\n",
    "- Scan all parquet files\n",
    "- Extract: event_type_code, source, well_id, run timestamp\n",
    "- Filter to WELL-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3825369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:37:37.121610Z",
     "iopub.status.busy": "2026-01-08T17:37:37.120566Z",
     "iopub.status.idle": "2026-01-08T17:37:43.128966Z",
     "shell.execute_reply": "2026-01-08T17:37:43.128063Z"
    },
    "papermill": {
     "duration": 6.016404,
     "end_time": "2026-01-08T17:37:43.131200",
     "exception": false,
     "start_time": "2026-01-08T17:37:37.114796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 2228 | WELL files: 1119 | Wells: 40\n",
      "Class counts:\n",
      " event_type_code\n",
      "0    594\n",
      "1      4\n",
      "2     22\n",
      "3     32\n",
      "4    343\n",
      "5     11\n",
      "6      6\n",
      "7     36\n",
      "8     14\n",
      "9     57\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Build file index\n",
    "# ============================================================\n",
    "def build_file_index(base: str) -> pd.DataFrame:\n",
    "    paths = []\n",
    "    for root, _, files in os.walk(base):\n",
    "        for f in files:\n",
    "            if f.endswith(\".parquet\"):\n",
    "                paths.append(os.path.join(root, f))\n",
    "\n",
    "    df = pd.DataFrame({\"path\": paths})\n",
    "    codes = df[\"path\"].str.extract(r\"/2\\.0\\.0/(\\d+)/\", expand=False)\n",
    "    df[\"event_type_code\"] = pd.to_numeric(codes, errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df.dropna(subset=[\"event_type_code\"])\n",
    "    df[\"event_type_code\"] = df[\"event_type_code\"].astype(int)\n",
    "\n",
    "    df[\"file\"] = df[\"path\"].str.split(\"/\").str[-1]\n",
    "    df[\"source\"] = df[\"file\"].str.extract(r\"^(WELL|SIMULATED|DRAWN)\")\n",
    "    df[\"well_id\"] = df[\"file\"].str.extract(r\"(WELL-\\d+)\")\n",
    "    df[\"run_ts\"] = df[\"file\"].str.extract(r\"_(\\d{14})\")\n",
    "    df[\"run_ts\"] = pd.to_datetime(df[\"run_ts\"], format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n",
    "\n",
    "    return df.sort_values([\"event_type_code\",\"source\",\"well_id\",\"run_ts\"]).reset_index(drop=True)\n",
    "\n",
    "df_files = build_file_index(BASE)\n",
    "df_w_files = df_files[df_files[\"source\"] == \"WELL\"].copy()\n",
    "assert df_w_files[\"well_id\"].notna().all()\n",
    "\n",
    "print(\"Total files:\", len(df_files), \"| WELL files:\", len(df_w_files), \"| Wells:\", df_w_files[\"well_id\"].nunique())\n",
    "print(\"Class counts:\\n\", df_w_files[\"event_type_code\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94d8a1",
   "metadata": {
    "papermill": {
     "duration": 0.004609,
     "end_time": "2026-01-08T17:37:43.140541",
     "exception": false,
     "start_time": "2026-01-08T17:37:43.135932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Cleaning (Per-file)\n",
    "\n",
    "- Parse timestamps and sort\n",
    "- Rename sensors\n",
    "- Coerce numeric dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18805e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:37:43.151412Z",
     "iopub.status.busy": "2026-01-08T17:37:43.151022Z",
     "iopub.status.idle": "2026-01-08T17:37:43.158113Z",
     "shell.execute_reply": "2026-01-08T17:37:43.157204Z"
    },
    "papermill": {
     "duration": 0.015029,
     "end_time": "2026-01-08T17:37:43.160114",
     "exception": false,
     "start_time": "2026-01-08T17:37:43.145085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Cleaning\n",
    "# ============================================================\n",
    "def clean_3w_instance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "        df = df.set_index(\"timestamp\")\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "\n",
    "    df = df[~df.index.isna()].sort_index()\n",
    "    df.index.name = \"timestamp\"\n",
    "    df = df.rename(columns=VAR_RENAME)\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c in (\"class_code\", \"state_code\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int16\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b0cc4",
   "metadata": {
    "papermill": {
     "duration": 0.004548,
     "end_time": "2026-01-08T17:37:43.169376",
     "exception": false,
     "start_time": "2026-01-08T17:37:43.164828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Feature Engineering (Per-file → Single Row)\n",
    "\n",
    "- Continuous sensors: robust z-stats + deltas between first/last time windows + missingness\n",
    "- State sensors: last state, transition counts/rates, state proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0af273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:37:43.180777Z",
     "iopub.status.busy": "2026-01-08T17:37:43.179920Z",
     "iopub.status.idle": "2026-01-08T17:37:43.200680Z",
     "shell.execute_reply": "2026-01-08T17:37:43.199637Z"
    },
    "papermill": {
     "duration": 0.028752,
     "end_time": "2026-01-08T17:37:43.202745",
     "exception": false,
     "start_time": "2026-01-08T17:37:43.173993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Feature extraction (time-window first/last + last-valid z_last)\n",
    "# ============================================================\n",
    "def summarize_timeseries_v3_1(df_clean: pd.DataFrame, frac: float = 0.1, state_max: int = 3) -> dict:\n",
    "    sensors = df_clean.drop(columns=list(LABEL_COLS), errors=\"ignore\")\n",
    "    num = sensors.select_dtypes(include=[np.number])\n",
    "\n",
    "    out = {\n",
    "        \"n_obs\": int(len(df_clean)),\n",
    "        \"duration_s\": float((df_clean.index.max() - df_clean.index.min()).total_seconds())\n",
    "                      if len(df_clean) else np.nan,\n",
    "    }\n",
    "    if num.shape[1] == 0 or len(num) == 0:\n",
    "        return out\n",
    "\n",
    "    state_cols = [c for c in num.columns if c.endswith(\"_state\")]\n",
    "    cont_cols  = [c for c in num.columns if c not in state_cols]\n",
    "\n",
    "    def _first_last_masks(index, frac: float, n: int):\n",
    "        k = max(1, int(n * frac))\n",
    "        if n < 2:\n",
    "            m = np.ones(n, dtype=bool)\n",
    "            return m, m\n",
    "\n",
    "        try:\n",
    "            idx = pd.DatetimeIndex(index)\n",
    "        except Exception:\n",
    "            idx = None\n",
    "\n",
    "        if idx is not None and idx.notna().all():\n",
    "            tmin, tmax = idx.min(), idx.max()\n",
    "            dur_s = (tmax - tmin).total_seconds()\n",
    "            if np.isfinite(dur_s) and dur_s > 0:\n",
    "                w = dur_s * frac\n",
    "                t_first_end = tmin + pd.Timedelta(seconds=w)\n",
    "                t_last_start = tmax - pd.Timedelta(seconds=w)\n",
    "\n",
    "                first_mask = np.asarray(idx <= t_first_end, dtype=bool)\n",
    "                last_mask  = np.asarray(idx >= t_last_start, dtype=bool)\n",
    "\n",
    "                if first_mask.sum() == 0:\n",
    "                    first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n",
    "                if last_mask.sum() == 0:\n",
    "                    last_mask = np.zeros(n, dtype=bool); last_mask[-k:] = True\n",
    "                return first_mask, last_mask\n",
    "\n",
    "        first_mask = np.zeros(n, dtype=bool); first_mask[:k] = True\n",
    "        last_mask  = np.zeros(n, dtype=bool); last_mask[-k:] = True\n",
    "        return first_mask, last_mask\n",
    "\n",
    "    # Continuous sensors\n",
    "    if len(cont_cols):\n",
    "        cont = num[cont_cols]\n",
    "        med_raw = cont.median()\n",
    "        iqr_raw = (cont.quantile(0.75) - cont.quantile(0.25))\n",
    "\n",
    "        iqr = iqr_raw.replace(0, np.nan)\n",
    "        z = (cont - med_raw) / iqr\n",
    "\n",
    "        first_mask, last_mask = _first_last_masks(df_clean.index, frac=frac, n=len(z))\n",
    "        first = z.iloc[first_mask].mean()\n",
    "        last  = z.iloc[last_mask].mean()\n",
    "\n",
    "        agg = z.agg([\"mean\", \"std\", \"min\", \"max\"]).T\n",
    "        miss = cont.isna().mean()\n",
    "\n",
    "        for col in cont_cols:\n",
    "            out[f\"{col}__raw_median\"] = med_raw[col]\n",
    "            out[f\"{col}__raw_iqr\"]    = iqr_raw[col]\n",
    "\n",
    "            s_raw = cont[col].dropna()\n",
    "            out[f\"{col}__raw_last\"] = s_raw.iloc[-1] if len(s_raw) else np.nan\n",
    "\n",
    "            out[f\"{col}__z_mean\"] = agg.loc[col, \"mean\"]\n",
    "            out[f\"{col}__z_std\"]  = agg.loc[col, \"std\"]\n",
    "            out[f\"{col}__z_min\"]  = agg.loc[col, \"min\"]\n",
    "            out[f\"{col}__z_max\"]  = agg.loc[col, \"max\"]\n",
    "\n",
    "            # last VALID z (not last row)\n",
    "            s_z = z[col].dropna()\n",
    "            out[f\"{col}__z_last\"] = s_z.iloc[-1] if len(s_z) else np.nan\n",
    "\n",
    "            out[f\"{col}__delta_last_first\"] = (last[col] - first[col])\n",
    "            out[f\"{col}__abs_delta\"]        = abs(last[col] - first[col])\n",
    "            out[f\"{col}__missing_frac\"]     = miss[col]\n",
    "\n",
    "    # State-like sensors\n",
    "    if len(state_cols):\n",
    "        st = num[state_cols]\n",
    "        for col in state_cols:\n",
    "            s = st[col]\n",
    "            s_non = s.dropna()\n",
    "\n",
    "            out[f\"{col}__missing_frac\"] = float(s.isna().mean())\n",
    "            out[f\"{col}__last\"] = float(s_non.iloc[-1]) if len(s_non) else np.nan\n",
    "\n",
    "            if len(s_non) >= 2:\n",
    "                n_trans = int((s_non != s_non.shift()).sum() - 1)\n",
    "            else:\n",
    "                n_trans = 0\n",
    "\n",
    "            out[f\"{col}__n_transitions\"] = n_trans\n",
    "            out[f\"{col}__transitions_rate\"] = n_trans / max(1, len(s_non))\n",
    "\n",
    "            known = 0.0\n",
    "            for v in range(state_max + 1):\n",
    "                p = float((s_non == v).mean()) if len(s_non) else np.nan\n",
    "                out[f\"{col}__p_state_{v}\"] = p\n",
    "                if not np.isnan(p):\n",
    "                    known += p\n",
    "            out[f\"{col}__p_state_other\"] = (1.0 - known) if len(s_non) else np.nan\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37185f1",
   "metadata": {
    "papermill": {
     "duration": 0.004705,
     "end_time": "2026-01-08T17:37:43.212247",
     "exception": false,
     "start_time": "2026-01-08T17:37:43.207542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Build Row-per-file Dataset (+ Caching)\n",
    "\n",
    "- Loop over WELL files\n",
    "- Read parquet → clean → summarize → append\n",
    "- Save/load cached parquet to speed up reruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7809dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:37:43.223234Z",
     "iopub.status.busy": "2026-01-08T17:37:43.222863Z",
     "iopub.status.idle": "2026-01-08T17:40:10.221609Z",
     "shell.execute_reply": "2026-01-08T17:40:10.220546Z"
    },
    "papermill": {
     "duration": 147.006665,
     "end_time": "2026-01-08T17:40:10.223639",
     "exception": false,
     "start_time": "2026-01-08T17:37:43.216974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2287d6fa29194a6d9ce58a7bd5de71b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building WELL dataset:   0%|          | 0/1119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built + saved features: (1119, 286)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Build row-per-file dataset (with caching)\n",
    "# ============================================================\n",
    "def build_row_per_file_dataset(df_files: pd.DataFrame, n_files: int | None = None, random_state: int = 42) -> pd.DataFrame:\n",
    "    if n_files is None:\n",
    "        sample = df_files.reset_index(drop=True)\n",
    "    else:\n",
    "        sample = df_files.sample(n_files, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in tqdm(sample.iterrows(), total=len(sample), desc=\"Building WELL dataset\"):\n",
    "        df_raw = pd.read_parquet(r[\"path\"])\n",
    "        df_clean = clean_3w_instance(df_raw)\n",
    "\n",
    "        feats = summarize_timeseries_v3_1(df_clean)\n",
    "        feats[\"event_type_code\"] = int(r[\"event_type_code\"])\n",
    "        feats[\"event_type_name\"] = EVENT_TYPE_CODE_TO_NAME.get(int(r[\"event_type_code\"]), \"Unknown\")\n",
    "        feats[\"well_id\"] = r[\"well_id\"]\n",
    "        feats[\"run_ts\"] = r[\"run_ts\"]\n",
    "        feats[\"file\"] = r[\"file\"]\n",
    "        rows.append(feats)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    df_ml_well = pd.read_parquet(CACHE_PATH)\n",
    "    print(\"Loaded cached features:\", df_ml_well.shape)\n",
    "else:\n",
    "    df_ml_well = build_row_per_file_dataset(df_w_files, n_files=N_WELL_FILES, random_state=RANDOM_STATE)\n",
    "    df_ml_well.to_parquet(CACHE_PATH, index=False)\n",
    "    print(\"Built + saved features:\", df_ml_well.shape)\n",
    "\n",
    "with open(f\"{OUT_DIR}/dataset_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"base\": BASE,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_well_files_used\": int(len(df_ml_well)),\n",
    "        \"features_version\": FEATURES_VERSION,\n",
    "        \"notes\": \"continuous: raw median/iqr/last + robust-z stats + deltas; \"\n",
    "                 \"state: last + transitions + proportions (+other); WELL-only; \"\n",
    "                 \"fixes: time-window first/last + z_last last-valid; \"\n",
    "                 \"imputer: hybrid median/-1 + prob semantic fix + indicators; \"\n",
    "                 \"cache versioned to avoid staleness\"\n",
    "    }, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6fe54",
   "metadata": {
    "papermill": {
     "duration": 0.004994,
     "end_time": "2026-01-08T17:40:10.233636",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.228642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Prepare ML Matrices\n",
    "\n",
    "- X: engineered features\n",
    "- y: event_type_code\n",
    "- groups: well_id (for leakage-safe splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c58ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:40:10.245606Z",
     "iopub.status.busy": "2026-01-08T17:40:10.244607Z",
     "iopub.status.idle": "2026-01-08T17:40:10.258865Z",
     "shell.execute_reply": "2026-01-08T17:40:10.257619Z"
    },
    "papermill": {
     "duration": 0.022604,
     "end_time": "2026-01-08T17:40:10.260984",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.238380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1119, 281) | y: (1119,) | wells: 40\n",
      "Label counts:\n",
      " event_type_code\n",
      "0    594\n",
      "1      4\n",
      "2     22\n",
      "3     32\n",
      "4    343\n",
      "5     11\n",
      "6      6\n",
      "7     36\n",
      "8     14\n",
      "9     57\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5) Build X/y/groups (NO global filtering leakage)\n",
    "# ============================================================\n",
    "def make_Xy_groups(df: pd.DataFrame):\n",
    "    y = df[\"event_type_code\"].astype(int).copy()\n",
    "    groups = df[\"well_id\"].astype(str).copy()\n",
    "    drop_cols = [\"event_type_code\",\"event_type_name\",\"file\",\"run_ts\",\"well_id\"]\n",
    "    X = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    return X, y, groups\n",
    "\n",
    "X, y, groups = make_Xy_groups(df_ml_well)\n",
    "print(\"X:\", X.shape, \"| y:\", y.shape, \"| wells:\", groups.nunique())\n",
    "print(\"Label counts:\\n\", y.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973a9b3",
   "metadata": {
    "papermill": {
     "duration": 0.004709,
     "end_time": "2026-01-08T17:40:10.270609",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.265900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Leakage-safe Preprocessing داخل Pipeline\n",
    "\n",
    "- Train-only column filtering\n",
    "- Hybrid imputation with probability semantics fix\n",
    "- Missing indicators for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97efa3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:40:10.282371Z",
     "iopub.status.busy": "2026-01-08T17:40:10.282033Z",
     "iopub.status.idle": "2026-01-08T17:40:10.306272Z",
     "shell.execute_reply": "2026-01-08T17:40:10.305473Z"
    },
    "papermill": {
     "duration": 0.03293,
     "end_time": "2026-01-08T17:40:10.308382",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.275452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Train-only column filter (inside pipeline)\n",
    "# ============================================================\n",
    "class TrainOnlyColumnFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_threshold: float = 0.98):\n",
    "        self.missing_threshold = missing_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy().replace([np.inf, -np.inf], np.nan)\n",
    "        all_missing = X.columns[X.isna().all()]\n",
    "        miss = X.isna().mean()\n",
    "        high_missing = miss[miss > self.missing_threshold].index\n",
    "        const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "        drop = set(all_missing) | set(high_missing) | set(const_cols)\n",
    "        self.keep_columns_ = [c for c in X.columns if c not in drop]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy().replace([np.inf, -np.inf], np.nan)\n",
    "        for c in getattr(self, \"keep_columns_\", []):\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "        return X[self.keep_columns_]\n",
    "\n",
    "# ============================================================\n",
    "# 7) Hybrid imputer (probability semantics fixed)\n",
    "#    - median for continuous\n",
    "#    - -1 for state-ish \"level/last/transition\" features\n",
    "#    - probability block per state sensor:\n",
    "#         * if ALL probs missing -> p_other=1, p_state_*=0\n",
    "#         * else fill NaN with 0 and renormalize to sum to 1\n",
    "#    - + missing indicators for all columns\n",
    "# ============================================================\n",
    "class HybridImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, state_token: str = \"_state\", fill_value_state: float = -1.0):\n",
    "        self.state_token = state_token\n",
    "        self.fill_value_state = fill_value_state\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_prob_block(col: str):\n",
    "        \"\"\"\n",
    "        Expect columns like: <sensor>_state__p_state_<k> or <sensor>_state__p_state_other\n",
    "        Returns (base_sensor, is_other, k or None) or None if not match.\n",
    "        \"\"\"\n",
    "        # examples:\n",
    "        # dhsv_state__p_state_0\n",
    "        # dhsv_state__p_state_other\n",
    "        m_k = re.match(r\"^(.*_state)__p_state_(\\d+)$\", col)\n",
    "        if m_k:\n",
    "            return m_k.group(1), False, int(m_k.group(2))\n",
    "        m_o = re.match(r\"^(.*_state)__p_state_other$\", col)\n",
    "        if m_o:\n",
    "            return m_o.group(1), True, None\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.cols_ = list(X.columns)\n",
    "\n",
    "        # any feature derived from a state sensor has \"_state\" in name\n",
    "        state_like = [c for c in self.cols_ if self.state_token in c]\n",
    "\n",
    "        # probability columns + group them by state sensor base\n",
    "        prob_cols = []\n",
    "        prob_groups = {}  # base -> {\"other\": col or None, \"states\": {k: col}}\n",
    "        for c in self.cols_:\n",
    "            parsed = self._parse_prob_block(c)\n",
    "            if parsed is None:\n",
    "                continue\n",
    "            base, is_other, k = parsed\n",
    "            prob_cols.append(c)\n",
    "            prob_groups.setdefault(base, {\"other\": None, \"states\": {}})\n",
    "            if is_other:\n",
    "                prob_groups[base][\"other\"] = c\n",
    "            else:\n",
    "                prob_groups[base][\"states\"][k] = c\n",
    "\n",
    "        self.prob_cols_ = prob_cols\n",
    "        self.prob_groups_ = prob_groups\n",
    "\n",
    "        # remaining state-like (non-prob) -> sentinel -1\n",
    "        self.state_cols_ = [c for c in state_like if c not in self.prob_cols_]\n",
    "\n",
    "        # continuous-ish are not state-derived -> median\n",
    "        self.cont_cols_ = [c for c in self.cols_ if c not in state_like]\n",
    "        self.cont_medians_ = (\n",
    "            X[self.cont_cols_].median(numeric_only=True)\n",
    "            if len(self.cont_cols_) else pd.Series(dtype=float)\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Ensure all fit columns exist\n",
    "        for c in self.cols_:\n",
    "            if c not in X.columns:\n",
    "                X[c] = np.nan\n",
    "        X = X[self.cols_]\n",
    "\n",
    "        # Missing indicators\n",
    "        miss = X.isna().astype(np.int8)\n",
    "        miss.columns = [f\"{c}__isna\" for c in miss.columns]\n",
    "\n",
    "        # Continuous: median then fallback to 0.0 if median was NaN\n",
    "        if len(self.cont_cols_):\n",
    "            X[self.cont_cols_] = X[self.cont_cols_].fillna(self.cont_medians_)\n",
    "            X[self.cont_cols_] = X[self.cont_cols_].fillna(0.0)\n",
    "\n",
    "        # Probabilities: semantic fix per sensor group\n",
    "        # - if all probs are NaN -> set p_other=1, p_state_k=0\n",
    "        # - else fill NaN with 0 and renormalize\n",
    "        if len(self.prob_cols_):\n",
    "            # work group-by-group\n",
    "            for base, g in self.prob_groups_.items():\n",
    "                other_col = g.get(\"other\", None)\n",
    "                state_map = g.get(\"states\", {})\n",
    "                cols = []\n",
    "                if other_col is not None:\n",
    "                    cols.append(other_col)\n",
    "                cols.extend([state_map[k] for k in sorted(state_map.keys())])\n",
    "\n",
    "                if not cols:\n",
    "                    continue\n",
    "\n",
    "                block = X[cols]\n",
    "                all_nan = block.isna().all(axis=1)\n",
    "\n",
    "                # initialize by filling NaNs with 0\n",
    "                block_filled = block.fillna(0.0)\n",
    "\n",
    "                # rows where all probs were missing: force p_other=1 and others 0\n",
    "                if other_col is not None:\n",
    "                    block_filled.loc[all_nan, other_col] = 1.0\n",
    "                # ensure all p_state_k are 0 for all_nan rows\n",
    "                for c in cols:\n",
    "                    if c != other_col:\n",
    "                        block_filled.loc[all_nan, c] = 0.0\n",
    "\n",
    "                # renormalize rows where NOT all_nan (sum>0) to sum to 1\n",
    "                not_all = ~all_nan\n",
    "                if not_all.any():\n",
    "                    s = block_filled.loc[not_all, cols].sum(axis=1)\n",
    "                    # if sum is 0 (possible if everything was NaN but other_col missing in schema)\n",
    "                    # fall back to \"other=1\" when available, else leave zeros.\n",
    "                    zero_sum = s == 0\n",
    "                    if zero_sum.any() and other_col is not None:\n",
    "                        block_filled.loc[not_all & zero_sum, other_col] = 1.0\n",
    "                        for c in cols:\n",
    "                            if c != other_col:\n",
    "                                block_filled.loc[not_all & zero_sum, c] = 0.0\n",
    "                        s = block_filled.loc[not_all, cols].sum(axis=1)\n",
    "\n",
    "                    # divide by sum safely\n",
    "                    block_filled.loc[not_all, cols] = block_filled.loc[not_all, cols].div(s, axis=0)\n",
    "\n",
    "                X[cols] = block_filled\n",
    "\n",
    "        # State-level / transition features: fill missing with -1 sentinel\n",
    "        if len(self.state_cols_):\n",
    "            X[self.state_cols_] = X[self.state_cols_].fillna(self.fill_value_state)\n",
    "\n",
    "        return pd.concat([X, miss], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f032b",
   "metadata": {
    "papermill": {
     "duration": 0.004851,
     "end_time": "2026-01-08T17:40:10.318339",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.313488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Models\n",
    "\n",
    "- Baseline: Logistic Regression\n",
    "- Main: Weighted HistGradientBoosting (class-balanced sample weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ce45e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:40:10.330338Z",
     "iopub.status.busy": "2026-01-08T17:40:10.329297Z",
     "iopub.status.idle": "2026-01-08T17:40:10.339419Z",
     "shell.execute_reply": "2026-01-08T17:40:10.338386Z"
    },
    "papermill": {
     "duration": 0.01852,
     "end_time": "2026-01-08T17:40:10.341672",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.323152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) Weighted HGB estimator (cloneable) + predict_proba\n",
    "# ============================================================\n",
    "class WeightedHGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=700,\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf_ = HistGradientBoostingClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            max_iter=self.max_iter,\n",
    "            max_leaf_nodes=self.max_leaf_nodes,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            l2_regularization=self.l2_regularization,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "        y_arr = np.asarray(y)\n",
    "        classes = np.unique(y_arr)\n",
    "        cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_arr)\n",
    "        cw_map = dict(zip(classes, cw))\n",
    "        sample_weight = np.array([cw_map[v] for v in y_arr], dtype=float)\n",
    "\n",
    "        self.clf_.fit(X, y_arr, sample_weight=sample_weight)\n",
    "        self.classes_ = classes\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf_.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf_.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27490ebe",
   "metadata": {
    "papermill": {
     "duration": 0.004737,
     "end_time": "2026-01-08T17:40:10.351526",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.346789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "- Final holdout split by well_id (GroupShuffleSplit)\n",
    "- CV on train wells only (StratifiedGroupKFold)\n",
    "- Stability: repeated group holdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3814c27c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:40:10.363220Z",
     "iopub.status.busy": "2026-01-08T17:40:10.362852Z",
     "iopub.status.idle": "2026-01-08T17:40:10.387543Z",
     "shell.execute_reply": "2026-01-08T17:40:10.386578Z"
    },
    "papermill": {
     "duration": 0.033436,
     "end_time": "2026-01-08T17:40:10.389675",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.356239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9) Evaluation helpers\n",
    "# ============================================================\n",
    "def choose_stratified_group_n_splits(X, y, groups, max_splits=4, random_state=42) -> int:\n",
    "    \"\"\"Pick the largest n_splits (<= max_splits) that doesn't raise ValueError.\"\"\"\n",
    "    for k in range(max_splits, 1, -1):\n",
    "        try:\n",
    "            cv = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "            _ = next(cv.split(X, y, groups=groups))\n",
    "            return k\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return 2\n",
    "\n",
    "def evaluate_stratified_group_cv(model, X, y, groups, n_splits=4, random_state=42, major_min_support=10):\n",
    "    labels = np.sort(pd.unique(y))\n",
    "    cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    macro_f1s, major_f1s, bin_f1s = [], [], []\n",
    "    per_class_f1 = {int(lbl): [] for lbl in labels}\n",
    "\n",
    "    for _, (tr, te) in enumerate(cv.split(X, y, groups=groups), start=1):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        m = clone(model)\n",
    "        m.fit(Xtr, ytr)\n",
    "        pred = m.predict(Xte)\n",
    "\n",
    "        macro_f1s.append(f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0))\n",
    "\n",
    "        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n",
    "        if major_classes:\n",
    "            major_labels = np.array(sorted(major_classes))\n",
    "            major_mask = yte.isin(major_labels)\n",
    "            if major_mask.any():\n",
    "                major_f1s.append(\n",
    "                    f1_score(\n",
    "                        yte[major_mask], pred[major_mask],\n",
    "                        average=\"macro\", labels=major_labels, zero_division=0\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                major_f1s.append(np.nan)\n",
    "        else:\n",
    "            major_f1s.append(np.nan)\n",
    "\n",
    "        bin_f1s.append(\n",
    "            f1_score((yte != 0).astype(int), (pred != 0).astype(int), zero_division=0)\n",
    "        )\n",
    "\n",
    "        rep = classification_report(yte, pred, labels=labels, output_dict=True, zero_division=0)\n",
    "        for lbl in labels:\n",
    "            per_class_f1[int(lbl)].append(rep[str(int(lbl))][\"f1-score\"])\n",
    "\n",
    "    return {\n",
    "        \"macro_f1_mean\": float(np.nanmean(macro_f1s)),\n",
    "        \"macro_f1_std\":  float(np.nanstd(macro_f1s)),\n",
    "        \"major_macro_f1_mean\": float(np.nanmean(major_f1s)),\n",
    "        \"fault_vs_normal_f1_mean\": float(np.nanmean(bin_f1s)),\n",
    "        \"per_class_f1_mean\": {str(k): float(np.nanmean(v)) for k, v in per_class_f1.items()},\n",
    "    }\n",
    "\n",
    "def evaluate_on_holdout(model, Xtr, ytr, Xho, yho, major_min_support=10):\n",
    "    labels = np.sort(pd.unique(pd.concat([ytr, yho], axis=0)))\n",
    "\n",
    "    m = clone(model)\n",
    "    m.fit(Xtr, ytr)\n",
    "    pred = m.predict(Xho)\n",
    "\n",
    "    macro = f1_score(yho, pred, average=\"macro\", labels=labels, zero_division=0)\n",
    "\n",
    "    major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n",
    "    major_labels = np.array(sorted(major_classes)) if major_classes else np.array([], dtype=int)\n",
    "    if len(major_labels) and yho.isin(major_labels).any():\n",
    "        major = f1_score(\n",
    "            yho[yho.isin(major_labels)], pred[yho.isin(major_labels)],\n",
    "            average=\"macro\", labels=major_labels, zero_division=0\n",
    "        )\n",
    "    else:\n",
    "        major = np.nan\n",
    "\n",
    "    bin_f1 = f1_score((yho != 0).astype(int), (pred != 0).astype(int), zero_division=0)\n",
    "    report_text = classification_report(yho, pred, labels=labels, zero_division=0)\n",
    "    report_dict = classification_report(yho, pred, labels=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"macro_f1\": float(macro),\n",
    "        \"major_macro_f1\": float(major) if np.isfinite(major) else np.nan,\n",
    "        \"fault_vs_normal_f1\": float(bin_f1),\n",
    "        \"classification_report\": report_text,\n",
    "        \"classification_report_dict\": report_dict,  # for tables\n",
    "        \"labels\": labels,\n",
    "        \"y_pred\": pred,\n",
    "    }\n",
    "\n",
    "def repeated_holdout(model, X, y, groups, repeats=30, test_size=0.2, random_state=42, major_min_support=10):\n",
    "    labels = np.sort(pd.unique(y))\n",
    "    gss = GroupShuffleSplit(n_splits=repeats, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    macs, bins, majors = [], [], []\n",
    "    for tr, te in gss.split(X, y, groups=groups):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "        m = clone(model)\n",
    "        m.fit(Xtr, ytr)\n",
    "        pred = m.predict(Xte)\n",
    "\n",
    "        macs.append(f1_score(yte, pred, average=\"macro\", labels=labels, zero_division=0))\n",
    "        bins.append(f1_score((yte != 0).astype(int), (pred != 0).astype(int), zero_division=0))\n",
    "\n",
    "        major_classes = set(ytr.value_counts()[lambda s: s >= major_min_support].index.astype(int))\n",
    "        major_labels = np.array(sorted(major_classes)) if major_classes else np.array([], dtype=int)\n",
    "        if len(major_labels) and yte.isin(major_labels).any():\n",
    "            majors.append(\n",
    "                f1_score(\n",
    "                    yte[yte.isin(major_labels)], pred[yte.isin(major_labels)],\n",
    "                    average=\"macro\", labels=major_labels, zero_division=0\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            majors.append(np.nan)\n",
    "\n",
    "    return {\n",
    "        \"macro_f1_mean\": float(np.nanmean(macs)),\n",
    "        \"macro_f1_std\": float(np.nanstd(macs)),\n",
    "        \"major_macro_f1_mean\": float(np.nanmean(majors)),\n",
    "        \"fault_vs_normal_f1_mean\": float(np.nanmean(bins)),\n",
    "        \"fault_vs_normal_f1_std\": float(np.nanstd(bins)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fef5aa",
   "metadata": {
    "papermill": {
     "duration": 0.004777,
     "end_time": "2026-01-08T17:40:10.399430",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.394653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Diagnostics & Explainability\n",
    "\n",
    "- Confusion matrix (CSV)\n",
    "- Top confusions table (CSV)\n",
    "- Per-class metrics (CSV)\n",
    "- Permutation importance on holdout (CSV)\n",
    "- Per-well performance (CSV)\n",
    "- Save a single results_summary.json with full traceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d76191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:40:10.412613Z",
     "iopub.status.busy": "2026-01-08T17:40:10.412260Z",
     "iopub.status.idle": "2026-01-08T18:07:17.711120Z",
     "shell.execute_reply": "2026-01-08T18:07:17.709931Z"
    },
    "papermill": {
     "duration": 1627.313699,
     "end_time": "2026-01-08T18:07:17.718766",
     "exception": false,
     "start_time": "2026-01-08T17:40:10.405067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train wells: 32 | Holdout wells: 8\n",
      "Holdout wells: ['WELL-00005', 'WELL-00013', 'WELL-00016', 'WELL-00019', 'WELL-00022', 'WELL-00029', 'WELL-00030', 'WELL-00040']\n",
      "Holdout label counts:\n",
      " event_type_code\n",
      "0    82\n",
      "2     1\n",
      "4    38\n",
      "5     4\n",
      "7     8\n",
      "8     6\n",
      "9     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Using StratifiedGroupKFold with n_splits=4\n",
      "\n",
      "=== CV on TRAIN ONLY ===\n",
      "LogReg CV: {'macro_f1_mean': 0.23411257009793876, 'macro_f1_std': 0.11036159661262168, 'major_macro_f1_mean': 0.3466105934245418, 'fault_vs_normal_f1_mean': 0.6515985012280396, 'per_class_f1_mean': {'0': 0.022547945205479453, '1': 0.0, '2': 0.5570821185617104, '3': 0.0, '4': 0.35916489738145785, '5': 0.0, '6': 0.0, '7': 0.4297297297297298, '8': 0.25, '9': 0.7226010101010102}}\n",
      "HGB   CV: {'macro_f1_mean': 0.414913949489049, 'macro_f1_std': 0.04739455413528661, 'major_macro_f1_mean': 0.6226991965158268, 'fault_vs_normal_f1_mean': 0.9249655129770701, 'per_class_f1_mean': {'0': 0.6873414934167565, '1': 0.08333333333333333, '2': 0.9074074074074073, '3': 0.0, '4': 0.8526504132709201, '5': 0.0, '6': 0.0, '7': 0.33461538461538465, '8': 0.5416666666666666, '9': 0.7421247961800218}}\n",
      "\n",
      "=== FINAL HOLDOUT (train wells -> holdout wells) ===\n",
      "LogReg Holdout: {'macro_f1': 0.24583333333333335, 'major_macro_f1': 0.34956880389951256, 'fault_vs_normal_f1': 0.3563218390804598}\n",
      "HGB   Holdout: {'macro_f1': 0.46379380829345, 'major_macro_f1': 0.5952119027113056, 'fault_vs_normal_f1': 0.9606299212598425}\n",
      "\n",
      "LogReg classification report (holdout):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        82\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.09      0.21      0.12        38\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       1.00      0.25      0.40         8\n",
      "           8       0.75      0.50      0.60         6\n",
      "           9       0.25      0.50      0.33         4\n",
      "\n",
      "    accuracy                           0.11       143\n",
      "   macro avg       0.31      0.25      0.25       143\n",
      "weighted avg       0.13      0.11      0.10       143\n",
      "\n",
      "\n",
      "HGB classification report (holdout):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        82\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.20      1.00      0.33         1\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.93      1.00      0.96        38\n",
      "           5       1.00      0.25      0.40         4\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       1.00      0.38      0.55         8\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.30      0.75      0.43         4\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.54      0.53      0.46       143\n",
      "weighted avg       0.96      0.90      0.91       143\n",
      "\n",
      "\n",
      "Saved HGB holdout confusion matrix: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/hgb_holdout_confusion_matrix.csv\n",
      "Saved top confusions table: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/hgb_holdout_top_confusions.csv\n",
      "\n",
      "Top-3 confusions (HGB holdout):\n",
      " true_class               true_name  pred_class                pred_name  count  pct_of_true_class\n",
      "          7          Scaling in PCK           9  Hydrate in Service Line      5           0.625000\n",
      "          5 Rapid Productivity Loss           2 Spurious Closure of DHSV      3           0.750000\n",
      "          0        Normal Operation           4         Flow Instability      3           0.036585\n",
      "\n",
      "Saved permutation importance: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/hgb_holdout_permutation_importance_f1macro.csv\n",
      "\n",
      "Top-15 important features (perm importance, macro-F1):\n",
      "                                    feature  importance_mean  importance_std\n",
      "                                      n_obs         0.251678        0.041485\n",
      "               annulus_pressure_pa__raw_iqr         0.148301        0.006111\n",
      "     prod_choke_upstream_pressure_pa__z_max         0.099141        0.034263\n",
      "   prod_choke_upstream_pressure_pa__raw_iqr         0.057163        0.010074\n",
      "      prod_choke_downstream_temp_c__raw_iqr         0.045720        0.009598\n",
      "              gas_lift_flow_m3s__raw_median         0.039007        0.008398\n",
      "  prod_choke_upstream_pressure_pa__raw_last         0.035381        0.008083\n",
      "            ann_wing_valve_state__p_state_1         0.030864        0.020223\n",
      "gl_choke_downstream_pressure_pa__raw_median         0.028279        0.057146\n",
      "              pdg_downhole_temp_c__raw_last         0.027797        0.010506\n",
      "  gl_choke_downstream_pressure_pa__raw_last         0.025531        0.022791\n",
      " prod_choke_downstream_temp_c__missing_frac         0.017231        0.000430\n",
      "           prod_choke_opening_pct__raw_last         0.014935        0.005645\n",
      "          ann_master_valve_state__p_state_0         0.014715        0.025487\n",
      "          ann_master_valve_state__p_state_1         0.014715        0.025487\n",
      "\n",
      "Saved per-well metrics: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/hgb_holdout_per_well_metrics.csv\n",
      "\n",
      "Worst 5 wells by macro-F1 (HGB holdout):\n",
      "   well_id  n_samples  macro_f1  fault_vs_normal_f1\n",
      "WELL-00022          8  0.054545            1.000000\n",
      "WELL-00019          5  0.100000            0.888889\n",
      "WELL-00013          1  0.100000            1.000000\n",
      "WELL-00029          1  0.100000            1.000000\n",
      "WELL-00030          1  0.100000            1.000000\n",
      "\n",
      "Best 5 wells by macro-F1 (HGB holdout):\n",
      "   well_id  n_samples  macro_f1  fault_vs_normal_f1\n",
      "WELL-00029          1  0.100000                1.00\n",
      "WELL-00030          1  0.100000                1.00\n",
      "WELL-00040          1  0.100000                1.00\n",
      "WELL-00016          7  0.120000                1.00\n",
      "WELL-00005        119  0.193671                0.95\n",
      "Saved HGB holdout per-class metrics table: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/hgb_holdout_per_class_metrics.csv\n",
      "\n",
      "HGB holdout per-class recall (quick view):\n",
      " class                       name   recall  support\n",
      "     0           Normal Operation 0.939024     82.0\n",
      "     1     Abrupt Increase of BSW 0.000000      0.0\n",
      "     2   Spurious Closure of DHSV 1.000000      1.0\n",
      "     3            Severe Slugging 0.000000      0.0\n",
      "     4           Flow Instability 1.000000     38.0\n",
      "     5    Rapid Productivity Loss 0.250000      4.0\n",
      "     6   Quick Restriction in PCK 0.000000      0.0\n",
      "     7             Scaling in PCK 0.375000      8.0\n",
      "     8 Hydrate in Production Line 1.000000      6.0\n",
      "     9    Hydrate in Service Line 0.750000      4.0\n",
      "\n",
      "=== Repeated Group Holdout (30 splits) ===\n",
      "LogReg: {'macro_f1_mean': 0.197535906867509, 'macro_f1_std': 0.08372364490217928, 'major_macro_f1_mean': 0.24975827655561952, 'fault_vs_normal_f1_mean': 0.6084785740724687, 'fault_vs_normal_f1_std': 0.2423381159575943}\n",
      "HGB   : {'macro_f1_mean': 0.38878672931629693, 'macro_f1_std': 0.1110308237109982, 'major_macro_f1_mean': 0.48737787121416637, 'fault_vs_normal_f1_mean': 0.8875212104777906, 'fault_vs_normal_f1_std': 0.13754766056670029}\n",
      "\n",
      "Saved: /kaggle/working/3w_prepared_v3_1_fixed_timewin_zlast_hybridImputer_probSemanticFix/results_summary.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10) Models (consistent preprocessing)\n",
    "# ============================================================\n",
    "preprocess_common = [\n",
    "    (\"col_filter\", TrainOnlyColumnFilter(missing_threshold=0.98)),\n",
    "    (\"imputer\", HybridImputer(state_token=\"_state\", fill_value_state=-1.0)),\n",
    "]\n",
    "\n",
    "logreg = Pipeline(preprocess_common + [\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(max_iter=8000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "hgb = Pipeline(preprocess_common + [\n",
    "    (\"clf\", WeightedHGBClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.06,\n",
    "        max_iter=700,\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# 11) Final holdout by WELL + CV on train only + repeated holdout\n",
    "#     + NEW: Save holdout diagnostics (HGB confusion matrix + per-class table)\n",
    "#     + NEW: Log holdout well IDs in JSON\n",
    "# ============================================================\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "tr_idx, ho_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_tr, y_tr, g_tr = X.iloc[tr_idx], y.iloc[tr_idx], groups.iloc[tr_idx]\n",
    "X_ho, y_ho, g_ho = X.iloc[ho_idx], y.iloc[ho_idx], groups.iloc[ho_idx]\n",
    "\n",
    "holdout_wells = sorted(g_ho.unique().tolist())\n",
    "train_wells = sorted(g_tr.unique().tolist())\n",
    "\n",
    "print(\"\\nTrain wells:\", len(train_wells), \"| Holdout wells:\", len(holdout_wells))\n",
    "print(\"Holdout wells:\", holdout_wells)\n",
    "print(\"Holdout label counts:\\n\", y_ho.value_counts().sort_index())\n",
    "\n",
    "n_splits = choose_stratified_group_n_splits(X_tr, y_tr, g_tr, max_splits=4, random_state=RANDOM_STATE)\n",
    "print(f\"\\nUsing StratifiedGroupKFold with n_splits={n_splits}\")\n",
    "\n",
    "print(\"\\n=== CV on TRAIN ONLY ===\")\n",
    "logreg_cv = evaluate_stratified_group_cv(logreg, X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\n",
    "hgb_cv    = evaluate_stratified_group_cv(hgb,   X_tr, y_tr, g_tr, n_splits=n_splits, random_state=RANDOM_STATE)\n",
    "print(\"LogReg CV:\", logreg_cv)\n",
    "print(\"HGB   CV:\", hgb_cv)\n",
    "\n",
    "print(\"\\n=== FINAL HOLDOUT (train wells -> holdout wells) ===\")\n",
    "logreg_hold = evaluate_on_holdout(logreg, X_tr, y_tr, X_ho, y_ho)\n",
    "hgb_hold    = evaluate_on_holdout(hgb,   X_tr, y_tr, X_ho, y_ho)\n",
    "\n",
    "print(\"LogReg Holdout:\", {k: v for k, v in logreg_hold.items() if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}})\n",
    "print(\"HGB   Holdout:\", {k: v for k, v in hgb_hold.items() if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}})\n",
    "\n",
    "print(\"\\nLogReg classification report (holdout):\\n\", logreg_hold[\"classification_report\"])\n",
    "print(\"\\nHGB classification report (holdout):\\n\", hgb_hold[\"classification_report\"])\n",
    "\n",
    "# ---- NEW: Holdout diagnostics for HGB (confusion matrix + per-class table)\n",
    "labels = hgb_hold[\"labels\"]\n",
    "pred_hgb = hgb_hold[\"y_pred\"]\n",
    "\n",
    "cm = confusion_matrix(y_ho, pred_hgb, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
    "cm_path = f\"{OUT_DIR}/hgb_holdout_confusion_matrix.csv\"\n",
    "cm_df.to_csv(cm_path, index=True)\n",
    "print(f\"\\nSaved HGB holdout confusion matrix: {cm_path}\")\n",
    "\n",
    "\n",
    "# ---- (A) Error analysis: Top confusions (off-diagonal)\n",
    "cm_counts = cm.copy()\n",
    "np.fill_diagonal(cm_counts, 0)\n",
    "\n",
    "row_support = cm.sum(axis=1)  # support per true class\n",
    "pairs = []\n",
    "for i, t in enumerate(labels):\n",
    "    for j, p in enumerate(labels):\n",
    "        if i == j:\n",
    "            continue\n",
    "        c = int(cm_counts[i, j])\n",
    "        if c > 0:\n",
    "            pairs.append({\n",
    "                \"true_class\": int(t),\n",
    "                \"true_name\": EVENT_TYPE_CODE_TO_NAME.get(int(t), \"Unknown\"),\n",
    "                \"pred_class\": int(p),\n",
    "                \"pred_name\": EVENT_TYPE_CODE_TO_NAME.get(int(p), \"Unknown\"),\n",
    "                \"count\": c,\n",
    "                \"pct_of_true_class\": c / max(1, int(row_support[i]))\n",
    "            })\n",
    "\n",
    "top_conf_df = (pd.DataFrame(pairs)\n",
    "               .sort_values([\"count\", \"pct_of_true_class\"], ascending=False)\n",
    "               .head(10))\n",
    "\n",
    "top_conf_path = f\"{OUT_DIR}/hgb_holdout_top_confusions.csv\"\n",
    "top_conf_df.to_csv(top_conf_path, index=False)\n",
    "print(f\"Saved top confusions table: {top_conf_path}\")\n",
    "\n",
    "print(\"\\nTop-3 confusions (HGB holdout):\")\n",
    "print(top_conf_df.head(3).to_string(index=False))\n",
    "\n",
    "# ---- (B) Permutation importance on HOLDOUT (macro-F1)\n",
    "hgb_final = clone(hgb).fit(X_tr, y_tr)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    hgb_final,\n",
    "    X_ho, y_ho,\n",
    "    n_repeats=8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "perm_df = (pd.DataFrame({\n",
    "    \"feature\": X_ho.columns,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False))\n",
    "\n",
    "perm_path = f\"{OUT_DIR}/hgb_holdout_permutation_importance_f1macro.csv\"\n",
    "perm_df.to_csv(perm_path, index=False)\n",
    "print(f\"\\nSaved permutation importance: {perm_path}\")\n",
    "\n",
    "print(\"\\nTop-15 important features (perm importance, macro-F1):\")\n",
    "print(perm_df.head(15).to_string(index=False))\n",
    "\n",
    "# ---- (C) Per-well performance on HOLDOUT\n",
    "pred_hgb_final = hgb_final.predict(X_ho)\n",
    "\n",
    "df_well_perf = pd.DataFrame({\n",
    "    \"well_id\": g_ho.values,\n",
    "    \"y_true\": y_ho.values,\n",
    "    \"y_pred\": pred_hgb_final\n",
    "})\n",
    "\n",
    "well_rows = []\n",
    "for well, gdf in df_well_perf.groupby(\"well_id\"):\n",
    "    yt = gdf[\"y_true\"].astype(int)\n",
    "    yp = gdf[\"y_pred\"].astype(int)\n",
    "    well_rows.append({\n",
    "        \"well_id\": well,\n",
    "        \"n_samples\": int(len(gdf)),\n",
    "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", labels=labels, zero_division=0)),\n",
    "        \"fault_vs_normal_f1\": float(f1_score((yt != 0).astype(int), (yp != 0).astype(int), zero_division=0)),\n",
    "    })\n",
    "\n",
    "well_metrics_df = (pd.DataFrame(well_rows)\n",
    "                   .sort_values([\"macro_f1\", \"n_samples\"], ascending=[True, False]))\n",
    "\n",
    "well_metrics_path = f\"{OUT_DIR}/hgb_holdout_per_well_metrics.csv\"\n",
    "well_metrics_df.to_csv(well_metrics_path, index=False)\n",
    "print(f\"\\nSaved per-well metrics: {well_metrics_path}\")\n",
    "\n",
    "print(\"\\nWorst 5 wells by macro-F1 (HGB holdout):\")\n",
    "print(well_metrics_df.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\nBest 5 wells by macro-F1 (HGB holdout):\")\n",
    "print(well_metrics_df.tail(5).to_string(index=False))\n",
    "\n",
    "\n",
    "rep_dict = hgb_hold[\"classification_report_dict\"]\n",
    "rows = []\n",
    "for l in labels:\n",
    "    key = str(int(l))\n",
    "    if key in rep_dict:\n",
    "        rows.append({\n",
    "            \"class\": int(l),\n",
    "            \"name\": EVENT_TYPE_CODE_TO_NAME.get(int(l), \"Unknown\"),\n",
    "            \"precision\": rep_dict[key][\"precision\"],\n",
    "            \"recall\": rep_dict[key][\"recall\"],\n",
    "            \"f1\": rep_dict[key][\"f1-score\"],\n",
    "            \"support\": rep_dict[key][\"support\"],\n",
    "        })\n",
    "per_class_df = pd.DataFrame(rows).sort_values(\"class\")\n",
    "per_class_path = f\"{OUT_DIR}/hgb_holdout_per_class_metrics.csv\"\n",
    "per_class_df.to_csv(per_class_path, index=False)\n",
    "print(f\"Saved HGB holdout per-class metrics table: {per_class_path}\")\n",
    "\n",
    "# Optional print: quick recall view\n",
    "print(\"\\nHGB holdout per-class recall (quick view):\")\n",
    "print(per_class_df[[\"class\",\"name\",\"recall\",\"support\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Repeated Group Holdout (30 splits) ===\")\n",
    "logreg_rep = repeated_holdout(logreg, X, y, groups, repeats=30, test_size=0.2, random_state=RANDOM_STATE)\n",
    "hgb_rep    = repeated_holdout(hgb,   X, y, groups, repeats=30, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"LogReg:\", logreg_rep)\n",
    "print(\"HGB   :\", hgb_rep)\n",
    "\n",
    "# ============================================================\n",
    "# Save results (JSON) + NEW: holdout wells + artifact paths\n",
    "# ============================================================\n",
    "results = {\n",
    "    \"features_version\": FEATURES_VERSION,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "\n",
    "    \"split_traceability\": {\n",
    "        \"train_wells\": train_wells,\n",
    "        \"holdout_wells\": holdout_wells,\n",
    "        \"n_train_wells\": len(train_wells),\n",
    "        \"n_holdout_wells\": len(holdout_wells),\n",
    "    },\n",
    "\n",
    "    \"logreg_cv\": logreg_cv,\n",
    "    \"hgb_cv\": hgb_cv,\n",
    "\n",
    "    \"logreg_holdout\": {\n",
    "        k: v for k, v in logreg_hold.items()\n",
    "        if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}\n",
    "    },\n",
    "    \"hgb_holdout\": {\n",
    "        k: v for k, v in hgb_hold.items()\n",
    "        if k not in {\"classification_report\",\"classification_report_dict\",\"labels\",\"y_pred\"}\n",
    "    },\n",
    "\n",
    "    \"holdout_diagnostics\": {\n",
    "    \"hgb_confusion_matrix_csv\": cm_path,\n",
    "    \"hgb_per_class_metrics_csv\": per_class_path,\n",
    "    \"hgb_top_confusions_csv\": top_conf_path,\n",
    "    \"hgb_permutation_importance_csv\": perm_path,\n",
    "    \"hgb_per_well_metrics_csv\": well_metrics_path,\n",
    "},\n",
    "\n",
    "    \"logreg_repeated_holdout\": logreg_rep,\n",
    "    \"hgb_repeated_holdout\": hgb_rep,\n",
    "\n",
    "    \"notes\": {\n",
    "        \"leakage_control\": [\n",
    "            \"train-only column filtering inside pipeline\",\n",
    "            \"group splitting by well_id\",\n",
    "            \"CV on train wells only (plus a separate holdout)\",\n",
    "            \"major classes computed from y_train per fold\",\n",
    "            \"fixed-label macro-F1 with zero_division=0\",\n",
    "            \"repeated group holdout metrics (mean/std)\",\n",
    "            \"hybrid imputation: median for continuous; prob semantic fix + renorm; state-level->-1; + missing indicators\",\n",
    "            \"cache path versioned to avoid stale features\",\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUT_DIR}/results_summary.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved: {OUT_DIR}/results_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162faa1",
   "metadata": {
    "papermill": {
     "duration": 0.005182,
     "end_time": "2026-01-08T18:07:17.729236",
     "exception": false,
     "start_time": "2026-01-08T18:07:17.724054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results (Current)\n",
    "- Logistic Regression (repeated group shuffle, 30): macro-F1 ≈ 0.22, fault-vs-normal F1 ≈ 0.61\n",
    "- HistGradientBoosting (repeated group shuffle, 30): macro-F1 ≈ 0.42, fault-vs-normal F1 ≈ 0.76\n",
    "- Note: Macro-F1 is unstable because some classes have very few samples (e.g., class 1 has 4 files).\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 988298,
     "sourceId": 9353254,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1787.311766,
   "end_time": "2026-01-08T18:07:18.354906",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T17:37:31.043140",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2287d6fa29194a6d9ce58a7bd5de71b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f7e54e48d91343f7a45edd39d63b2e93",
        "IPY_MODEL_882141e1267a42b787837dba07d8e1fc",
        "IPY_MODEL_2fcc7bf1a1be4827bb25d628223d8f08"
       ],
       "layout": "IPY_MODEL_8985a830c14c4f57ab9f96d02b25dfd6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "239384542b79418db4019a373aab8c9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2fcc7bf1a1be4827bb25d628223d8f08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d35eb60d9148418f80229575adbbf63c",
       "placeholder": "​",
       "style": "IPY_MODEL_8aea6ee8cba442ea922d9785dc7e1449",
       "tabbable": null,
       "tooltip": null,
       "value": " 1119/1119 [02:26&lt;00:00,  5.35it/s]"
      }
     },
     "46c90ea082974975b8e2413838a6e5d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "882141e1267a42b787837dba07d8e1fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_239384542b79418db4019a373aab8c9f",
       "max": 1119.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d1034a3804624ee2a89a5fdeff9b8a71",
       "tabbable": null,
       "tooltip": null,
       "value": 1119.0
      }
     },
     "8985a830c14c4f57ab9f96d02b25dfd6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8aea6ee8cba442ea922d9785dc7e1449": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af1a2b02d737485da0d8ac422e3f4a2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d1034a3804624ee2a89a5fdeff9b8a71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d35eb60d9148418f80229575adbbf63c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f7e54e48d91343f7a45edd39d63b2e93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_46c90ea082974975b8e2413838a6e5d8",
       "placeholder": "​",
       "style": "IPY_MODEL_af1a2b02d737485da0d8ac422e3f4a2e",
       "tabbable": null,
       "tooltip": null,
       "value": "Building WELL dataset: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
